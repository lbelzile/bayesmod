---
title: "Assignment 2"
---

Assignment 2 is due by Tuesday, November 7th 2023, at 8:00am on ZoneCours. 

- Please submit your report as a PDF and your code as a **R** file (in addition to Quarto or Rmarkdown files if any). 
- Please use the naming convention `studentid.ext` for all your files, replacing `studentid` by your student id and `ext` by the extension (either `pdf`, `qmd`, `rmd` or `r`).
- Show your work and make sure your analysis is **reproducible** (by setting the seed and checking before submission that your code compiles without error).

The databases used in the assignment are available from the `hecbayes` and `LearnBayes` **R** packages. To download the latest version of the latter, use the command

```{r}
#| eval: false
#| echo: true
install.packages(c("LearnBayes", "remotes"))
remotes::install_github("lbelzile/hecbayes")
```

# 2.1 Why Americans (don't) vote

The `voting` database contains results from a Ipsos survey commissioned by FiveThirtyEight to study determinants of electoral participation during national elections in the United States: results are discussed in [*Why Millions of Americans Don't Vote*](https://projects.fivethirtyeight.com/non-voters-poll-2020-election/). 


We are interested in modelling the frequency of voting (variable `vote`), an ordered categorical variable which is treated as a draw from a multinomial distribution with $d=3$ categories. Records have been coupled with census registers and weighted to reflect the US population as a whole; the `weight` variable reflects the sampling weight. 

The Dirichlet distribution, whose density for a $d$-variate vector $(x_1, \ldots, x_d)$ on the $d$-simplex^[The components of the vector $\boldsymbol{x} \in \mathbb{R}^d_{+}$ are positive and sum to one.] is 
$$
f(x; \boldsymbol{\alpha}) = \frac{\Gamma\left(\sum_{j=1}^d \alpha_j\right)}{\prod_{j=1}^d \Gamma(\alpha_j)}\prod_{j=1}^d x_j^{\alpha_j-1}, \qquad \alpha_j > 0 \, (j=1, \ldots, d).
$$

1. Perform an exploratory data analysis: what explains patterns of voting?
2. Derive the posterior distribution of the proportion for each voting habit **per education level** if we assign to each independent uniform Dirichlet priors with $\boldsymbol{\alpha}=\mathbf{1}_3$.\
*Hint*: start by writing the weighted log likelihood function.
3. Report the expected posterior proportion of `always` voters in each class.
4. Plot the posterior density of the odds ratio for `always` voters for all pairs and associated 89% credible intervals. Comment on the results
5. 
    a. Split records by age category (with age bands $(20,35], (35, 55], (55, 70] \ldots, (70, 100]$). 
    b. For each age category, compute the weighted number of people per voter category.
    c. Next, draw 1000 samples from the posterior predictive distributions (depending on education level) for each individual and compute the weighted number of person per age bin, in order to get 50% credible intervals for each age bands. 
    d. Plot the resulting bands with your data estimates. 
    e. What do these suggest about the goodness of fit (i.e., is the variability sufficient to capture observed effects)?
6. @fig-age shows the unconditional change in probability of voting as a function of age. What these suggest about the patterns uncovered, and how could these be addressed in a statistical analysis?

```{r}
#| message: false
#| warning: false
#| eval: true
#| echo: false
#| label: fig-age
#| fig-cap: "Probability of belonging to a voting category as a function of age; estimates are based on a multinomial generalized additive logistic model."
data(voting, package = "hecbayes")
votecat <- as.integer(factor(voting$vote, ordered = FALSE))-1L
mod <- mgcv::gam(
  formula = list(
    votecat ~ s(age, bs = "cr"), 
            ~ s(age, bs = "cr")), 
  weights = weight,
  family = mgcv::multinom(K = 2), 
  data = voting)

newage <- 20:90
nb <- length(newage)
pred <- c(predict(mod,
                newdata = data.frame(age = newage), 
                type = "response"))
cat <- factor(rep(c("rarely/never","sporadic","always"), each = nb))
library(ggplot2)
ggplot(data = data.frame(age = rep(newage, 3L*nb),
                         pred = pred, 
                         cat = cat),
       mapping = aes(x = age, y = pred, col = cat, group = cat)) + 
  geom_line() + 
  scale_y_continuous(limits = c(0,1), 
                     expand = c(0,0),
                     labels = scales::percent) + 
  MetBrewer::scale_color_met_d(name = "Hiroshige") +
  labs(color = "voting category",
       x = "age (in years)",
       y = "",
       subtitle = "Smoothed proportion per voting category") +
  theme_classic() +
  theme(legend.position = "bottom")
```

# 2.2 Can speed limits reduce accidents?

The `sweden` dataset [@Svensson:1981] contains records for road traffic accidents following a speed limits experiment on motorways in Sweden. Road traffic accidents and speed limits experiment on motorway in Sweden.

We consider a Poisson regression model: let
$Y_{i1}$ (respectively $Y_{i2}$) denote the number of accidents in 1961
(respectively 1962) on day $i$ and let $\texttt{limit}_{ij}$ denote a binary
indicator equal to one if speed limits were enforced on day $i$ of year
$j$. 
\begin{align*}
Y_{i1} &\sim \mathsf{Po}\{\exp(\delta_i + \alpha_1 \texttt{limit}_{i1})\},\\
Y_{i2} &\sim\mathsf{Po}\{\exp(\delta_i + \gamma +  \alpha_2 \texttt{limit}_{i2})\}, \qquad i=1, \ldots, 92. 
\end{align*}
The nuisance parameters $\delta_1, \ldots, \delta_{92}$ control for changes in background number of accidents and are of no practical interest, while $\gamma$ denotes the change from 1961 to 1962. We are interested here in assessing changes in the number of accidents due to the policy, $\alpha$; of secondary interest is to determine whether there has been a decrease in the number of accident relative to 1961. 

Use vague priors for $\gamma$ and $\alpha$ and use a random effect $\delta_i \sim \mathsf{No}(0, \tau^2)$ with a suitable prior for $\tau$.

1. Using Metropolis-within-Gibbs, obtain posterior draws for the parameters. Perform enough simulations to obtain an effective sample size of 1000 for each parameter and report the effective sample sizes.
2. Produce trace plots of the Markov chains and correlograms for parameters $\alpha$, $\gamma$ and $\tau$. Comment on the quality of your sampler.
3. Return the posterior mode, posterior mean and 89\% HPD region, in addition to density plot for the multiplicative effect on traffic, $\exp(\gamma)$. Comment on the effectiveness of the policy.
4. Obtain a caterpillar plot of the parameters $\delta_i$ ($i=1,\ldots, 92$) as follows: 
  a. obtain point estimates of the posterior mode for each random effect $\delta_i$ and sort them in increasing order
  b. for each parameter, obtain 80% credible intervals.
  c. plot the point estimates and intervals against order.
5. Fit the model using maximum likelihood treating day as a fixed effect using the code provided below. Superimpose the estimated effects on your caterpillar and discuss the shrinkage effect of the prior.

```{r}
#| eval: false
#| echo: true
options(contrasts=c("contr.sum","contr.poly")) # sum-to-zero constraint for factors
data(sweden, package = "hecbayes")
mod <- glm(accidents ~ day + limit + year,
               family = poisson("log"), data = sweden)
fixef <- coef(mod)[substr(names(coef(mod)), 1,3) == "day"]
fixef <- c(fixef, day92 = -sum(fixef))
```

6. Perform a sensitivity analysis of your prior for $\tau$ by comparing with, e.g., a folded Student-$t$, the improper prior $\tau^{-1}$, an inverse gamma conditionally conjugate prior or a penalized complexity prior with different rate parameters. Comment on the robustness of your conclusions to the choice of prior for $\tau$.

# 2.3 Predicting success in university based on high school and standardized tests

@Albert:1994 considers the grade point average (GPA) of university students as a function of their high school ranking (HSR, eight categories) and their ACT standardized score test results (five categories, higher scores are better). This kind of problem is useful for student retention, as it may help target students who may need additional resources early on.

Model grade point averages as Gaussian, with 
\begin{align*}
Y_{ij} &\sim \mathsf{No}(\mu_{ij}, \sigma^2),\\
\mu_{ij} &= \alpha_{i} + \gamma_{j} \qquad i=1, \ldots, 8; j =1, \ldots, 5
\end{align*}
Suppose we want to enforce the constraint that the average GPA is increasing as a function of `HSR` (so $\alpha_1 \leq \alpha_2 \leq \cdots \leq \alpha_8$) and of `ACT` scores $(\gamma_1 \leq \cdots \leq \gamma_5)$, with higher values and ranking indicating higher average GPA. Use a conjugate prior for $\sigma$ centered at 0.65. The `iowagpa` data set from package `LearnBayes` contains the data frame with the average response `gpa`, the counts `n` in each category and both categorical explanatories `HSR` and `ACT`.

For simplicity, use improper priors 
$$
p(\sigma, \boldsymbol{\alpha}, \boldsymbol{\gamma}) =\frac{ \mathsf{I}(-\infty \leq \alpha_1 \leq \cdots \leq \alpha_8 \leq \infty) \mathsf{I}(-\infty \leq \gamma_1 \leq \cdots \leq \gamma_5 \leq \infty)}{\sigma},
$$
where $\mathsf{I}(\cdot)$ denotes the indicator function. You could also use a conjugate prior for $\sigma$.

1. Use Gibbs sampling with a random scan to sample from the 40 mean parameters, enforcing the monotonicity constraints.
2. Consider a block update to sample all of $\boldsymbol{\alpha}$ and $\boldsymbol{\gamma}$, in turn. Compare the Markov chains with the one parameter at a time: does it improve mixing?

*Hint*: to sample a $d$-variate Gaussian vector $\boldsymbol{Z} \in \mathbb{R}^d$ with mean $\boldsymbol{\mu}$ and variance $\boldsymbol{\Sigma}$ subject to the linear constraints $\mu_{i+1} -\mu_i \geq 0$ for $i=2, \ldots, d$, we encode the restrictions in a $d \times d$ invertible matrix $\mathbf{A}$, simulate $\boldsymbol{X} \sim \mathsf{No}(\mathbf{A}\boldsymbol{\mu}, \mathbf{A}\boldsymbol{\Sigma}\mathbf{A}^{\top})$ truncated below by $(-\infty, \boldsymbol{0}_{d-1})$ and return $\boldsymbol{Z} = \mathbf{A}^{-1}\boldsymbol{X}$. The `TruncNormA` function below uses the `TruncatedNormal` package to accomplish this.

```{r}
#| eval: false
#| echo: true
TruncNormA <- function(mu, sigma){
  d <- length(mu)
  stopifnot(d == ncol(matrix), 
            d == nrow(matrix),
            isTRUE(all(eigen(sigma, only.values = TRUE)$values > 0)))
  # Build constraint matrix
  A <- diag(1, d)
  for (i in 2:d) {
    A[i, i - 1] <- -1
  }
  Ainv <- solve(A)
  samp <- TruncatedNormal::rtmvnorm(
    n = 1,
    mu = c(A %*% mu),
    sigma = A %*% sigma %*% t(A),
    lb = c(-Inf, rep(0, d - 1)),
    ub = rep(Inf, d)
  )
  
  return(as.numeric(Ainv %*% samp))
}
```

3. Plot the posterior means as a function of `HSR` and `ACT` and superimpose the cell means. Does the model appear reasonable?
4. Consider instead a simpler linear regression model where $\mu= \beta_0 + \beta_1 \texttt{HSR} + \beta_2 \texttt{ACT}$, with suitable priors for $\boldsymbol{\beta}$ to be determined. Use a Metropolis--Hastings algorithm to draw posterior samples.
   a. Plot the posterior density of both slope parameters and comment on the postulated relationship.
   b. Using the Bayesian workflow, assess the adequacy of the models in reproducing the average cell means. Contrast the two models.
5. Use the models to obtain the posterior probability of having a satisfactory GPA of 2.5 and above using either model for each cell.

