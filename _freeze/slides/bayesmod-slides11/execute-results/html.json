{
  "hash": "4ded6a309ec4486f5dbd713f612d22a8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"LÃ©o Belzile\"\nsubtitle: \"Expectation propagation\"\ndate: today\ndate-format: \"[Last compiled] dddd MMM D, YYYY\"\neval: true\necho: true\ncache: true\nbibliography: MATH80601A.bib\nformat:\n  revealjs:\n    slide-number: true\n    html-math-method: mathjax\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n    logo: \"fig/logo_hec_montreal_bleu_web.png\"\n---\n\n\n\n\n\n## Revisiting Kullback--Leibler divergence\n\nThe Kullback--Leibler divergence between densities $f_t(\\cdot)$ and $g(\\cdot; \\boldsymbol{\\psi}),$ is\n\\begin{align*}\n\\mathsf{KL}(f_t \\parallel g) &=\\int \\log \\left(\\frac{f_t(\\boldsymbol{x})}{g(\\boldsymbol{x}; \\boldsymbol{\\psi})}\\right) f_t(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}\\\\\n\\\\ &= \\mathsf{E}_{f_t}\\{\\log f_t(\\boldsymbol{X})\\} - \\mathsf{E}_{f_t}\\{\\log g(\\boldsymbol{X}; \\boldsymbol{\\psi})\\}\n\\end{align*}\n\n## Forward Kullback--Leibler divergence\n\nIf $g(\\cdot; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ is Gaussian approximating density, then we minimize the KL divergence by matching moments:\n\\begin{align*}\n\\boldsymbol{\\mu}^* &= \\mathsf{E}_{f_t}(\\boldsymbol{X})\\\\\n\\boldsymbol{\\Sigma}^* &= \\mathsf{E}_{f_t}\\left\\{(\\boldsymbol{X}-\\boldsymbol{\\mu})(\\boldsymbol{X}-\\boldsymbol{\\mu})^\\top\\right\\}\n\\end{align*}\nSee Exercise 10.1 for a derivation.\n\n\n\n## Variational inference\n\nWe don't know the posterior mean and variance! (they depend on unknown normalizing constant).\n\nVariational inference finds rather the approximation that minimizes the **reverse Kullback--Leibler divergence** $\\mathsf{KL}(g \\parallel f_t).$\n\nQualitatively, this yields a very different approximation.\n\n## Comparing approximations\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Approximation of a correlated bivariate Gaussian density by independent Gaussians.](bayesmod-slides11_files/figure-revealjs/fig-klvsrev-1.png){#fig-klvsrev width=960}\n:::\n:::\n\n\n\n\n\n## Gaussian as exponential family\n\nWrite the Gaussian distribution in terms of canonical parameters\n\\begin{align*}\n q(\\boldsymbol{\\theta}) \\propto \\exp \\left( - \\frac{1}{2} \\boldsymbol{\\theta}^\\top \\mathbf{Q}\\boldsymbol{\\theta} + \\boldsymbol{\\theta}^\\top \\boldsymbol{r}\\right)\n\\end{align*}\nwhere $\\mathbf{Q}$ is the precision matrix and $\\boldsymbol{r}=\\mathbf{Q}\\boldsymbol{\\mu},$ the linear shift.\n\n\n## Notation\n\n\nLet $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})=\\exp\\{-\\psi(\\boldsymbol{\\theta})\\}$ denote the posterior density. \n\n\nSince logarithm is a monotonic transform, we can equivalent minimize $\\psi(\\boldsymbol{\\theta})$ to find the posterior mode.\n\nDenote \n- the gradient $\\nabla_{\\boldsymbol{\\theta}} \\psi(\\boldsymbol{\\theta}) = \\partial \\psi/\\partial \\boldsymbol{\\theta}$\n- the Hessian matrix $\\mathbf{H}(\\boldsymbol{\\theta}) = \\partial^2 \\psi/(\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top).$\n\n## Newton algorithm\n\nStarting from an initial value $\\boldsymbol{\\theta}_{(0)},$  we consider at step $i$, a second order Taylor series expansion of $\\psi(\\boldsymbol{\\theta})$ around $\\boldsymbol{\\theta}_{(i)},$ which gives\n\\begin{align*}\n \\psi(\\boldsymbol{\\theta}) &\\approx \\psi(\\boldsymbol{\\theta}_{(i)}) + \\nabla_{\\boldsymbol{\\theta}} \\psi(\\boldsymbol{\\theta}_{(i)})(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{(i)}) \\\\& \\quad + (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{(i)})^\\top\\mathbf{H}(\\boldsymbol{\\theta}_{(i)})(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{(i)})\n\\end{align*}\n\n\n## Gaussian smoothing\n\nThe term $\\psi(\\boldsymbol{\\theta}_{(i)})$ is constant, so if we plug-in this inside the exponential, we obtain\n\\begin{align*}\n q_{(i+1)}(\\boldsymbol{\\theta}) &\\propto \\exp \\left\\{ - \\frac{1}{2} \\boldsymbol{\\theta}^\\top\\mathbf{H}(\\boldsymbol{\\theta}_{(i)}) \\boldsymbol{\\theta} + \\boldsymbol{\\theta}^\\top\\mathbf{H}(\\boldsymbol{\\theta}_{(i)})\\boldsymbol{\\theta}_{(i+1)}\\right\\}\n \\end{align*}\n where the mean of the approximation is\n \\begin{align*}\n \\boldsymbol{\\theta}_{(i+1)} = \\boldsymbol{\\theta}_{(i)} - \\mathbf{H}^{-1}(\\boldsymbol{\\theta}_{(i)}) \\nabla_{\\boldsymbol{\\theta}} \\psi(\\boldsymbol{\\theta}_{(i)}).\n\\end{align*}\n\n## Side remarks\n\nThe new mean vector $\\boldsymbol{\\theta}_{(i+1)}$ corresponds to a Newton update, and at the same time we have defined a sequence of Gaussian updating approximations.\n\nThis scheme works provided that $\\mathbf{H}(\\boldsymbol{\\theta}_{(i)})$ is positive definite and invertible. Without convexity, we get a divergent sequence.\n\nThe fixed point to which the algorithm converges is the Laplace approximation.\n\n## Location-scale transformation gradients\n\nFor location-scale family, with a Gaussian approximation on the target $\\boldsymbol{\\theta} = \\boldsymbol{\\mu} + \\mathbf{L}\\boldsymbol{Z}$ with $\\mathbf{LL}^\\top=\\boldsymbol{\\Sigma}$ and $\\boldsymbol{Z} \\sim \\mathsf{Gauss}_p(\\boldsymbol{0}_p, \\mathbf{I}_p)$ that the gradient satisfies\n\\begin{align*}\n \\nabla_{\\boldsymbol{\\mu}}\\mathsf{ELBO}(q)&= -\\mathsf{E}_{\\boldsymbol{Z}}\\{\\nabla_{\\boldsymbol{\\theta}}\\psi(\\boldsymbol{\\theta})\\} \\\\\n \\nabla_{\\mathbf{L}}\\mathsf{ELBO}(q)&= -\\mathsf{E}_{\\boldsymbol{Z}}\\{\\nabla_{\\boldsymbol{\\theta}}\\psi(\\boldsymbol{\\theta})\\boldsymbol{Z}^\\top\\} + \\mathbf{L}^{-\\top}\n\\end{align*}\n\n## Stein's lemma\n\nConsider $h: \\mathbb{R}^d \\to \\mathbb{R}$ a differentiable function and integration with respect to $\\boldsymbol{X} \\sim \\mathsf{Gauss}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ such that the gradient is absolutely integrable, $\\mathsf{E}_{\\boldsymbol{X}}\\{|\\nabla_i h(\\boldsymbol{X})|\\} < \\infty$ for $i=1, \\ldots, d.$ Then [@Liu:1994],\n\\begin{align*}\n\\mathsf{E}_{\\boldsymbol{X}}\\left\\{h(\\boldsymbol{X})(\\boldsymbol{X}-\\boldsymbol{\\mu})\\right\\} = \\boldsymbol{\\Sigma}\\mathsf{E}_{\\boldsymbol{X}}\\left\\{\\nabla h(\\boldsymbol{X})\\right\\}\n\\end{align*}\n\n\n\n\n## Alternative expression for the scale\n\nIf we apply Stein's lemma,\n\\begin{align*}\n  \\nabla_{\\mathbf{L}}\\mathsf{ELBO}(q)&= -\\mathsf{E}_{\\boldsymbol{Z}}\\left\\{ \\frac{\\partial^2 \\psi(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^\\top}\\right\\}\\mathbf{L} + \\mathbf{L}^{-\\top}.\n\\end{align*}\n\n## Variational inference\n\nAt a critical point, both of these derivatives must be zero, whence\n\\begin{align*}\n \\mathsf{E}_{\\boldsymbol{Z}}\\{\\nabla_{\\boldsymbol{\\theta}}\\psi(\\boldsymbol{\\theta})\\} &= \\boldsymbol{0}_p. \\\\\n \\mathsf{E}_{\\boldsymbol{Z}}\\left\\{ \\frac{\\partial^2 \\psi(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^\\top}\\right\\} &= \\boldsymbol{\\Sigma}^{-1}.\n\\end{align*}\n\n## Variational inference vs Laplace\n\nCompared to the Laplace approximation, the variational Gaussian approximation returns \n\n- a vector $\\boldsymbol{mu}$ around which the expected value of the gradient is zero \n- and similarly $\\boldsymbol{\\Sigma}$ for which the  expected value of the curvature (Hessian) is equal to the precision. \n\nThe averaging step is what distinguishes the Laplace and variational approximations.\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}