{
  "hash": "d28752134ec0a3ba5ab04c4cc0765b56",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"LÃ©o Belzile\"\nsubtitle: \"Expectation propagation\"\ndate: today\ndate-format: \"[Last compiled] dddd MMM D, YYYY\"\neval: true\necho: true\ncache: true\nbibliography: MATH80601A.bib\nformat:\n  revealjs:\n    slide-number: true\n    html-math-method: mathjax\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n    logo: \"fig/logo_hec_montreal_bleu_web.png\"\n---\n\n\n\n\n\n## Revisiting Kullback--Leibler divergence\n\nThe Kullback--Leibler divergence between densities $f_t(\\cdot)$ and $g(\\cdot; \\boldsymbol{\\psi}),$ is\n\\begin{align*}\n\\mathsf{KL}(f_t \\parallel g) &=\\int \\log \\left(\\frac{f_t(\\boldsymbol{x})}{g(\\boldsymbol{x}; \\boldsymbol{\\psi})}\\right) f_t(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}\\\\\n\\\\ &= \\mathsf{E}_{f_t}\\{\\log f_t(\\boldsymbol{X})\\} - \\mathsf{E}_{f_t}\\{\\log g(\\boldsymbol{X}; \\boldsymbol{\\psi})\\}\n\\end{align*}\n\n## Forward Kullback--Leibler divergence\n\nIf $g(\\cdot; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ is Gaussian approximating density, then we minimize the KL divergence by matching moments:\n\\begin{align*}\n\\boldsymbol{\\mu}^* &= \\mathsf{E}_{f_t}(\\boldsymbol{X})\\\\\n\\boldsymbol{\\Sigma}^* &= \\mathsf{E}_{f_t}\\left\\{(\\boldsymbol{X}-\\boldsymbol{\\mu}^*)(\\boldsymbol{X}-\\boldsymbol{\\mu}^*)^\\top\\right\\}\n\\end{align*}\nSee Exercise 10.1 for a derivation.\n\n\n\n## Variational inference\n\nWe don't know the posterior mean and variance! (they depend on unknown normalizing constant).\n\nVariational inference finds rather the approximation that minimizes the **reverse Kullback--Leibler divergence** $\\mathsf{KL}(g \\parallel f_t).$\n\nQualitatively, this yields a very different approximation.\n\n## Comparing approximations\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Approximation of a correlated bivariate Gaussian density by independent Gaussians.](bayesmod-slides11_files/figure-revealjs/fig-klvsrev-1.png){#fig-klvsrev width=960}\n:::\n:::\n\n\n\n\n\n## Gaussian as exponential family\n\nWrite the Gaussian distribution in terms of canonical parameters\n\\begin{align*}\n q(\\boldsymbol{\\theta}) \\propto \\exp \\left( - \\frac{1}{2} \\boldsymbol{\\theta}^\\top \\mathbf{Q}\\boldsymbol{\\theta} + \\boldsymbol{\\theta}^\\top \\boldsymbol{r}\\right)\n\\end{align*}\nwhere $\\mathbf{Q}$ is the precision matrix and $\\boldsymbol{r}=\\mathbf{Q}\\boldsymbol{\\mu},$ the linear shift.\n\n\n## Notation\n\n\nLet $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})=\\exp\\{-\\psi(\\boldsymbol{\\theta})\\}$ denote the posterior density. \n\n\nSince logarithm is a monotonic transform, we can equivalent minimize $\\psi(\\boldsymbol{\\theta})$ to find the posterior mode.\n\nDenote \n\n- the gradient $\\nabla_{\\boldsymbol{\\theta}} \\psi(\\boldsymbol{\\theta}) = \\partial \\psi/\\partial \\boldsymbol{\\theta}$\n- the Hessian matrix $\\mathbf{H}(\\boldsymbol{\\theta}) = \\partial^2 \\psi/(\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top).$\n\n## Newton algorithm\n\nStarting from an initial value $\\boldsymbol{\\theta}_{(0)},$  we consider at step $i$, a second order Taylor series expansion of $\\psi(\\boldsymbol{\\theta})$ around $\\boldsymbol{\\theta}_{(i)},$ which gives\n\\begin{align*}\n \\psi(\\boldsymbol{\\theta}) &\\approx \\psi(\\boldsymbol{\\theta}_{(i)}) + \\nabla_{\\boldsymbol{\\theta}} \\psi(\\boldsymbol{\\theta}_{(i)})(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{(i)}) \\\\& \\quad + (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{(i)})^\\top\\mathbf{H}(\\boldsymbol{\\theta}_{(i)})(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{(i)})\n\\end{align*}\n\n\n## Gaussian smoothing\n\nThe term $\\psi(\\boldsymbol{\\theta}_{(i)})$ is constant, so if we plug-in this inside the exponential, we obtain\n\\begin{align*}\n q_{(i+1)}(\\boldsymbol{\\theta}) &\\propto \\exp \\left\\{ - \\frac{1}{2} \\boldsymbol{\\theta}^\\top\\mathbf{H}(\\boldsymbol{\\theta}_{(i)}) \\boldsymbol{\\theta} + \\boldsymbol{\\theta}^\\top\\mathbf{H}(\\boldsymbol{\\theta}_{(i)})\\boldsymbol{\\theta}_{(i+1)}\\right\\}\n \\end{align*}\n where the mean of the approximation is\n \\begin{align*}\n \\boldsymbol{\\theta}_{(i+1)} = \\boldsymbol{\\theta}_{(i)} - \\mathbf{H}^{-1}(\\boldsymbol{\\theta}_{(i)}) \\nabla_{\\boldsymbol{\\theta}} \\psi(\\boldsymbol{\\theta}_{(i)}).\n\\end{align*}\n\n## Side remarks\n\nThe new mean vector $\\boldsymbol{\\theta}_{(i+1)}$ corresponds to a Newton update, and at the same time we have defined a sequence of Gaussian updating approximations.\n\nThis scheme works provided that $\\mathbf{H}(\\boldsymbol{\\theta}_{(i)})$ is positive definite and invertible. Without convexity, we get a divergent sequence.\n\nThe fixed point to which the algorithm converges is the Laplace approximation.\n\n## Location-scale transformation gradients\n\nFor location-scale family, with a Gaussian approximation on the target $\\boldsymbol{\\theta} = \\boldsymbol{\\mu} + \\mathbf{L}\\boldsymbol{Z}$ with $\\mathbf{LL}^\\top=\\boldsymbol{\\Sigma}$ and $\\boldsymbol{Z} \\sim \\mathsf{Gauss}_p(\\boldsymbol{0}_p, \\mathbf{I}_p)$ that the gradient satisfies\n\\begin{align*}\n \\nabla_{\\boldsymbol{\\mu}}\\mathsf{ELBO}(q)&= -\\mathsf{E}_{\\boldsymbol{Z}}\\{\\nabla_{\\boldsymbol{\\theta}}\\psi(\\boldsymbol{\\theta})\\} \\\\\n \\nabla_{\\mathbf{L}}\\mathsf{ELBO}(q)&= -\\mathsf{E}_{\\boldsymbol{Z}}\\{\\nabla_{\\boldsymbol{\\theta}}\\psi(\\boldsymbol{\\theta})\\boldsymbol{Z}^\\top\\} + \\mathbf{L}^{-\\top}\n\\end{align*}\n\n## Stein's lemma\n\nConsider $h: \\mathbb{R}^d \\to \\mathbb{R}$ a differentiable function and integration with respect to $\\boldsymbol{X} \\sim \\mathsf{Gauss}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ such that the gradient is absolutely integrable, $\\mathsf{E}_{\\boldsymbol{X}}\\{|\\nabla_i h(\\boldsymbol{X})|\\} < \\infty$ for $i=1, \\ldots, d.$ Then [@Liu:1994],\n\\begin{align*}\n\\mathsf{E}_{\\boldsymbol{X}}\\left\\{h(\\boldsymbol{X})(\\boldsymbol{X}-\\boldsymbol{\\mu})\\right\\} = \\boldsymbol{\\Sigma}\\mathsf{E}_{\\boldsymbol{X}}\\left\\{\\nabla h(\\boldsymbol{X})\\right\\}\n\\end{align*}\n\n\n## Alternative expression for the scale\n\nIf we apply Stein's lemma,\n\\begin{align*}\n  \\nabla_{\\mathbf{L}}\\mathsf{ELBO}(q)&= -\\mathsf{E}_{\\boldsymbol{Z}}\\left\\{ \\frac{\\partial^2 \\psi(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^\\top}\\right\\}\\mathbf{L} + \\mathbf{L}^{-\\top}.\n\\end{align*}\n\n## Variational inference\n\nAt a critical point, both of these derivatives must be zero, whence\n\\begin{align*}\n \\mathsf{E}_{\\boldsymbol{Z}}\\{\\nabla_{\\boldsymbol{\\theta}}\\psi(\\boldsymbol{\\theta} = \\boldsymbol{\\mu} + \\mathbf{L}\\boldsymbol{Z})\\} &= \\boldsymbol{0}_p. \\\\\n \\mathsf{E}_{\\boldsymbol{Z}}\\left\\{ \\frac{\\partial^2 \\psi(\\boldsymbol{\\theta} = \\boldsymbol{\\mu} + \\mathbf{L}\\boldsymbol{Z})}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^\\top}\\right\\} &= \\boldsymbol{\\Sigma}^{-1}.\n\\end{align*}\n\n## Variational inference vs Laplace\n\nCompared to the Laplace approximation, the variational Gaussian approximation returns \n\n- a vector $\\boldsymbol{\\mu}$ around which the **expected value of the gradient** is zero\n- and similarly $\\boldsymbol{\\Sigma}$ which matches the expected value of the Hessian.\n\nThe averaging step is what distinguishes the Laplace and variational approximations.\n\n## Expectation propagation\n\n\nExpectation propagation is an approximation algorithm proposed by @Minka:2001.\n\nIt is more accurate, but generally slower than variational Bayes.\n\nHowever, the algorithm can be parallelized, which makes it fast.\n\n\n## Decomposition\n\nEP builds on a decomposition of the posterior as a product of terms; with likelihood contributions $L_i(\\boldsymbol{\\theta})$\n\\begin{align*}\n p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\propto p(\\boldsymbol{\\theta}) \\prod_{i=1}^n L_i(\\boldsymbol{\\theta}) = \\prod_{i=0}^n L_i(\\boldsymbol{\\theta})\n\\end{align*}\nWe call $L_i$ the \"factors\" or \"sites\", and $L_0(\\boldsymbol{\\theta})$ is the prior density.\n\n## Comment on factorization\n\n\nSuch factorization is also feasible in graphical models (e.g., autoregressive processes, Markov fields), but needs not be unique.\n\n- Note that it is not equivalent to the factorization of the posterior (mean-field approximation) for variational Bayes, as every term in the EP approximation is a function of the whole vector $\\boldsymbol{\\theta}.$\n\n## Expectation propagation approximating density\n\nConsiders a factor structure approximation in which each $q_i$ is Gaussian with precision $\\mathbf{Q}_i$ and linear shift $\\boldsymbol{r}_i$,\n\\begin{align*}\n q(\\boldsymbol{\\theta}) &\\propto \\prod_{i=1}^n q_i(\\boldsymbol{\\theta})\n \\\\& \\propto \\prod_{i=0}^n \\exp \\left(-\\frac{1}{2} \\boldsymbol{\\theta}^\\top\\mathbf{Q}_i\\boldsymbol{\\theta} + \\boldsymbol{\\theta}^\\top\\boldsymbol{r}_i\\right)\n \\\\ &= \\exp \\left( - \\frac{1}{2} \\boldsymbol{\\theta}^\\top \\sum_{i=0}^n\\mathbf{Q}_i\\boldsymbol{\\theta} + \\boldsymbol{\\theta}^\\top \\sum_{i=0}^n\\boldsymbol{r}_i\\right).\n\\end{align*}\n\n\n## Step 1 of expectation propagation {.smaller}\n\nForm the **cavity** by removing one factor $q_j$, so that\n\\begin{align*}\n q_{-j}(\\boldsymbol{\\theta}) &= \\prod_{\\substack{i=0 \\\\ i \\neq j}}^n q_i(\\boldsymbol{\\theta}) =q(\\boldsymbol{\\theta})/q_j(\\boldsymbol{\\theta})\\\\\n&\\propto \\exp \\left\\{ - \\frac{1}{2} \\boldsymbol{\\theta}^\\top \\left(\\sum_{i=0}^n\\mathbf{Q}_i - \\mathbf{Q}_j\\right)\\boldsymbol{\\theta} \\right.\\\\&+ \\left.\\boldsymbol{\\theta}^\\top \\left(\\sum_{i=0}^n\\boldsymbol{r}_i - \\boldsymbol{r}_j\\right)\\right\\}.\n\\end{align*}\n\n## Step 2 of expectation propagation\n\nConstruct an hybrid or tilted distribution \n\\begin{align*}\n h_j(\\boldsymbol{\\theta}) \\propto q_{-j}(\\boldsymbol{\\theta})L_j(\\boldsymbol{\\theta}).\n\\end{align*}\nThe resulting density is unnormalized.\n\n## Global approximation\n\nThe overall approximation is Gaussian with precision $\\mathbf{Q} = \\sum_{i=0}^n\\mathbf{Q}_i$ and linear shift $\\boldsymbol{r}=\\sum_{i=0}^n\\boldsymbol{r}_i$\n\nThese parameters are obtained by optimizing each hybrid distribution with a Gaussian.\n\nThat is, we minimize the $\\mathsf{KL}(h_j \\parallel q_j)$ at each step conditional on the other parameters.\n\n\n## Step 3 of expectation propagation\n\nCompute normalizing constant and moments\n\n\\begin{align*}\n c_j &= \\int h_j(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta} \\\\\n \\boldsymbol{\\mu}_j &= c_{j}^{-1} \\int \\boldsymbol{\\theta} h_j(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\n \\\\ \\boldsymbol{\\Sigma}_j &= c_j^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_j)(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_j)^\\top h_j(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\n\n## Comment on step 3\n\nThe normalizing constant, mean and variance in the above are written in terms of $p$-dimensional integrals.\n\nFor exponential family of distributions, we can perform dimension reduction. \n\n\nFor example, with generalized linear models, the update for hybrid $j$ depends only on the summary statistic $\\mathbf{x}_j\\boldsymbol{\\theta}$, where $\\mathbf{x}$ is the $j$th row of the model matrix. Then, the integral is one-dimensional.\n\n\n## Projection of Gaussians\n\nLinear combinations of Gaussian vectors are also Gaussian.\n\nIf $\\boldsymbol{\\beta} \\sim \\mathsf{Gauss}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}),$ then \n$$\\mathbf{x}\\boldsymbol{\\beta} \\sim \\mathsf{Gauss}(\\mathbf{x}\\boldsymbol{\\mu}, \\mathbf{x}\\boldsymbol{\\Sigma}\\mathbf{x}^\\top)$$\n\n## Step 4 of expectation propagation {.smaller}\n\n\nConvert moments $\\boldsymbol{\\mu}_j^{*}$ and $\\boldsymbol{\\Sigma}^{*}_j$ to canonical parameters $\\mathbf{Q}_j^{*}$ and $\\boldsymbol{r}_j^{*}.$\n\n\nUpdate the global approximation with\n\\begin{align*}\nq(\\boldsymbol{\\theta}) &\\propto \\exp \\left\\{ - \\frac{1}{2} \\boldsymbol{\\theta}^\\top \\left(\\sum_{\\substack{i=0 \\\\ i \\neq j}}^n\\mathbf{Q}_i + \\mathbf{Q}^{*}_j\\right)\\boldsymbol{\\theta}  + \\boldsymbol{\\theta}^\\top \\left(\\sum_{\\substack{i=0 \\\\ i \\neq j}}^n\\boldsymbol{r}_i + \\boldsymbol{r}^{*}_j\\right)\\right\\}.\n\\end{align*}\n\n## Recap of expectation propagation\n\n\nThe EP algorithm iterates the steps until convergence:\n\n1. Initialize the site-specific parameters\n2. Loop over each observation of the likelihood factorization:\n    1. form the cavity and the hybrid distribution\n    2. compute the moments of the hybrid $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$\n    3. transform back to canonical parameters $\\mathbf{Q}$ and $\\boldsymbol{r}$\n    4. update the global approximation\n3. Declare convergence when change in parameters is less than tolerance.\n\n\n## Improving convergence\n\nThere is no guarantee that the fixed-point algorithm will converge...\n\nThe algorithm behaves like a smoothed Newton method [@Dehaene.Barthelme:2018], so we can borrow tricks from numerical optimization to improve convergence.\n\n- linearly interpolate between updates, with weight $0< w\\le 1$ to the current update where at step $t$.\n\n## Comments on updates\n\nThe algorithm can be run in parallel. \n\nSome individual factor updates may yield non-positive definite precision for individual terms $\\mathbf{Q}_j$, which is okay as long as the global approximation $\\mathbf{Q}$ remains positive.\n\nVariants (power EP) leads to more smoothing, but change fixed points.\n\n## Example: EP for logistic regression\n\nConsider a binary response $Y \\in \\{-1, 1\\}$ with logistic model\n\\begin{align*}\n\\Pr(Y=1 \\mid \\mathbf{x}, \\boldsymbol{\\beta}) = \\left\\{1+\\exp(-\\mathbf{x}\\boldsymbol{\\beta})\\right\\}^{-1} = \\mathrm{expit}(\\mathbf{x}\\boldsymbol{\\beta}).\n\\end{align*}\nWe assume for simplicity that $p(\\boldsymbol{\\beta})\\propto 1;$ a Gaussian prior could also be used.\n\n## EP for logistic regression\n\nThe likelihood contribution of observation $i$ with factors $$L_i(\\boldsymbol{\\beta}) = \\mathrm{expit}(y_i \\mathbf{x}_i\\boldsymbol{\\beta}).$$ \n\nThe approximation of the factor will have parameters $(a\\mathbf{x}_i^\\top, b\\mathbf{x}_i^\\top\\mathbf{x}_i)$, which only depend on two scalars $a$ and $b.$\n\n## EP iterations\n\nThe dynamics of the expectation propagation given vectors of individual contributions $\\boldsymbol{a}^{(t)}$ and $\\boldsymbol{b}^{(t)}$ at iteration $t$ and the canonical parameters\n\\begin{align*}\n\\boldsymbol{r}^{(t)} = \\sum_{i=1}^n a_i^{(t)}\\mathbf{x}_i^\\top, \\qquad \\mathbf{Q}^{(t)} = \\sum_{i=1}^n b_i\\mathbf{x}_i^\\top\\mathbf{x}_i,\n\\end{align*} initialized to $t=0$, are as follows for step $t$:\n\n## Steps for logistic regression\n\n1. Compute the natural parameters of the cavity distribution \\begin{align*}\n\\boldsymbol{r}_{-i} &= \\boldsymbol{r} - a_i^{(t-1)}\\mathbf{x}_i^\\top\n\\\\\\mathbf{Q}_{-i} &= \\mathbf{Q} - b_i^{(t-1)}\\mathbf{x}_i^\\top\\mathbf{x}_i.\n\\end{align*}\n2. Transform the canonical parameters to moments, with $\\boldsymbol{\\Sigma}_{-i} = \\mathbf{Q}_{-i}^{-1}$ and $\\boldsymbol{\\mu}_{-i} = \\mathbf{Q}_{-i}^{-1}\\boldsymbol{r}_{-i}$.\n\n\n## Steps for logistic regression\n\n3. Obtain the mean and variance of the conditional distribution with variance $v_i = \\mathbf{x}_i\\boldsymbol{\\Sigma}_{-i}\\mathbf{x}_i^\\top$ and mean $u_i=\\mathbf{x}_i\\boldsymbol{\\mu}.$\n4. Define the marginal hybrid as\n\\begin{align*}\nh_i(z) \\propto \\mathrm{expit}(y_iz) \\exp\\left\\{-\\frac{(z-u)^2}{2v}\\right\\}.\n\\end{align*}\n\n## Steps for logistic regression\n\n5. Compute the normalizing constant, the mean $\\mathsf{E}_{h_i}(Z)$ and the variance $\\mathsf{Va}_{h_i}(Z)$ by numerical integration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' @param mu_lc mean of the linear combination\n#' @param sd_lc std. dev of the linear combination\nep_update <- function(y, mu_lc, sd_lc){\n  # Calculate outside of the loop the cavity\n  fn <- function(x){ dnorm(x, mean = mu_lc, sd = sd_lc)*plogis(y*x)}\n  # Compute normalizing constant\n  cst <- integrate(f = fn, lower = -Inf, upper = Inf)$value\n  mu <- integrate(f = function(x){fn(x)*x}, -Inf, Inf)$value/cst\n  va <- integrate(f = function(x){fn(x)*(x-mu)^2}, -Inf, Inf)$value/cst\n}\n```\n:::\n\n\n\n## Steps for logistic regression\n\n6. Back-transform the parameters to get the parameters for \\begin{align*}a_i^{(t)} &= \\mathsf{E}_{h_i}(Z)/\\mathsf{Va}_{h_i}(Z) - u/v\n\\\\ b_i^{(t)} &= 1/\\mathsf{Va}_{h_i}(Z).\n\\end{align*}\n7. Update the parameters \n\\begin{align*}\n\\boldsymbol{r} &\\gets \\boldsymbol{r}_{-i} + a_i^{(t)}\\mathbf{x}_i^\\top \\\\\\mathbf{Q} &\\gets \\mathbf{Q}_{-i} + b_i^{(t)}\\mathbf{x}_i^\\top\\mathbf{x}_i.\n\\end{align*}\n\n## Comments on EP for exponential families \n\nThere are explicit formulas for the upgrade of probit regression.\n\nThe updates to the parameters for more general exponential families are found in page 23 of @Cseke.Heskes:2011.\n\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}