{
  "hash": "abb733040b983368dc9fa287041acbea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"LÃ©o Belzile\"\nsubtitle: \"Variational inference\"\ndate: today\ndate-format: \"[Last compiled] dddd MMM D, YYYY\"\neval: true\necho: true\ncache: true\nbibliography: MATH80601A.bib\nformat:\n  revealjs:\n    slide-number: true\n    html-math-method: mathjax\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n    logo: \"fig/logo_hec_montreal_bleu_web.png\"\n---\n\n\n\n\n\n## Variational inference\n\nLaplace approximation provides a heuristic for large-sample approximations, but it fails to characterize well $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$.\n\nWe consider rather a setting where we approximate $p$ by another distribution $g$ which we wish to be close.\n\n\nThe terminology **variational** is synonym for optimization in this context.\n\n## Kullback--Leibler divergence\n\nThe Kullback--Leibler divergence between densities $f_t(\\cdot)$ and $g(\\cdot; \\boldsymbol{\\psi}),$ is\n\\begin{align*}\n\\mathsf{KL}(f_t \\parallel g) &=\\int \\log \\left(\\frac{f_t(\\boldsymbol{x})}{g(\\boldsymbol{x}; \\boldsymbol{\\psi})}\\right) f_t(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}\\\\\n&= \\int \\log f_t(\\boldsymbol{x}) f_t(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x} - \\int \\log g(\\boldsymbol{x}; \\boldsymbol{\\psi}) f_t(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}\n\\\\ &= {\\color{#c38f16}{\\mathsf{E}_{f_t}\\{\\log f_t(\\boldsymbol{X})\\}}} - \\mathsf{E}_{f_t}\\{\\log g(\\boldsymbol{X}; \\boldsymbol{\\psi})\\}\n\\end{align*}\nThe ${\\color{#c38f16}{\\text{negative entropy}}}$ does not depend on $g(\\cdot).$\n\n\n## Model misspecification\n\n\n- The divergence is strictly positive unless $g(\\cdot; \\boldsymbol{\\psi}) \\equiv f_t(\\cdot).$\n- The divergence is not symmetric.\n\n\nThe Kullback--Leibler divergence notion is central to study of model misspecification.\n\n- if we fit $g(\\cdot)$ when data arise from $f_t,$ the maximum likelihood estimator of the parameters $\\widehat{\\boldsymbol{\\psi}}$ will be the value of the parameter that minimizes the  Kullback--Leibler divergence $\\mathsf{KL}(f_t \\parallel g)$.\n\n\n\n## Marginal likelihood\n\nConsider now the problem of approximating the marginal likelihood, sometimes called the evidence,\n\\begin{align*}\np(\\boldsymbol{y}) = \\int_{\\boldsymbol{\\Theta}} p(\\boldsymbol{y}, \\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}.\n\\end{align*}\nwhere we only have the joint $p(\\boldsymbol{y}, \\boldsymbol{\\theta})$ is the product of the likelihood times the prior.\n\n## Approximating the marginal likelihood\n\nConsider $g(\\boldsymbol{\\theta};\\boldsymbol{\\psi})$ with $\\boldsymbol{\\psi} \\in \\mathbb{R}^J$ an approximating density function \n\n- whose integral is one over $\\boldsymbol{\\Theta} \\subseteq \\mathbb{R}^p$ (normalized density)\n- whose support is part of that of $\\mathrm{supp} (g) \\subseteq \\mathrm{supp}(p) = \\boldsymbol{\\Theta}$ (so KL divergence is not infinite)\n\nObjective: minimize the Kullback--Leibler divergence $$\\mathsf{KL}\\left\\{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\parallel g(\\boldsymbol{\\theta};\\boldsymbol{\\psi})\\right\\}.$$\n\n## Problems ahead\n\nMinimizing the Kullback--Leibler divergence is not feasible to evaluate the posterior.\n\nTaking $f_t = p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$ is not feasible: we need the marginal likelihood\nto compute the expectation!\n\n\n## Alternative expression for the marginal likelihood\n\n\nWe consider a different objective to bound the marginal likelihood. Write\n\n\n\\begin{align*}\np(\\boldsymbol{y}) = \\int_{\\boldsymbol{\\Theta}}  \\frac{p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{g(\\boldsymbol{\\theta};\\boldsymbol{\\psi})} g(\\boldsymbol{\\theta};\\boldsymbol{\\psi}) \\mathrm{d} \\boldsymbol{\\theta}.\n\\end{align*}\n\n## Bounding the marginal likelihood\n\n\nFor $h(x)$ a convex function, **Jensen's inequality** implies that $$h\\{\\mathsf{E}(X)\\} \\leq \\mathsf{E}\\{h(X)\\},$$ and applying this with $h(x)=-\\log(x),$ we get\n\\begin{align*}\n-\\log p(\\boldsymbol{y}) \\leq -\\int_{\\boldsymbol{\\Theta}} \\log  \\left(\\frac{p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{g(\\boldsymbol{\\theta};\\boldsymbol{\\psi})}\\right) g(\\boldsymbol{\\theta};\\boldsymbol{\\psi}) \\mathrm{d} \\boldsymbol{\\theta}.\n\\end{align*}\n\n\n## Evidence lower bound\n\nWe can thus consider the model that minimizes the **reverse Kullback--Leibler divergence**\n\\begin{align*}\ng(\\boldsymbol{\\theta}; \\widehat{\\boldsymbol{\\psi}}) = \\mathrm{argmin}_{\\boldsymbol{\\psi}} \\mathsf{KL}\\{g(\\boldsymbol{\\theta};\\boldsymbol{\\psi}) \\parallel p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\}.\n\\end{align*}\n\nSince $p(\\boldsymbol{\\theta}, \\boldsymbol{y}) = p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) p(\\boldsymbol{y})$, \n\\begin{align*}\n\\mathsf{KL}\\{g(\\boldsymbol{\\theta};\\boldsymbol{\\psi}) \\parallel p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\} &= \\mathsf{E}_{g}\\{\\log g(\\boldsymbol{\\theta})\\} - \\mathsf{E}_g\\{\\log p( \\boldsymbol{\\theta}, \\boldsymbol{y})\\} \\\\&\\quad+ \\log p(\\boldsymbol{y}).\n\\end{align*}\n\n\n## Evidence lower bound\n\nInstead of minimizing the Kullback--Leibler divergence, we can equivalently maximize the so-called **evidence lower bound** (ELBO)\n\\begin{align*}\n\\mathsf{ELBO}(g) = \\mathsf{E}_g\\{\\log p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\} - \\mathsf{E}_{g}\\{\\log g(\\boldsymbol{\\theta})\\}\n\\end{align*}\n\nThe ELBO is a lower bound for the marginal likelihood because a Kullback--Leibler divergence is non-negative and\n\\begin{align*}\n\\log p(\\boldsymbol{y}) = \\mathsf{ELBO}(g) +  \\mathsf{KL}\\{g(\\boldsymbol{\\theta};\\boldsymbol{\\psi}) \\parallel p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\}.\n\\end{align*}\n\n## Use of ELBO\n\nThe idea is that we will approximate the density $$p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\approx g(\\boldsymbol{\\theta}; \\widehat{\\boldsymbol{\\psi}}).$$\n\n- the ELBO can be used for model comparison (but we compare bounds...)\n- we can sample from $q$ as before.\n\n## Heuristics of ELBO\n\nMaximize the evidence, subject to a regularization term:\n\\begin{align*}\n\\mathsf{ELBO}(g) = \\mathsf{E}_g\\{\\log p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\} - \\mathsf{E}_{g}\\{\\log g(\\boldsymbol{\\theta})\\}\n\\end{align*}\n\nThe ELBO is an objective function comprising:\n\n- the first term will be maximized by taking a distribution placing mass near the MAP of $p(\\boldsymbol{y}, \\boldsymbol{\\theta}),$\n- the second term can be viewed as a penalty that favours high entropy of the approximating family (higher for distributions which are diffuse).\n\n## Laplace vs variational approximation\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Skewed density with the Laplace approximation (dashed orange) and variational Gaussian approximation (dotted blue).](bayesmod-slides10_files/figure-revealjs/fig-skewed-1.png){#fig-skewed width=960}\n:::\n:::\n\n\n\n\n## Choice of approximating density\n\nIn practice, the quality of the approximation depends on the choice of $g(\\cdot; \\boldsymbol{\\psi}).$\n\n- We typically want matching support.\n- The approximation will be affected by the correlation between posterior components $\\boldsymbol{\\theta} \\mid \\boldsymbol{y}.$\n- Derivations can also be done for $(\\boldsymbol{U}, \\boldsymbol{\\theta})$, where $\\boldsymbol{U}$ are latent variables from a data augmentation scheme.\n\n\n## Factorization\n\nWe can consider densities  $g(;\\boldsymbol{\\psi})$ that factorize into blocks with parameters $\\boldsymbol{\\psi}_1, \\ldots, \\boldsymbol{\\psi}_M,$ where\n\\begin{align*}\ng(\\boldsymbol{\\theta}; \\boldsymbol{\\psi}) = \\prod_{j=1}^M g_j(\\boldsymbol{\\theta}_j; \\boldsymbol{\\psi}_j)\n\\end{align*}\nIf we assume that each of the $J$ parameters $\\theta_1, \\ldots, \\theta_J$ are independent, then\nwe obtain a **mean-field** approximation.\n\n## Maximizing the ELBO one step at a time\n\n\\begin{align*}\n\\mathsf{ELBO}(g) &= \\int \\log p(\\boldsymbol{y}, \\boldsymbol{\\theta}) \\prod_{j=1}^M g_j(\\boldsymbol{\\theta}_j)\\mathrm{d} \\boldsymbol{\\theta} \\\\&\\quad- \\sum_{j=1}^M \\int \\log \\{ g_j(\\boldsymbol{\\theta}_j) \\} g_j(\\boldsymbol{\\theta}_j) \\mathrm{d}  \\boldsymbol{\\theta}_j\n \\\\& \\stackrel{\\boldsymbol{\\theta}_i}{\\propto} \\mathsf{E}_{i}\\left[\\mathsf{E}_{-i}\\left\\{\\log p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right\\} \\right] - \\mathsf{E}_i\\left[\\log \\{ g_i(\\boldsymbol{\\theta}_i) \\}\\right]\n\\end{align*}\nwhich is the negative of a Kullback--Leibler divergence.\n\n## Optimal choice of approximating density\n\nThe maximum possible value of zero for the KL is attained when $$\\log \\{ g_i(\\boldsymbol{\\theta}_i) \\} = \\mathsf{E}_{-i}\\left\\{\\log p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right\\}.$$\nThe choice of marginal $g_i$ that maximizes the ELBO is\n\\begin{align*}\n g^{\\star}_i(\\boldsymbol{\\theta}_i) \\propto \\exp \\left[ \\mathsf{E}_{-i}\\left\\{\\log p(\\boldsymbol{y}, \\boldsymbol{\\theta})\\right\\}\\right].\n\\end{align*}\nOften, we look at the kernel of $g^{\\star}_j$ to deduce the normalizing constant.\n\n\n## Coordinate-ascent variational inference (CAVI)\n\n- We can maximize $g^{\\star}_j$ in turn for each $j=1, \\ldots, M$ treating the other parameters as fixed.\n- This scheme is guaranteed to monotonically increase the ELBO until convergence to a local maximum.\n- Convergence: monitor ELBO and stop when the change is lower then some present numerical tolerance.\n- The approximation may have multiple local optima: perform random initializations and keep the best one.\n\n## Example of CAVI mean-field for Gaussian target\n\n\nWe consider the example from Section 2.2.2 of @Ormerod.Wand:2010 for approximation of a Gaussian distribution, with\n\\begin{align*}\nY_i &\\sim \\mathsf{Gauss}(\\mu, \\tau^{-1}), \\qquad i =1, \\ldots, n;\\\\\n\\mu &\\sim \\mathsf{Gauss}(\\mu_0, \\tau_0^{-1}) \\\\\n\\tau &\\sim \\mathsf{gamma}(a_0, b_0).\n\\end{align*}\nThis is an example where the full posterior is available in closed-form, so we can compare our approximation with the truth.\n\n## Variational approximation to Gaussian --- mean\n\nWe assume a factorization of the variational approximation $g_\\mu(\\mu)g_\\tau(\\tau);$ the factor for $g_\\mu$ is proportional to\n\\begin{align*}\n \\log g^{\\star}_\\mu(\\mu) \\propto -\\frac{\\mathsf{E}_{\\tau}(\\tau)}{2} \\sum_{i=1}^n (y_i-\\mu)^2-\\frac{\\tau_0}{2}(\\mu-\\mu_0)^2\n\\end{align*}\nwhich is quadratic in $\\mu$ and thus must be Gaussian with precision $\\tau_n = \\tau_0 + n\\tau$ and mean $\\tau_n^{-1}\\{\\tau_0\\mu_0 + \\mathsf{E}_{\\tau}(\\tau)n\\overline{y}\\}$\n\n## Variational approximation to Gaussian --- precision\n\nThe optimal precision factor satisfies\n\\begin{align*}\n \\ln g^{\\star}_{\\tau}(\\tau) &\\propto (a_0-1 +n/2) \\log \\tau \\\\& \\quad - \\tau {\\color{#c38f16}{\\left[b_0  + \\frac{1}{2} \\mathsf{E}_{\\mu}\\left\\{\\sum_{i=1}^n (y_i-\\mu)^2\\right\\}\\right]}}.\n\\end{align*}\nThis is of the same form as  $p(\\tau \\mid \\mu, \\boldsymbol{y}),$ namely a gamma with shape $a_n =a_0 +n/2$ and rate ${\\color{#c38f16}{b_n}}$.\n\n## Rate of the gamma for $g_\\tau$\n\nIt is helpful to rewrite the expected value as\n\\begin{align*}\n \\mathsf{E}_{\\mu}\\left\\{\\sum_{i=1}^n (y_i-\\mu)^2\\right\\} = \\sum_{i=1}^n \\{y_i - \\mathsf{E}_{\\mu}(\\mu)\\}^2 + n \\mathsf{Var}_{\\mu}(\\mu),\n\\end{align*}\nso that it depends on the parameters of the distribution of $\\mu$ directly.\n\n## CAVI for Gaussian\n\nThe algorithm cycles through the following updates until convergence:\n\n- $\\mathsf{Va}_{\\mu}(\\mu) = \\{\\tau_0 + n \\mathsf{E}_{\\tau}(\\tau)\\}^{-1},$\n- $\\mathsf{E}_{\\mu}(\\mu) = \\mathsf{Va}_{\\mu}(\\mu)\\{\\tau_0\\mu_0 + \\mathsf{E}_{\\tau}(\\tau)n \\overline{y}\\},$\n- $\\mathsf{E}_{\\tau}(\\tau) = a_n/b_n$ where $b_n$ is a function of both $\\mathsf{E}_{\\mu}(\\mu)$ and $\\mathsf{Var}_{\\mu}(\\mu).$\n\nWe only compute the ELBO at the end of each cycle.\n\n## Monitoring convergence\n\nThe derivation of the ELBO is straightforward but tedious; we only need to monitor\n\\begin{align*}\n- \\frac{\\tau_0}{2} \\mathsf{E}_{\\mu}\\{(\\mu - \\mu_0)^2\\} - \\frac{\\log\\tau_n}{2}-a_n\\log b_n\n\\end{align*}\nfor convergence, although other normalizing constants would be necessary if we wanted to approximate the marginal likelihood.\n\nWe can also consider relative changes in parameter values as tolerance criterion.\n\n## Stochastic optimization\n\n\nWe consider alternative numeric schemes which rely on stochastic optimization [@Hoffman:2013].\n\nThe key idea behind these methods is that\n\n- we can use gradient-based algorithms,\n- and approximate the expectations with respect to $g$ by drawing samples from it\n\nAlso allows for minibatch (random subset) selection to reduce computational costs in large samples\n\n## Black-box variational inference\n\n@Ranganath.Gerrish.Blei:2014 shows that the gradient of the ELBO reduces to\n\\begin{align*}\n \\frac{\\partial}{\\partial \\boldsymbol{\\psi}} \\mathsf{ELBO}(g) &=\\mathsf{E}_{g}\\left\\{\\frac{\\partial \\log g(\\boldsymbol{\\theta}; \\boldsymbol{\\psi})}{\\partial \\boldsymbol{\\psi}} \\times \\log \\left( \\frac{p(\\boldsymbol{\\theta}, \\boldsymbol{y})}{g(\\boldsymbol{\\theta}; \\boldsymbol{\\psi})}\\right)\\right\\}\n\\end{align*}\nusing the change rule, differentiation under the integral sign (dominated convergence theorem) and the identity\n\\begin{align*}\n\\frac{\\partial \\log g(\\boldsymbol{\\theta}; \\boldsymbol{\\psi})}{\\partial \\boldsymbol{\\psi}} g(\\boldsymbol{\\theta}; \\boldsymbol{\\psi}) = \\frac{\\partial g(\\boldsymbol{\\theta}; \\boldsymbol{\\psi})}{\\partial \\boldsymbol{\\psi}}\n\\end{align*}\n\n## Black-box variational inference in practice\n\n- Note that the gradient simplifies for $g_i$ in exponential families (covariance of sufficient statistic with $\\log(p/g)$).\n- The gradient estimator is particularly noisy, so @Ranganath.Gerrish.Blei:2014 provide two methods to reduce the variance of this expression using control variates and Rao--Blackwellization.\n\n## Automatic differentiation variational inference\n@Kucukelbir:2017 proposes a stochastic gradient algorithm, but with two main innovations.\n\n- The first is the general use of Gaussian approximating densities for factorized density, with parameter transformations to map from the support of $T: \\boldsymbol{\\Theta} \\mapsto \\mathbb{R}^p$ via $T(\\boldsymbol{\\theta})=\\boldsymbol{\\zeta}.$\n- The second is to use the resulting **location-scale** family to obtain an alternative form of the gradient.\n\n## Gaussian full-rank approximation\n\nConsider an approximation $g(\\boldsymbol{\\zeta}; \\boldsymbol{\\psi})$ where $\\boldsymbol{\\psi}$ consists of\n\n- mean parameters $\\boldsymbol{\\mu}$ and\n- covariance $\\boldsymbol{\\Sigma}$, parametrized through a Cholesky decomposition\n\nThe full approximation is of course more flexible when the transformed parameters $\\boldsymbol{\\zeta}$ are correlated, but is more expensive to compute than the mean-field approximation.\n\n## Change of variable\n\nThe change of variable introduces a Jacobian term $\\mathbf{J}_{T^{-1}}(\\boldsymbol{\\zeta})$ for the approximation to the density $p(\\boldsymbol{\\theta}, \\boldsymbol{y})$, where\n\n\\begin{align*}\np(\\boldsymbol{\\theta}, \\boldsymbol{y}) = p(\\boldsymbol{\\zeta}, \\boldsymbol{y}) \\left|\\mathbf{J}_{T^{-1}}(\\boldsymbol{\\zeta})\\right|\n\\end{align*}\n\n## Gaussian entropy\n\nThe entropy of the multivariate Gaussian with mean $\\boldsymbol{\\mu}$ and covariance $\\boldsymbol{\\Sigma} = \\mathbf{LL}^\\top$, where $\\mathbf{L}$ is a lower triangular matrix, is\n\\begin{align*}\n \\mathcal{E}(\\mathbf{L}) = - \\mathsf{E}_g(\\log g) &= \\frac{D+D\\log(2\\pi) + \\log |\\mathbf{LL}^\\top|}{2},\n\\end{align*}\nand only depends on $\\boldsymbol{\\Sigma}$.\n\n## ELBO with Gaussian approximation\n\nSince the Gaussian is a location-scale family, we can rewrite the model in terms of a standardized Gaussian variable $\\boldsymbol{Z}\\sim \\mathsf{Gauss}_p(\\boldsymbol{0}_p, \\mathbf{I}_p)$ where $\\boldsymbol{\\zeta} = \\boldsymbol{\\mu} + \\mathbf{L}\\boldsymbol{Z}$ (this transformation has unit Jacobian).\n\nThe ELBO with the transformation becomes\n\\begin{align*}\n \\mathsf{E}_{\\boldsymbol{Z}}\\left[ \\log p\\{\\boldsymbol{y}, T^{-1}(\\boldsymbol{\\zeta})\\} + \\log \\left|\\mathbf{J}_{T^{-1}}(\\boldsymbol{\\zeta})\\right|\\right] + \\mathcal{E}(\\mathbf{L}).\n\\end{align*}\n\n## Chain rule\n\nIf $\\boldsymbol{\\theta} = T^{-1}(\\boldsymbol{\\zeta})$ and $\\boldsymbol{\\zeta} = \\boldsymbol{\\mu} + \\mathbf{L}\\boldsymbol{z},$ we have for $\\boldsymbol{\\psi}$ equal to either $\\boldsymbol{\\mu}$ or $\\mathbf{L}$, using the chain rule,\n\\begin{align*}\n & \\frac{\\partial}{\\partial \\boldsymbol{\\psi}}\\log p(\\boldsymbol{y}, \\boldsymbol{\\theta})\n \\\\&\\quad  = \\frac{\\partial \\log p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n   \\times \\frac{\\partial T^{-1}(\\boldsymbol{\\zeta})}{\\partial \\boldsymbol{\\zeta}}\n\\times \\frac{\\partial (\\boldsymbol{\\mu} + \\mathbf{L}\\boldsymbol{z})}{\\partial \\boldsymbol{\\psi}}\n\\end{align*}\n\n## Gradients for ADVI {.smaller}\n\nThe gradients of the ELBO with respect to the mean and variance are\n\\begin{align*}\n \\nabla_{\\boldsymbol{\\mu}} &= \\mathsf{E}_{\\boldsymbol{Z}}\\left\\{\\frac{\\partial \\log p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} \\frac{\\partial T^{-1}(\\boldsymbol{\\zeta})}{\\partial \\boldsymbol{\\zeta}}  + \\frac{\\partial \\log \\left|\\mathbf{J}_{T^{-1}}(\\boldsymbol{\\zeta})\\right|}{\\partial \\boldsymbol{\\zeta}}\\right\\} \\\\\n \\nabla_{\\mathbf{L}} &= \\mathsf{E}_{\\boldsymbol{Z}}\\left[\\left\\{\\frac{\\partial \\log p(\\boldsymbol{y}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} \\frac{\\partial T^{-1}(\\boldsymbol{\\zeta})}{\\partial \\boldsymbol{\\zeta}}  + \\frac{\\partial \\log \\left|\\mathbf{J}_{T^{-1}}(\\boldsymbol{\\zeta})\\right|}{\\partial \\boldsymbol{\\zeta}}\\right\\}\\boldsymbol{Z}^\\top\\right] + \\mathbf{L}^{-\\top}.\n\\end{align*}\nand we can approximate the expectation by drawing standard Gaussian samples $\\boldsymbol{Z}_1, \\ldots, \\boldsymbol{Z}_B.$\n\n## Quality of approximation\n\nConsider the stochastic volatility model.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](fig/stochvol-volatility.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\nFitting HMC-NUTS to the exchange rate data takes 156 seconds for 10K iterations, vs 2 seconds for the mean-field approximation.\n\n\n\n\n\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}