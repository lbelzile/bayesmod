{
  "hash": "9d58a39f5b61f93949a23cb60529fb81",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"LÃ©o Belzile\"\nsubtitle: \"Bayesian regression\"\ndate: today\ndate-format: \"[Last compiled] dddd MMM D, YYYY\"\neval: true\necho: true\ncache: true\nbibliography: MATH80601A.bib\nformat:\n  revealjs:\n    slide-number: true\n    html-math-method: mathjax\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n    logo: \"fig/logo_hec_montreal_bleu_web.png\"\n---\n\n\n\n\n\n## Bayesian regression\n\nSame, same, but different...\n\n- Generalized linear models, with distributional assumptions and link functions.\n- We assign priors to $\\boldsymbol{\\beta}$ which\n   - can provide shrinkage (regularization towards zero)\n   - can enable variable selection (spike and slab)\n\n## Model setup\n\nConsider regression models with \n\n- model (or design) matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ \n- regression coefficients $\\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_p)^\\top \\in \\mathbb{R}^p$\n\n## Ordinary linear regression model.\n\nIn the ordinary linear regression model, observations are independent and homoscedastic and \n\\begin{align*}\n\\boldsymbol{Y} \\mid \\mathbf{X}, \\boldsymbol{\\beta}, \\omega \\sim \\mathsf{Gauss}_n(\\beta_0\\mathbf{1}_n + \\mathbf{X}\\boldsymbol{\\beta}, \\omega^{-1}\\mathbf{I}_n).\n\\end{align*}\n\nThe intercept $\\beta_0$ receives special treatment, is always included. It is typically assigned an improper prior $p(\\beta_0) \\propto 1.$\n\n\n\n\n## Decomposition of quadratic forms\n\nFor quadratic forms (in $\\boldsymbol{x}$) with\n\\begin{align*}\n& (\\boldsymbol{x} - \\boldsymbol{a})^\\top \\mathbf{A}(\\boldsymbol{x} - \\boldsymbol{a}) + (\\boldsymbol{x} - \\boldsymbol{b})^\\top \\mathbf{B}(\\boldsymbol{x} - \\boldsymbol{b}) \\\\\n&\\stackrel{\\boldsymbol{x}}{\\propto} (\\boldsymbol{x} - \\boldsymbol{c})^\\top \\mathbf{C}(\\boldsymbol{x} - \\boldsymbol{c})\n\\end{align*}\nwhere $\\mathbf{C} = \\mathbf{A} + \\mathbf{B}$ and $\\boldsymbol{c}= \\mathbf{C}^{-1}(\\mathbf{A}\\boldsymbol{a} + \\mathbf{B}\\boldsymbol{b})$.\n\nThis is useful to complete the square in Gaussian-Gaussian models.\n\n## Bayesian Gaussian linear model\n\nConsider Gaussian-gamma **conjugate** priors for the mean and precision parameters $\\boldsymbol{\\beta}$ and $\\omega$, \n\\begin{align*}\n\\boldsymbol{\\beta} \\mid \\omega &\\sim \\mathsf{Gauss}\\left\\{\\boldsymbol{\\mu}_0, (\n\\omega\\boldsymbol{\\Omega}_0)^{-1}\\right\\} \\\\\n\\omega &\\sim \\mathsf{gamma}(\\nu_0/2,\\tau_0/2).\n\\end{align*}\nRecall the sampling distribution of the ordinary least squares estimator is $$\\widehat{\\boldsymbol{\\beta}} \\sim \\mathsf{Gauss}_p\\{\\boldsymbol{\\beta}, (\\omega\\mathbf{X}^\\top\\mathbf{X})^{-1}\\}.$$\n\n## Conditional distributions\n\nThe conditional and marginal posterior distributions for the mean coefficients $\\boldsymbol{\\beta}$ and for the precision $\\omega$ are\n\\begin{align*}\n\\boldsymbol{\\beta} \\mid \\omega, \\boldsymbol{y} &\\sim \\mathsf{Gauss}_p\\left\\{\\boldsymbol{\\mu}_n, (\\omega\\boldsymbol{\\Omega}_n)^{-1}\\right\\}  \\\\\n\\omega \\mid  \\boldsymbol{y} &\\sim \\mathsf{gamma}\\left\\{(\\nu_0 + n)/2,  \\tau^2_n/2\\right\\}.\n\\end{align*}\n\nIf we integrate over the precision, we get instead\n\\begin{align*}\n\\boldsymbol{\\beta} \\mid  \\boldsymbol{y} &\\sim \\mathsf{Student}_p(\\boldsymbol{\\mu}_n,  \\tau_n/(\\nu_0+n) \\times \\mathbf{\\Omega}_n^{-1}, \\nu_0 + n)\n\\end{align*}\n\n## Posterior parameters\n\n\nThe precision is the sum of the precision of OLS estimator and prior precision.\n\nThe posterior mean is a weighted combination of the prior and OLS means, weighted by the scaled precision.\n\\begin{align*}\n\\boldsymbol{\\Omega}_n &= \\mathbf{X}^\\top\\mathbf{X} + \\boldsymbol{\\Omega}_0\\\\\n\\boldsymbol{\\mu}_n &= \\boldsymbol{\\Omega}_n^{-1}(\\mathbf{X}^\\top\\mathbf{X}\\widehat{\\boldsymbol{\\beta}} + \\boldsymbol{\\Omega}_0\\boldsymbol{\\mu}_0) = \\boldsymbol{\\Omega}_n^{-1}(\\mathbf{X}^\\top\\boldsymbol{y} + \\boldsymbol{\\Omega}_0\\boldsymbol{\\mu}_0)\\\\\n\\tau_n &= \\tau_0 + (\\boldsymbol{y} - \\mathbf{X}\\widehat{\\boldsymbol{\\beta}})^\\top(\\boldsymbol{y} - \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}) + (\\boldsymbol{\\mu}_n - \\widehat{\\boldsymbol{\\beta}})^\\top \\mathbf{X}^\\top\\mathbf{X}(\\boldsymbol{\\mu}_n - \\widehat{\\boldsymbol{\\beta}}) \\\\& \\quad + (\\boldsymbol{\\mu}_n-\\boldsymbol{\\mu}_0)^\\top\\boldsymbol{\\Omega}_0(\\boldsymbol{\\mu}_n-\\boldsymbol{\\mu}_0)\n\\end{align*}\n\n\n## Scale mixture of Gaussians\n\nIf $X \\mid \\sigma^2 \\sim \\mathsf{Gauss}(0, \\sigma^2)$ and we assign a prior $p(\\sigma^2)$\n\n- if $\\sigma^2 \\sim \\mathsf{inv. gamma}(\\nu/2, \\nu/2)$, then $X \\sim \\mathsf{Student}(0,1, \\nu)$\n- if $\\sigma^2 \\sim \\mathsf{exp}(1/\\lambda^2),$ then\n$X \\sim \\mathsf{Laplace}(0, \\lambda).$\n\n## Sketch of proof \n\n1. write down the joint posterior as\n\\begin{align*}\n p(\\boldsymbol{\\beta}, \\omega \\mid \\boldsymbol{y}) &\\propto p(\\boldsymbol{y} \\mid \\boldsymbol{\\beta}, \\omega) p(\\omega)\n\\end{align*}\n2. rewrite the first quadratic form in $\\boldsymbol{y}-\\mathbf{X}\\boldsymbol{\\beta}$ using the orthogonal decomposition\n\\begin{align*}\n (\\boldsymbol{y}-\\mathbf{X}\\widehat{\\boldsymbol{\\beta}}) + (\\mathbf{X}\\widehat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta})\n\\end{align*}\n3. pull terms  together and separate the conditional posterior $p(\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\omega)$ and $p(\\omega \\mid \\boldsymbol{y})$\n\n## Sketch of proof (continued)\n\n4. use decomposition of quadratic forms with $\\boldsymbol{a} = \\widehat{\\boldsymbol{\\beta}}$, $\\mathbf{A}=\\mathbf{X}^\\top\\mathbf{X}$, $\\boldsymbol{b} = \\boldsymbol{\\mu}_0$ and $\\mathbf{B}=\\boldsymbol{\\Omega}_0$\n5. the marginal of $\\boldsymbol{\\beta}$ is obtained by regrouping all terms that depend on $\\omega$ and integrating over the latter, recognizing the integral as an unnormalized gamma density\n\n## Cultural appropriation\n\nStudy 4 of @Lin.Kim.Uduehi.Keinan:2024 is a 3 by 2 by 2 three-way between-subject ANOVA focusing on cultural appropriation using a fictional scenario on publication of a soul food recipe cookbook from Chef Dax.\n\n## Experimental variables\n\n- ethnicity: chef is African-American or not\n- action:  the way he obtained the recipes (by peeking without permission in kitchens, by asking permission or without mention (control)\n- political ideology of respondant (liberal or conservative). \n\n\n\n\n\n## Posterior densities for marginal effects\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Difference in appropriation rating for black vs non-black Chef Dax, average accross different levels of brand action.](bayesmod-slides8_files/figure-revealjs/fig-contrasts-1.png){#fig-contrasts width=960}\n:::\n:::\n\n\n\n## Chef Dax and cultural appropriation\n\nThe coefficients and standard errors from the linear regression are very nearly similar to the posterior mean and standard deviations for $\\boldsymbol{\\beta}$ from the marginal Student-$t,$ owing to the large sample size and uninformative priors.\n\nOn average, liberals perceive cultural appropriation more strongly (with nearly 2 points more), than conservatives (0.7 points on average).\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## Modelling random effects\n\nGaussian mixed models in frequentist statistics are of the form\n\\begin{align*}\n\\boldsymbol{Y} \\mid \\mathcal{B}=\\boldsymbol{b} &\\sim \\mathsf{Gauss}_n\\left(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{b}, \\sigma^2 \\mathbf{I}_n\\right)\\\\\n\\mathcal{B} &\\sim \\mathsf{Gauss}_q(\\boldsymbol{0}_q, \\boldsymbol{\\Psi}).\n\\end{align*}\n\nBayesians also assign priors to $\\boldsymbol{\\beta}$ as well! but typically apriori independent with  $\\mathsf{Va}_{\\boldsymbol{\\beta}}(\\boldsymbol{\\beta}) \\propto \\mathbf{I}_p.$\n\n## Prior for covariance matrices\n\nWe need a prior for $p \\times p$  symmetric positive definite  matrix random matrices!\n\nWe consider two cases: \n\n- Wishart (precision) / inverse Wishart (covariance), the conjugate prior for Gaussian\n- onion peel prior on the correlation matrix\n\n\nWishart allows for conjugacy, but has unintuitive properties.\n\n## Wishart distribution\n\nWe say $\\mathbf{Q} \\sim \\mathsf{Wishart}_p(\\nu, \\mathbf{S})$ for $\\nu>0$ degrees of freedom and scale $\\mathbf{S}$ if it's density is proportional to\n\\begin{align*}\n f(\\boldsymbol{Q}) \\stackrel{\\boldsymbol{Q}}{\\propto} |\\boldsymbol{Q}|^{(\\nu-p-1)/2}\\exp\\left\\{-\\frac{\\mathrm{tr}(\\mathbf{S}^{-1}\\boldsymbol{Q})}{2}\\right\\}, \\quad \\nu > p-1.\n\\end{align*}\nwhere $|\\cdot|$ denotes the determinant of the matrix and $\\mathrm{tr}(\\cdot)$ the trace operator. \n\n## Wishart distribution\n\nThe Wishart arises from considering $n \\geq p$ independent and identically distributed mean zero Gaussian vectors $\\boldsymbol{Y}_i \\sim \\mathsf{Gauss}_p(\\boldsymbol{0}_p, \\mathbf{S})$, where\n\\begin{align*}\n \\sum_{i=1}^{\\nu} \\boldsymbol{Y}_i\\boldsymbol{Y}_i^\\top \\sim \\mathsf{Wishart}_p(\\nu, \\mathbf{S}).\n\\end{align*}\n\n## Prior elicitation for Wishart\n\nFor prior elicitation, the mean of the Wishart is $\\nu \\mathbf{S}$\n\n- $\\nu$ is thus a prior sample size\n- $\\mathbf{S}$ is a scale matrix, often the identity matrix.\n\n## Inverse Wishart\n\nConsider a prior for the covariance matrix $\\boldsymbol{\\Sigma} = \\boldsymbol{Q}^{-1}$. Applying the change of variable formula, we get Jacobian $|\\boldsymbol{\\Sigma}|^{p+1}$, and so $\\boldsymbol{\\Sigma} \\sim \\mathsf{inv. Wishart}(\\nu, \\mathbf{S}^{-1}),$ with density \\begin{align*}\n p(\\boldsymbol{\\Sigma}) \\propto |\\boldsymbol{\\Sigma}|^{-(\\nu+p+1)/2} \\exp\\left\\{-\\frac{1}{2} \\mathrm{tr}\\left(\\boldsymbol{S}^{-1}\\boldsymbol{\\Sigma}^{-1}\\right)\\right\\}\n\\end{align*}\nwith expectation $\\mathbf{S}^{-1}(\\nu-p-1)$ for $\\nu > p+1.$\n\n\n## Wishart as conjugate prior in Gaussian model\n\nConsider $\\boldsymbol{\\mu} \\sim \\mathsf{Gauss}_p(\\boldsymbol{\\mu}_0, \\boldsymbol{Q}^{-1})$ and $\\boldsymbol{Q} \\sim \\mathsf{Wishart}_p(\\nu, \\mathbf{S})$ for $\\nu \\geq p$. Then,\n\\begin{align*}\n p(\\boldsymbol{Q} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\mu}_0) \\propto &  |\\boldsymbol{Q}|^{(\\nu-p-1)/2}\\exp\\{-\\mathrm{tr}(\\mathbf{S}^{-1}\\boldsymbol{Q})/2\\} \\\\ &\\times |\\boldsymbol{Q}|^{1/2} \\exp \\left\\{ -\\frac{1}{2} (\\boldsymbol{\\mu}-\\boldsymbol{\\mu}_0)^\\top\\boldsymbol{Q}(\\boldsymbol{\\mu}-\\boldsymbol{\\mu}_0)\\right\\} \n\\end{align*}\nand thus \n$$ \\boldsymbol{Q} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\mu}_0 \\sim \\mathsf{Wishart}_p\\{\\nu + 1/2, \\boldsymbol{S} + (\\boldsymbol{\\mu}-\\boldsymbol{\\mu}_0)(\\boldsymbol{\\mu}-\\boldsymbol{\\mu}_0)^\\top\\}.$$ \n\n## Conjugacy\n\nNote that a $1 \\times 1$ matrix is equal to it's trace, and the trace operator is invariant to cyclic of it's argument, meaning that\n\\begin{align*}\n (\\boldsymbol{\\mu}-\\boldsymbol{\\mu}_0)^\\top\\boldsymbol{Q}(\\boldsymbol{\\mu}-\\boldsymbol{\\mu}_0) = \\mathrm{tr}\\left\\{ \\boldsymbol{Q}(\\boldsymbol{\\mu}-\\boldsymbol{\\mu}_0)(\\boldsymbol{\\mu}-\\boldsymbol{\\mu}_0)^\\top\\right\\}.\n\\end{align*}\n\n## Properties of Wishart\n\nThe marginal precision for the Wishart variate are gamma distributed with the same degrees of freedom $\\nu$. \n\n- there is a single parameter governing all marginal variances.\n\nMoreover, the absolute value of the correlation and marginal variance parameters are negatively related [@Gelman:2013]. Large variance thus correspond to small correlations shrunk towards zero when the degrees of freedom increase.\n\n## Wishart draws\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Prior draws from a bivariate inverse Wishart with identity scale matrix and $\\nu \\in \\{3, 20\\}$ degrees of freedom.](bayesmod-slides8_files/figure-revealjs/fig-draws-Wishart-1.png){#fig-draws-Wishart width=960}\n:::\n:::\n\n\n\n## Onion peel prior\n\n\nA better alternative is to specify \n\n- different prior for each marginal scale $\\sigma_j$ and \n- a prior on the correlation matrix $\\mathbf{R}.$\n\n\nFor the latter, the onion peel or LKJ prior, named after the authors of @Lewandowski:2009, is $$p(\\mathbf{R}) \\propto |\\mathbf{R}|^{\\eta-1}, \\eta>0$$ \n\nThe case $\\eta=1$ leads to uniform over the space of correlation matrices, and $\\eta>1$ favours the identity matrix.\n\n\n\n## Shrinkage and variable selection\n\n\nWith $p$ covariates, there are $2^p$ potential regression models. \n\nThis is too many models to explore for $p$ large, and too many parameters relative to sample size $n$.\n\nTwo solutions:\n\n\n- shrinkage priors: penalize small coefficients by shrinking towards zero via priors on $\\boldsymbol{\\beta}$\n- Bayesian model averaging: assign prior to each model (different sets of covariates $\\mathbf{X}$) and get a mixture of models.\n\n\n\n\n## Spike-and-slab prior\n\nThe **discrete spike-and-slab prior** [@Mitchell.Beauchamp:1988] is a two-component mixture with\n\n- the spike: a point mass $\\delta_0$ or a vary narrow distribution centered at zero\n- the slab, a diffuse distribution.\n\n\\begin{align*}\n\\beta_j \\mid \\gamma_j, \\sigma^2 \\sim \\gamma_j \\delta_0 + (1-\\gamma_j)\\mathsf{Gauss}(0, \\sigma^2)\n\\end{align*}\n\n\n## Prior for the spike-and-slab prior probability\n\nSet independent and identically distributed conjugate prior for $\\gamma_j \\sim \\mathsf{binom}(1, \\omega),$ whence\n\\begin{align*}\np(\\boldsymbol{\\gamma} \\mid \\omega) = \\prod_{j=1}^n \\omega^{\\gamma_j} (1-\\omega)^{1-\\gamma_j}\n\\end{align*}\n\nApriori, we set $\\omega \\in (0,1)$ as the proportion of the $p$ coefficients $\\boldsymbol{\\beta}$ that are zero (so $p(1-\\omega)$ nonzero coefficients).\n\n\n\n## Continuous spike-and-slab prior\n\n@George.McCulloch:1993 replaced the spike by a Gaussian with near infinite precision around zero, with\n\\begin{align*}\n \\beta_j \\mid \\gamma_j, \\sigma_j^2,\\phi^2_j \\sim  \\gamma_j \\mathsf{Gauss}(0, \\sigma_j^2\\phi^2) + (1-\\gamma_j) \\mathsf{Gauss}(0, \\sigma^2_j)\n\\end{align*}\nwhere $\\phi^2_j$ is very nearly zero, typically $\\phi_j^2=0.001$. \n\nThe construction allows for variable augmentation with mixture indicators and Gibbs sampling, although mixing tends to be poor.\n\n## Horseshoe prior\n\n\nThe horseshoe prior of @Carvalho.Polson.Scott:2010 is a hierarchical prior of the form \n\\begin{align*}\n\\beta_j \\mid \\sigma^2_j &\\sim \\mathsf{Gauss}(0, \\sigma^2_j),\\\\\\sigma^2_j \\mid \\lambda  &\\sim \\mathsf{Student}_{+}(0, \\lambda, 1),\\\\ \\lambda &\\sim \\mathsf{Student}_{+}(0, \\omega, 1)\n\\end{align*}\nwhere $\\mathsf{Student}_{+}(0, a, 1)$ denotes a half-Cauchy distribution with scale $a>0,$ truncated on $\\mathbb{R}_{+}.$\n\n\n## Understanding shrinkage priors\n\nThe choice of $\\sigma^2_j$ leads to an unconditional scale mixture of Gaussian for $\\beta_j$. \n\nBetter is to consider \n$$\\kappa = 1 - 1/(1+\\sigma^2) \\in [0,1].$$\nPenalization of near-zero components can be deduced from the density of $\\kappa \\to 0$, and similarly penalization of large signals by looking at the density when $\\kappa \\to 1.$\n\n\n\n\n## Shrinkage weights\n \nWeighting implied by Gaussian mixture density with Cauchy/Laplace/horsehoe.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Density of penalization weights $\\kappa$ of spike (near zero) and slab (near one) for three shrinkage priors.](bayesmod-slides8_files/figure-revealjs/fig-weights-shrinkage-1.png){#fig-weights-shrinkage width=960}\n:::\n:::\n\n\n\n## Comparison of shrinkage priors\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Marginal density for a regression coefficient $\\beta$ with horseshoe prior (full), Laplace (dashed) and a Student-$t$ (thick dotted).](bayesmod-slides8_files/figure-revealjs/fig-shrinkage-1.png){#fig-shrinkage width=960}\n:::\n:::\n\n\n\n## Comments about the horseshoe\n\nWhile the horseshoe prior guarantees that large coefficients are not regularized, this feature of the shrinkage prior is harmful in certain instances, for example separation of variables for logistic regression.\n\n\nMarkov chain Monte Carlo simulations are hampered by these parameters whose posterior mean does not exist, leading to poor mixing. \n\n## Finnish horseshoe (aka regularized horseshoe)\n\n@Piironen.Vehtari:2017 proposed instead\n\\begin{align*}\n\\beta_j \\mid \\lambda, \\tau_j, c^2 &\\sim \\mathsf{Gauss}\\left(0, \\lambda\\frac{c^2\\tau_j^2}{c^2 + \\tau^2_j\\lambda^2}\\right), \\\\\n\\tau_j &\\sim \\mathsf{Student}_{+}(0, 1, 1)\\\\\nc^2 \\mid s^2, \\nu &\\sim \\mathsf{inv. gamma}(\\nu/2, \\nu s^2/2).\n\\end{align*}\n\n\n## Shrinkage for Finnish horsshoe\n\nWhen $\\tau^2\\lambda^2_j$ is much greater than $c^2$, this amounts to having a Student slab with $\\nu$ degrees of freedom for large coefficients.\n\nTaking a small value of $\\nu$ allows for large, but not extreme components, and the authors use $s^2=2, \\nu=4.$ \n\n## Hyperprior for Finnish horseshoe\n\nThe above specification does not specify the prior for the global scale $\\lambda$, for which @Piironen.Vehtari:2017 recommend $$\\lambda \\sim \\mathsf{Student}_{+}\\left\\{0, \\frac{p_0}{(p-p_0)}\\frac{\\sigma}{n^{1/2}}, 1\\right\\},$$ where $p_0$ is a prior guess for the number of non-zero components out of $p,$ $n$ is the sample size and $\\sigma$ is some level of the noise.\n\n\n## Comparison of shrinkage priors\n\nWe revisit the `diabetes` data from the **R** package `lars`, which was used in @Park.Casella:2008 to illustrate the Bayesian LASSO. We consider three methods: \n\n- the default Gaussian prior, which gives a ridge penalty, \n- the Bayesian LASSO of @Park.Casella:2008\n- the horseshoe prior.\n\nModels are fitted using the `bayesreg` package.\n\n## Density estimates of ordered coefficients \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Density estimates for regression coefficients with Gaussian (ridge), double exponential (Laplace) and horseshoe priors for the `diabetes` data.](bayesmod-slides8_files/figure-revealjs/fig-shrinkage-dens-comparison-1.png){#fig-shrinkage-dens-comparison width=960}\n:::\n:::\n\n\n\n## Comments on penalization\n\n- Ridge has the widest intervals of all methods, providing some shrinkage only for large values of $\\beta$. \n- The horseshoe has typically narrower intervals, with more mass in a neighborhood of zero for smaller coefficients, and asymmetric intervals.\n- The effective sample size fraction relative to the number of samples ranges from 11% to 85%, compared to 54% to 100% for the Bayesian LASSO and near-independent draws with the conjugate ridge.\n\n## Bayesian model averaging\n\nBMA refers to situation where we specify a mixture of models $M_1, \\ldots,$, and we wish to recover the posterior weights of these.\n\nThis is useful for predictions (ensemble) methods to account for uncertainty in variable selection.\n\nWe consider design of MCMC for moving between models.\n\n## Reversible jump MCMC\n\nReversible jump Markov chain Monte Carlo [@Green:1995] is an extension of the classical Metropolis--Hastings scheme that allows for arbitrary measures and through this varying dimensions.\n\nVarying dimensions occurs not only with variable selection, but also changepoint analysis and mixture models with varying number of components.\n\n\nReversible jump requires **dimension-balancing**  and defining different types of moves for jumping between dimensions. \n\n## Dimension changes and jacobians\n\nDimensions changes are integrated in the Metropolis--Hastings step through a Jacobian term $J$: the probability of rejection $R$ for Metropolis becomes\n\\begin{align*}\n    R = J\\frac{p(\\boldsymbol{\\theta}_t^{\\star})}{p(\\boldsymbol{\\theta}_{t-1})}\\frac{q(\\boldsymbol{\\theta}_{t-1} \\mid \\boldsymbol{\\theta}_t^{\\star} )}{q(\\boldsymbol{\\theta}_t^{\\star} \\mid \\boldsymbol{\\theta}_{t-1})}\n\\end{align*}\n\nIn regression models, we will consider moves that adds or removes one parameter/regressor at a time.\n\n## Setup\n\nWe consider models $M_1, \\ldots, M_m$ with for simplicity $p(M_i)=1$ for all models that include an intercept.\n\nWe write $|M|$ for the cardinality of the set of non-zero coefficients $\\boldsymbol{\\beta}$ in model $M.$\n\nDefine $\\mathbf{X}^{(m)}$ and $\\boldsymbol{\\beta}^{(m)}$ as the model matrix and the associated vector of non-zero coefficients associated with model $M_m$\n\n## Setup for regression \n\\begin{align*}\n\\boldsymbol{Y} \\mid M_m, \\boldsymbol{\\beta}, \\sim \\mathsf{Gauss}(\\mathbf{X}^{(m)}\\boldsymbol{\\beta}^{(m)}, \\sigma^2 \\mathbf{I}_n).\n\\end{align*}\n\nWe assign a Gaussian prior on $\\boldsymbol{\\beta}^{(m)} \\mid M_m,$ is assigned a Gaussian prior, etc.\n\n## Conditional Bayes factor\n\n\nWrite $\\boldsymbol{\\theta}$ for all parameters other than the response, model and vector of coefficients.\n\nWe can consider a joint update of the regression parameters $\\boldsymbol{\\beta}, M$ by sampling from their joint distribution via $$p(\\boldsymbol{\\beta} \\mid M, \\boldsymbol{\\theta}) p(M \\mid \\boldsymbol{\\theta}).$$ The update for $p(\\boldsymbol{\\beta} \\mid M, \\boldsymbol{\\theta})$ is as usual.\n\n## Update for model\n\nThe conditional Bayes factor\n\\begin{align*}\n p(M \\mid \\boldsymbol{Y}, \\boldsymbol{\\theta}) &\\stackrel{M}{\\propto} p(M) p(\\boldsymbol{Y} \\mid M, \\boldsymbol{\\theta})\n \\\\&= p(M) \\int_{\\mathbb{R}^{\\mathbb{|M|}}}p(\\boldsymbol{Y} \\mid M, \\boldsymbol{\\beta},\\boldsymbol{\\theta}) p(\\boldsymbol{\\beta} \\mid M, \\boldsymbol{\\theta}) d \\boldsymbol{\\beta}\n\\end{align*}\n\n## Marginalization\n\nWe can  thus marginalize over $\\boldsymbol{\\beta}$ to get\n\\begin{align*}\np(\\boldsymbol{M} \\mid \\boldsymbol{Y}, \\boldsymbol{\\theta}) \\propto p(M) |\\boldsymbol{Q}_{\\boldsymbol{\\beta}}|^{-1/2}\\exp\\left( \\frac{1}{2} \\boldsymbol{\\mu}_{\\boldsymbol{\\beta}}^\\top\\boldsymbol{Q}_{\\boldsymbol{\\beta}} \\boldsymbol{\\mu}_{\\boldsymbol{\\beta}}\\right)\n\\end{align*}\nwhere $\\boldsymbol{\\mu}_{\\boldsymbol{\\beta}}$ and $\\boldsymbol{Q}_{\\boldsymbol{\\beta}}$ are the mean and precision of $p(\\boldsymbol{\\beta} \\mid \\boldsymbol{Y}, M, \\boldsymbol{\\theta}).$\n\n## Moves for variable selection\n\nWe consider different types of move for the $k_{\\max}$ potential covariates (including interactions, etc.) [@Holmes:2002]\n\n- birth: adding an unused covariate chosen at random from the remaining ones\n- death: removing one covariate at random from the current matrix\n- swap an active covariate for an unused one.\n\nOnly the last type of move preserves the dimension.\n\n## Jacobians for reversible jump\n\nFor most moves $J=1$ in this case, except in four cases where the dimension $|M| \\in\\{1, 2, k_{\\max}-1, k_{\\max}\\}$ and\n\n- $J=2/3$ if $|M|=1$ and we try to add a covariate, or if $|M|=k_{\\max}$ and we try to remove a covariate\n- $J=3/2$ if $|M|=2$ and we try to remove a covariate, or if $|M|=k_{\\max}-1$ and we try to add the last covariate.\n\n## Posterior weights\n\nWe can keep track of which variables are active at each iteration of the MCMC and obtain the marginal posterior probability of inclusion through sample proportions.\n\nThis methods that explores neighbouring models (Grey code) only works with a limited number of covariates $p < 25.$\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}