{
  "hash": "776ebfcbb6884735b2269cb882de5583",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"LÃ©o Belzile\"\nsubtitle: \"Deterministic approximations\"\ndate: today\ndate-format: \"[Last compiled] dddd MMM D, YYYY\"\neval: true\necho: true\ncache: true\nbibliography: MATH80601A.bib\nformat:\n  revealjs:\n    slide-number: true\n    html-math-method: mathjax\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n    logo: \"fig/logo_hec_montreal_bleu_web.png\"\n---\n\n\n\n\n\n## Rationale for deterministic approximations\n\nMarkov chain Monte Carlo methods require tuning and can be markedly slow when the dimension of the parameter space grows.\n\nThe curse of dimensionality affects the performance of MCMC.\n\nWe consider simple approximations to the marginal likelihood, posterior moments, or posterior density that only require **numerical optimization**.\n\n## Landau notation\n\nWe need notation to characterize the growth rate of functions: when $n \\to \\infty$\n\n- big-O: $x = \\mathrm{O}(n)$ means that $x/n \\to c \\in \\mathbb{R}$ as $n\\to \\infty$ \n- little-o: $x =\\mathrm{o}(n)$ implies $x/n \\to 0$ as $n\\to \\infty.$\n\n## Taylor series expansion\n\nConsider a concave function $h(x)$ assumed twice continuously differentiable with mode at $x_0$. Then, a Taylor series expansion around $x_0$ gives\n\\begin{align*}\nh(x) = h(x_0) + h'(x_0)(x-x_0) + h''(x_0)(x-x_0)^2/2 + R\n\\end{align*}\nwhere the remainder $R=\\mathrm{O}\\{(x-x_0)^3\\}.$\n\n## Multivariate Taylor series expansion\n\nSimilarly, for $h(\\boldsymbol{x})$ a smooth vector valued function with mode at $\\boldsymbol{x}_0 \\in \\mathbb{R}^p$, we can write\n\\begin{align*}\n h(\\boldsymbol{x}) &= h(\\boldsymbol{x}_0) + (\\boldsymbol{x}- \\boldsymbol{x}_0)^\\top h'(\\boldsymbol{x}_0) \\\\&+ \\frac{1}{2}(\\boldsymbol{x}- \\boldsymbol{x}_0)^\\top h''(\\boldsymbol{x}_0)(\\boldsymbol{x}- \\boldsymbol{x}_0) + R.\n\\end{align*}\nUnder regularity conditions, the mode $\\boldsymbol{x}_0$ is such that $h'(\\boldsymbol{x}_0)=\\boldsymbol{0}_p.$\n\n## Laplace approximation\n\nThe **Laplace approximation** is used to approximate integrals of non-negative functions $g(\\boldsymbol{x})$ that are $\\mathrm{O}(n)$ of the form\n\\begin{align*}\nI_n = \\int_{\\mathbb{R}_p} g(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x} = \\int \\exp\\{h(\\boldsymbol{x})\\}\\mathrm{d} \\boldsymbol{x}.\n\\end{align*}\n\nThe idea is that we can, ignoring terms above third order and assuming  $\\boldsymbol{x}_0$ satisfies $h'(\\boldsymbol{x}_0)=\\boldsymbol{0}_p,$ approximate $g(\\cdot)$ by\na multivariate Gaussian density.\n\n## Laplace approximation to integrals\n\nIf we perform a Taylor series expansion of the log of the integrand, then\n\\begin{align*}\n I_n \\approx (2\\pi)^{p/2} | \\mathbf{H}(\\boldsymbol{x}_0)|^{-1/2}\\exp\\{h(\\boldsymbol{x}_0)\\} + \\mathrm{O}(n^{-1})\n\\end{align*}\nwhere $|\\mathbf{H}(\\boldsymbol{x}_0)|$ is the determinant of the Hessian matrix of $-h(\\boldsymbol{x})$ evaluated at the mode  $\\boldsymbol{x}_0.$\n\n## Laplace approximation\n\n- The idea behind the Laplace approximation is to approximate the log of the density (since the latter must be non-negative).\n- Compared to sampling-based methods, the Laplace approximation requires **optimization**. \n- The Laplace approximation is not invariant to reparametrization: in practice, it is best to perform it on a scale where the likelihood is as close to quadratic as possible in $g(\\boldsymbol{\\theta})$ and back-transform using a change of variable.\n\n## Laplace approximation to the marginal likelihood\n\n\nConsider a simple random sample of size $n$ from a distribution with parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^p.$ \n\nWrite [@Raftery:1995]\n\\begin{align*}\n p(\\boldsymbol{y}) = \\int_{\\mathbb{R}^p} p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\nand take \n\\begin{align*}h(\\boldsymbol{\\theta}) = \\log p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) + \\log p(\\boldsymbol{\\theta}).\n\\end{align*}\n\n## Laplace approximation to the marginal likelihood\n\nEvaluating at the maximum a posteriori $\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}}$ and letting $\\mathbf{H} = - \\log p(\\boldsymbol{y}, \\boldsymbol{\\theta}) / \\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top$ denote the Hessian matrix of second partial derivatives of the unnormalized log posterior, we get [@Tierney.Kadane:1986]\n\\begin{align*}\n \\log p(\\boldsymbol{y}) &= \\log p(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}}) + \\log p(\\boldsymbol{y} \\mid \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}}) \\\\&\\quad + \\frac{p}{2} \\log (2\\pi) - \\frac{1}{2}\\log |\\mathbf{H}(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}})| + \\mathrm{O}(n^{-1})\n\\end{align*}\n\n\n## Example with exponential likelihood\n\n\nConsider an exponential likelihood $Y_i \\mid \\lambda \\sim \\mathsf{expo}(\\lambda)$ with conjugate gamma prior $\\lambda \\sim \\mathsf{gamma}(a,b)$. The exponential model  has information $i(\\lambda)=n/\\lambda^2$ and the mode of the posterior is $$\\widehat{\\lambda}_{\\mathrm{MAP}}=\\frac{n+a-1}{\\sum_{i=1}^n y_i + b}.$$\n\n## Marginal likelihood approximation for exponential likelihood\n\nWe can also obtain an estimate of the marginal likelihood, which is equal for the conjugate model\n\\begin{align*}\np(\\boldsymbol{y}) = \\frac{\\Gamma(n+a)}{\\Gamma(a)}\\frac{b^a}{\\left(b + \\sum_{i=1}^n y_i \\right)^{n+a}}.\n\\end{align*}\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\nFor the sample of size $62,$ the exponential model marginal likelihood is $-276.5.$\n\n## Numerical approximation to marginal likelihood\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(waiting, package = \"hecbayes\")\na <- 0.01; b <- 0.01\nn <- length(waiting); s <- sum(waiting)\nmap <- (n + a - 1)/(s + b) #posterior mode\nlogpost <- function(x){\n  sum(dexp(waiting, rate = x, log = TRUE)) +\n    dgamma(x, a, b, log = TRUE)\n}\n# Hessian evaluated at MAP\nH <- -c(numDeriv::hessian(logpost, x = map))\n# Laplace approximation\nmarg_lik_laplace <- 0.5*log(2*pi) - log(H) + logpost(map)\n```\n:::\n\n\n\nThe Laplace approximation gives $-281.9.$\n\n\n\n## Posterior expectation using Laplace method\n\nIf we are interested in computing the posterior expectation of a positive real-valued functional $g(\\boldsymbol{\\theta}): \\mathbb{R}^p \\to \\mathbb{R}_{+},$ we may write\n\\begin{align*}\n \\mathsf{E}_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y}}\\{g(\\boldsymbol{\\theta}) \\mid \\boldsymbol{y}\\} &=  \\frac{\\int g(\\boldsymbol{\\theta}) p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) p( \\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}}{\\int p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})p( \\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}}\n\\end{align*}\n\n## Posterior expectation via Laplace\n\nWe can apply Laplace's method to both numerator and denominator. Let $\\widehat{\\boldsymbol{\\theta}}_g$ and $\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}}$ of the integrand of the numerator and denominator, respectively, and the negative of the Hessian matrix of the log integrands\n\\begin{align*}\n\\jmath_g&=  -\\frac{\\partial^2}{\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top} \\left\\{ \\log g(\\boldsymbol{\\theta}) + \\log p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) + \\log p(\\boldsymbol{\\theta})\\right\\}, \\\\\n\\jmath &=  -\\frac{\\partial^2}{\\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top} \\left\\{\\log p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) + \\log p(\\boldsymbol{\\theta})\\right\\}.\n\\end{align*}\n\n## Posterior expectation approximation\n\n\nPutting these together\n\\begin{align*}\n\\mathsf{E}_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y}}\\{g(\\boldsymbol{\\theta}) \\mid \\boldsymbol{y}\\} \\approx \\frac{|\\jmath(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}})|^{1/2}}{|\\jmath_g(\\widehat{\\boldsymbol{\\theta}}_g)|^{1/2}} \\frac{g(\\widehat{\\boldsymbol{\\theta}}_g) p(\\boldsymbol{y} \\mid \\widehat{\\boldsymbol{\\theta}}_g) p( \\widehat{\\boldsymbol{\\theta}}_g)}{p(\\boldsymbol{y} \\mid \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}}) p(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}})}.\n\\end{align*}\nWhile the Laplace method has an error $\\mathrm{O}(n^{-1}),$ the leading order term of the expansion cancels out from the ratio and the above has error of $\\mathrm{O}(n^{-2}).$\n\n## Example of posterior mean for exponential likelihood\n\nConsider the posterior mean $\\mathsf{E}_{\\Lambda \\mid \\boldsymbol{Y}}(\\lambda)$ and let $s=\\sum_{i=1}^n y_i$. Then, \\begin{align*}\n\\widehat{\\lambda}_g &= \\frac{(n+a)}{s + b} \\\\\n|\\jmath_g(\\widehat{\\lambda}_g)|^{1/2} &= \\left(\\frac{n+a}{\\widehat{\\lambda}_g^2}\\right)^{1/2} =  \\frac{s + b}{(n+a)^{1/2}}\n\\end{align*}\n\n## Posterior mean\n\nSimplification gives the approximation\n\\begin{align*}\n\\frac{\\exp(-1)}{s + b} \\frac{(n+a)^{n+a+1/2}}{(n+a-1)^{n+a-1/2}}\n\\end{align*}\nwhich gives $0.03457,$ whereas the true posterior mean is $(n+a)/(s+b) = 0.03457.$\n\n\n\n## Aside on prior and likelihood relative contribution\n\nUsually,\n\n- $p(\\boldsymbol{\\theta}) = \\mathrm{O}(1)$ and\n- $p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) = \\mathrm{O}(n)$\n\nThus, provided the prior does not impose unnecessary support constraints, we could alternatively \n\n- replace the MAP by the MLE, and\n- the Hessian $-\\mathbf{H}(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}})$ by the Fisher information $n\\boldsymbol{\\imath}(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}}).$\n\n## Frequentist approximations\n\nIf we use these approximations instead, we get\n\\begin{align*}\n \\log p(\\boldsymbol{y}) &= \\log p(\\boldsymbol{y} \\mid \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}}) -\\frac{p}{2} \\log n + \\log p(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})\\\\& \\quad   - \\frac{1}{2} \\log |\\boldsymbol{\\imath}(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})| + \\frac{p}{2} \\log(2\\pi)  + \\mathrm{O}(n^{-1/2})\n\\end{align*}\nwhere the error is now $\\mathrm{O}(n^{-1/2})$ due to replacing the true information by it's sample counterpart. \n\n\n## Reducing the approximation rate\n\nIgnoring all but the two first terms leads to $\\mathrm{O}(1)$ approximation error, unless we consider the setting where we take a prior centered at the MLE with unit Fisher information precision (equivalent to $n=1$ phantom observation). Then, due to cancellation of terms in the expansion,\n$$\\boldsymbol{\\theta} \\sim \\mathsf{Gauss}_p\\{ \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}}, \\boldsymbol{\\imath}^{-1}(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})\\},$$ \nwith approximation error of $\\mathrm{O}(n^{-1/2}).$\n\n## Frequentist approximation to the marginal likelihood\n\nThis gives the approximation, whose quality improves with increasing sample size $n$:\n\\begin{align*}\n-2\\log p(\\boldsymbol{y}) \\approx \\mathsf{BIC} = -2\\log p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) + p\\log n\n\\end{align*}\n\nThe unnormalized weight $\\exp(-\\mathsf{BIC}/2)$ is an approximation fo the marginal likelihood sometimes used for model comparison in Bayes factor.\n\n## Bayesian model averaging via BIC\n\nWe consider the `diabetes` model from @Park.Casella:2008 and models with 10 predictors plus intercept.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(leaps)\ndata(diabetes, package = \"lars\")\nsearch <- leaps::regsubsets(\n  x = diabetes$x,\n  y = diabetes$y,\n  intercept = TRUE,\n  method = \"exhaustive\", \n  nvmax = 10, nbest = 99, really.big = TRUE)\nmodels_bic <- summary(search)$bic\n# Renormalize BIC and keep only models with some weight\nbic <- models_bic - min(models_bic)\nmodels <- which(bic < 7)\nbma_weights <- exp(-bic[models]/2)/sum(exp(-bic[models]/2))\n```\n:::\n\n\n\n## Weights and model components\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![BIC as a function of the linear model covariates (left) and Bayesian model averaging approximate weights (in percentage) for the 10 models with the highest posterior weights according to the BIC approximation.](bayesmod-slides9_files/figure-revealjs/fig-bmaweights-1.png){#fig-bmaweights width=960}\n:::\n:::\n\n\n\n\n## Gaussian approximation to the posterior\n\nWe can also use similar ideas to approximate the posterior. Suppose that we can Taylor expand the log prior and log density around their respective mode, say $\\widehat{\\boldsymbol{\\theta}}_0$ and $\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}},$ with $\\jmath_0(\\widehat{\\boldsymbol{\\theta}}_0)$ and $\\jmath(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})$ denoting negative of the corresponding Hessian matrices.\n\n## Approximation to posterior\n\nTogether, these yield\n\\begin{align*}\n\\log p(\\boldsymbol{\\theta}) &\\approx \\log p(\\widehat{\\boldsymbol{\\theta}}_0) - \\frac{1}{2}(\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}_0)^\\top\\jmath_0(\\widehat{\\boldsymbol{\\theta}}_0)(\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}_0)\\\\\n\\log p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta}) &\\approx \\log p(\\boldsymbol{y} \\mid \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})\\\\& \\quad  - \\frac{1}{2}(\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})^\\top\\jmath(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})(\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})\n\\end{align*}\n\n## Gaussian approximation to  posterior\n\nThe approximate posterior must be Gaussian with precision $\\jmath_n^{-1}$ and mean $\\mu_n,$ where\n\\begin{align*}\n\\jmath_n &= \\jmath_0(\\widehat{\\boldsymbol{\\theta}}_{0}) + \\jmath(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})\n\\\\\n\\widehat{\\boldsymbol{\\theta}}_n &= \\jmath_n^{-1}\\left\\{ \\jmath_0(\\widehat{\\boldsymbol{\\theta}}_{0})\\widehat{\\boldsymbol{\\theta}}_{0} + \\jmath(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}}\\right\\}\n\\end{align*}\nand note that $\\jmath_0(\\widehat{\\boldsymbol{\\theta}}_{0}) = \\mathrm{O}(1),$ whereas $\\jmath_n$ is $\\mathrm{O}(n).$\n\n\n## Gaussian large-sample approximation to MLE\n\nSuppose that the prior is continuous and positive in a neighborhood of the maximum.\n\n\nAssume further that the regularity conditions for maximum likelihood estimator holds. Then, in the limit as $n \\to \\infty$ \n\\begin{align*}\n\\boldsymbol{\\theta} \\mid \\boldsymbol{y} \\stackrel{\\cdot}{\\sim} \\mathsf{Gauss}_p\\{ \\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}},  \\jmath^{-1}(\\widehat{\\boldsymbol{\\theta}}_{\\mathrm{MLE}})\\}\n\\end{align*}\n\n## Corollary\n\nIn large samples, the inference obtained from using likelihood-based inference and Bayesian methods will be equivalent\n\n- credible intervals will also have guaranteed frequentist coverage. \n\nMisspecified model: Bayesian will return the model from the family that minimizes the Kullback--Leibler divergence with the true data generating process.\n\n## Regularity conditions\n\n\n- The maximizer must be uniquely identified from the data and must not be on boundary, so that we can perform a two-sided Taylor series expansion around $\\boldsymbol{\\theta}_0.$ \n- We need to be able to apply the law of large numbers to get the variance (reciprocal Fisher information) and apply a central limit theorem to the score.\n-  The third-order derivative of the likelihood is bounded: we can get away with weaker, but this is easiest to check (and ensures that the higher order terms of the Taylor series expansion vanishes asymptotically).\n\n\n## Gaussian approximation to gamma posterior\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Gaussian approximation (dashed) to the posterior density (full line) of the exponential rate $\\lambda$ for the `waiting` dataset with an exponential likelihood and a gamma prior with $a=0.01$ and $b=0.01.$ The plots are based on the first $10$ observations (left) and the whole sample of size $n=62$ (right).](bayesmod-slides9_files/figure-revealjs/fig-post-gamma-laplace-1.png){#fig-post-gamma-laplace width=960}\n:::\n:::\n\n\n\n\n\n## Structured models\n\nModels in genomics data or spatio-temporal applications including random effects (e.g., spline smoothers) are predominant.\n\nThey often contain several thousands or millions of latent parameters.\n\nInference becomes **unfeasible** in reasonable time using methods we covered so far.\n\n\n## Integrated nested Laplace approximation\n\n\nConsider a model with response $\\boldsymbol{y}$ which depends on covariates $\\mathbf{x}$ through a latent Gaussian process $\\boldsymbol{\\beta}.$\n\n\nTypically, the prior of coefficients $\\boldsymbol{\\beta} \\in \\mathbb{R}^p.$ The dimension $p$ can be substantial (several thousands) with a comparably low-dimensional hyperparameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^m.$\n\n\nConsider data that are conditionally independent given $\\boldsymbol{\\beta}$ and $\\boldsymbol{\\beta} \\sim \\mathsf{Gauss}_p(\\boldsymbol{0}_p, \\mathbf{Q}^{-1})$ for simplicity.\n\n## Gaussian approximations considered\n\nThen,\n\\begin{align*}\np(\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\boldsymbol{\\theta}) \\propto \\exp\\left\\{-\\frac{1}{2} \\boldsymbol{\\beta}^\\top\\mathbf{Q}\\boldsymbol{\\beta} + \\sum_{i=1}^n \\log p(y_i \\mid \\beta_i, \\boldsymbol{\\theta})\\right\\}\n\\end{align*}\nIf $\\boldsymbol{\\beta}$ is a Gaussian Markov random field, it's precision matrix $\\mathbf{Q}$ will be sparse. A Gaussian approximation to this model would have precision matrix $\\mathbf{Q} + \\mathrm{diag}(\\boldsymbol{c})$ for some vector $\\boldsymbol{c}$ obtained from the second-order expansion of the likelihood.\n\nThis allows one to use dedicated algorithms for sparse matrices.\n\n\n\n## INLA: targets of inference\n\nInterest typically then lies in marginal parameters\n\\begin{align*}\np(\\beta_i \\mid \\boldsymbol{y}) &= \\int p(\\beta_i \\mid \\boldsymbol{\\theta}, \\boldsymbol{y}) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}\\\\\np(\\theta_i \\mid \\boldsymbol{y}) &= \\int p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}_{-i}\n\\end{align*}\nwhere $\\boldsymbol{\\theta}_{-i}$ denotes the vector of hyperparameters excluding the $i$th element $\\theta_i.$ \n\n## Philosophy of INLA\n\nThe INLA method builds Laplace approximations to the integrands $p(\\beta_i \\mid \\boldsymbol{\\theta}, \\boldsymbol{y})$ and $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}),$ and evaluates the integral using quadrature rules over a coarse grid of values of $\\boldsymbol{\\theta}.$\n\n\nThe marginal posterior $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$ is approximated by writing $$p(\\boldsymbol{\\beta}, \\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\propto p(\\boldsymbol{\\beta} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y}) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$$ and performing a Laplace approximation for fixed value of $\\boldsymbol{\\theta}$ for the term $p(\\boldsymbol{\\beta} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y}),$ whose mode we denote by $\\widehat{\\boldsymbol{\\beta}}.$\n\n## INLA approximation (step 1)\n\nThis yields\n\\begin{align*}\n\\widetilde{p}(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\propto \\frac{p(\\widehat{\\boldsymbol{\\beta}}, \\boldsymbol{\\theta} \\mid \\boldsymbol{y})}{ p_{G}(\\widehat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{y}, \\boldsymbol{\\theta})} = \\frac{p(\\widehat{\\boldsymbol{\\beta}}, \\boldsymbol{\\theta} \\mid \\boldsymbol{y})}{ |\\mathbf{H}(\\widehat{\\boldsymbol{\\beta}})|^{1/2}}\n\\end{align*}\n\n## Note on approximation\n\nThe Laplace approximation $p_{G}(\\widehat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{y}, \\boldsymbol{\\theta})$ has kernel $$p_{G}(\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\boldsymbol{\\theta}) \\propto |\\mathbf{H}(\\widehat{\\boldsymbol{\\beta}})|^{1/2}\\exp\\{-(\\boldsymbol{\\beta}- \\widehat{\\boldsymbol{\\beta}})^\\top \\mathbf{H}(\\widehat{\\boldsymbol{\\beta}})(\\boldsymbol{\\beta}- \\widehat{\\boldsymbol{\\beta}})/2\\};$$\nsince it is evaluated at $\\widehat{\\boldsymbol{\\beta}},$ we retrieve only the determinant of the negative Hessian of \n$p(\\boldsymbol{\\beta} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y}),$ namely $\\mathbf{H}(\\widehat{\\boldsymbol{\\beta}}).$ Note that the latter varies with $\\boldsymbol{\\theta}.$\n\n## Numerical integration\n\nTo obtain $p(\\theta_i \\mid \\boldsymbol{y})$, we then proceed with\n\n1. finding the mode of $\\widetilde{p}(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$ using a Newton's method, approximating the gradient and Hessian via finite differences.\n2. Compute the negative Hessian at the mode to get an approximation to the covariance of $\\boldsymbol{\\theta}.$ Use an eigendecomposition to get the principal directions $\\boldsymbol{z}$.\n\n## Numerical integration (2)\n\n3. In each direction of $\\boldsymbol{z}$, consider drops in $\\widetilde{p}(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$ as we move away from the mode and define a coarse grid based on these, keeping points where the difference in $\\widetilde{p}(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$ relative to the mode is less than some small $\\delta.$\n4. Retrieve the marginal by numerical integration using the central composition design outline above. We can also use directly avoid the integration and use the approximation at the posterior mode of $\\widetilde{p}(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}).$\n\n\n## Approximation of marginal of Gaussian latent effect\n\nFor the marginal $p(\\beta_i \\mid \\boldsymbol{y})$ term, @Rue.Martino.Chopin:2009 proceed instead with the marginal for $\\beta_i$ by building an approximation of it based on maximizing $\\boldsymbol{\\beta}_{-i} \\mid \\beta_i, \\boldsymbol{\\theta}, \\boldsymbol{y}$ to yield $\\widehat{\\boldsymbol{\\beta}}_{(i)}$ whose $i$th element is $\\beta_i,$ yielding\n\\begin{align*}\n\\widetilde{p}(\\beta_i \\mid \\boldsymbol{\\theta}, \\boldsymbol{y}) \\propto \\frac{p(\\widehat{\\boldsymbol{\\beta}}_{(i)}, \\boldsymbol{\\theta} \\mid \\boldsymbol{y})}{\\widetilde{p}(\\widehat{\\boldsymbol{\\beta}}_{(i),-i} \\mid \\beta_i, \\boldsymbol{\\theta}, \\boldsymbol{y})},\n\\end{align*}\nwith a suitable renormalization of $\\widetilde{p}(\\widehat{\\boldsymbol{\\beta}}_{(i),-i} \\mid \\beta_i, \\boldsymbol{\\theta}, \\boldsymbol{y}).$\n\n## Remark on approximation\n\nWhile we could use the Laplace approximation $p_{G}(\\widehat{\\boldsymbol{\\beta}} \\mid \\boldsymbol{y}, \\boldsymbol{\\theta})$ and marginalize the latter directly, this leads to evaluation of the Laplace approximation to the density far from the mode, which is often inaccurate. \n\n## Numerical challenges to Laplace approximation\n\nOne challenge is that $p$ is **very large**, so calculation of the Hessian $\\mathbf{H}$ is costly to evaluate. \n\n\nHaving to evaluate it repeatedly for each marginal $\\beta_i$ for $i=1, \\ldots, p$ is prohibitive since it involves factorizations of $p \\times p$ matrices.\n\n## Further approximations\n\nTo reduce the computational costs, @Rue.Martino.Chopin:2009 propose to use the approximate mean to avoid optimizing and use the conditional of the Gaussian approximation with mean $\\widehat{\\boldsymbol{\\beta}}$ and covariance $\\boldsymbol{\\Sigma} = \\mathbf{H}^{-1}(\\widehat{\\boldsymbol{\\beta}}),$\n\\begin{align*}\n\\boldsymbol{\\beta}_{-i} \\mid \\beta_i, \\boldsymbol{\\theta}, \\boldsymbol{y} &\\approx \\mathsf{Gauss}_{p-1}\\left(\\widetilde{\\boldsymbol{\\beta}}_{(i)}, \\mathbf{M}^{-1}_{-i,-i}\\right);\n\\\\\n\\widetilde{\\boldsymbol{\\beta}}_{(i)} &= \\widehat{\\boldsymbol{\\beta}}_{-i} + \\boldsymbol{\\Sigma}_{i,i}^{-1}\\boldsymbol{\\Sigma}_{i,-i}(\\beta_i - \\widehat{\\beta}_i),\n\\end{align*}\nThis only requires a rank-one update. \n\n## Further approximations\n\n@Wood:2019 suggest to use a Newton step to correct $\\widetilde{\\boldsymbol{\\beta}}_{(i)},$ starting from the conditional mean.\n\nThe second step is to exploit the local dependence on $\\boldsymbol{\\beta}$ using the Markov structure to build an improvement to the Hessian. Various strategies are proposed in @Rue.Martino.Chopin:2009 and @Wood:2019.\n\nNowadays, the INLA software uses a low-rank variational correction to Laplace method, proposed in @vanNiekerk.Rue:2024.\n\n## The INLA software\n\n\nThe `INLA` [**R** package](https://www.r-inla.org/) provides an interface to fit models with Gaussian latent random effects. While the software is particularly popular for spatio-temporal applications using the SPDE approach, we revisit two examples in the sequel where we can exploit the Markov structure.\n\n\n## Stochastic volatility model with INLA \n\nFinancial returns $Y_t$ typically exhibit time-varying variability. The **stochastic volatility** model is a parameter-driven model that specifies\n\\begin{align*}\nY_t &= \\exp(h_t/2) Z_t \\\\\nh_t &= \\gamma + \\phi (h_{t-1} - \\gamma) + \\sigma U_t\n\\end{align*}\nwhere $U_t \\stackrel{\\mathrm{iid}}{\\sim} \\mathsf{Gauss}(0,1)$ and $Z_t \\sim  \\stackrel{\\mathrm{iid}}{\\sim} \\mathsf{Gauss}(0,1).$\nThe [`INLA` documentation](https://inla.r-inla-download.org/r-inla.org/doc/likelihood/stochvolgaussian.pdf) provides information about which default prior and hyperparameters are specified. We use a $\\mathsf{gamma}(1, 0.001)$ prior for the precision.\n\n## Stochastic volality code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(INLA)\ndata(exchangerate, package = \"hecbayes\")\n# Compute response from raw spot exchange rates at noon\ny <- 100*diff(log(exchangerate$dexrate))\ntime <- seq_along(y)\ndata <- data.frame(y = y, time = time)\nf_stochvol <- y ~ f(time, model = \"ar1\",\n                            param = list(prec = c(1, 0.001)))\nmod_stochvol <- inla(f_stochvol, family = \"stochvol\", data = data)\n# Obtain summary\nsummary <- summary(mod_stochvol)\n# plot(mod_stochvol)\nmarg_prec <- mod_stochvol$marginals.hyperpar[[1]]\nmarg_phi <- mod_stochvol$marginals.hyperpar[[2]]\n```\n:::\n\n\n \n## Marginal posterior approximations \n \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Marginal densities of precision and autocorrelation parameters from the Gaussian stochastic volatility model.](bayesmod-slides9_files/figure-revealjs/fig-stochvol-inla-1.png){#fig-stochvol-inla width=960}\n:::\n:::\n\n\n\n## Comment on stochastic volatility\n\n@fig-stochvol-inla shows that the correlation $\\phi$ is nearly one, leading to random walk behaviour and high persistence over time (this is also due to the frequency of observations).\n\nThis strong serial dependence in the variance is in part responsible for the difficulty in fitting this model using MCMC.\n\n## Marginal approximations\n\nWe can use the marginal density approximations to obtain quantiles for summary of interest, marginal posterior moments, etc. \n\n\nThe software also includes utilities to transform the parameters using the change of variable formula.\n\n## Marginal summaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute density, quantiles, etc. via inla.*marginal\n# Change of variable to get variance from precision\nmarg_var <- INLA::inla.tmarginal(\n  fun = function(x) { 1 / x }, \n  marginal = marg_prec)\nINLA::inla.qmarginal(marg_var, p = c(0.025, 0.5,  0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2864911 0.4543033 0.7396786\n```\n\n\n:::\n\n```{.r .cell-code}\n# Posterior marginal mean and variance of phi\nmom1 <- INLA::inla.emarginal(\n    fun = function(x){x}, \n    marginal = marg_phi)\nmom2 <- INLA::inla.emarginal(\n    fun = function(x){x^2}, \n    marginal = marg_phi)\nc(mean = mom1, sd = sqrt(mom2 - mom1^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       mean          sd \n0.984052715 0.005762476 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Tokyo binomial time series\n\nWe revisit the Tokyo rainfall example, but this time fit the model with `INLA` rather than MCMC. \n\n\nWe specify the mean model without intercept and fit a logistic regression, with a second-order cyclic random walk prior for the coefficients, and the default priors for the other parameters.\n\n## Code syntax\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Tokyo, package = \"INLA\")\n# Formula (removing intercept)\nformula <- y ~ f(time, model = \"rw2\", cyclic = TRUE) - 1\nmod <- INLA::inla(\n   formula = formula, \n   family = \"binomial\",\n   Ntrials = n, \n   data = Tokyo)\n```\n:::\n\n\n\nThe marginal posterior with pointwise 95% credible intervals on the next slide show nearly identical results to marginals from the probit model.\n\n## Posterior of random effect prior\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Posterior probability per day of the year with posterior median and 95% credible interval for the Tokyo rainfall binomial time series.](bayesmod-slides9_files/figure-revealjs/fig-rainfall-inla-1.png){#fig-rainfall-inla width=960}\n:::\n:::\n\n\n\n\n\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}