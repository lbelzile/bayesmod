{
  "hash": "b60b0f8980a9af88149b235096c9a6bf",
  "result": {
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"LÃ©o Belzile\"\nsubtitle: \"Priors\"\ndate: today\ndate-format: YYYY\neval: true\necho: true\ncache: true\nformat:\n  revealjs:\n    slide-number: true\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n---\n\n\n\n\n\n## Priors\n\nThe posterior density is \n\n\\begin{align*}\n\\color{#D55E00}{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})} = \\frac{\\color{#0072B2}{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})} \\times  \\color{#56B4E9}{p(\\boldsymbol{\\theta})}}{\\color{#E69F00}{\\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\\mathrm{d} \\boldsymbol{\\theta}}},\n\\end{align*}\n\nwhere $$\\color{#D55E00}{\\text{posterior}} \\propto \\color{#0072B2}{\\text{likelihood}} \\times \\color{#56B4E9}{\\text{prior}}$$\n\nWe need to determine a suitable prior.\n\n## Impact of the prior\n\nThe posterior is a compromise prior and likelihood:\n\n- the more informative the prior, the more the posterior resembles it.\n- in large samples, the effect of the prior is often negligible^[depends on the parameter!]\n\n## Controversial?\n\n- No unique choice for the prior: different analysts get different inferences\n- What is the robustness to the prior specification? Check through sensitivity analysis.\n- By tuning the prior, we can get any answer we get (if informative enough)\n- Even with prior knowledge, hard to elicit parameter (many different models could yield similar summary statistics)\n\n\n## Choosing priors\n\nInfinite number of choice, but many default choices...\n\n- conditionally conjugate priors (ease of interpretation, computational advantages)\n- flat priors and vague priors (mostly uninformative)\n- informative priors (expert opinion)\n- Jeffrey's priors (improper, invariant to reparametrization)\n- penalized complexity (regularization)\n- shrinkage priors (variable selection, reduce overfitting)\n\n## Determining hyperparameters\n\nWe term **hyperparameters** the parameters of the (hyper)priors.\n\nHow to elicit reasonable values for them?\n\n- use moment matching to get sensible values\n- trial-and-error using the prior predictive\n\n## Example of simple linear regression\n\nWorking with standardized response and inputs $$x_i \\mapsto (x_i - \\overline{x})/\\mathrm{sd}(\\boldsymbol{x}),$$\n\n- the slope is the correlation between explanatory $\\mathrm{X}$ and response $Y$\n- the intercept should be mean zero\n- are there sensible bounds for the range of the response?\n\n\n## Bixi counts\n\n\n\n::: {.cell hash='bayesmod-slides2_cache/revealjs/fig-bixi_00bab6ff8d67d2e72ea7c8ea1bdeb42c'}\n::: {.cell-output-display}\n![Prior draws of the linear regression coefficients with observed data superimposed (left), and scatterplot of prior predictive draws (light gray) against observed data (right). There are 20 docks on the platform.](bayesmod-slides2_files/figure-revealjs/fig-bixi-1.png){#fig-bixi width=960}\n:::\n:::\n\n\n##  Example 2 - simple linear regression\n\nConsider the relationship between height ($Y$, in cm) and weight ($X$, in kg) among humans adults.^[Section 4.4.1 of @McElreath:2020]\n\nModel using a simple linear regression\n\n\\begin{align*}\nh_i &\\sim \\mathsf{No}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1(\\mathrm{x}_i - \\overline{x}) \\\\\n\\beta_0 &\\sim \\mathsf{No}(178, 20^2) \\\\\n\\sigma &\\sim \\mathsf{U}(0, 50)\n\\end{align*}\n\n\n\n## Priors for the slope {.smaller}\n\n\n::: {.cell hash='bayesmod-slides2_cache/revealjs/fig-priors-draw-height-weight_adaf691a0e6516ce595f769ecd45ddde'}\n::: {.cell-output-display}\n![Prior draws of linear regressions with different priors: vague $\\beta_1 \\sim \\mathsf{No}(0, 100)$ (left) and lognormal $\\ln(\\beta_1) \\sim \\mathsf{No}(0,1)$ (right). Figure 4.5 of @McElreath:2020. The Guiness record for the world's tallest person is 272cm.](bayesmod-slides2_files/figure-revealjs/fig-priors-draw-height-weight-1.png){#fig-priors-draw-height-weight width=960}\n:::\n:::\n\n\n## Conjugate priors\n\nA prior density $p(\\boldsymbol{\\theta})$ is conjugate for likelihood $L(\\boldsymbol{\\theta}; \\boldsymbol{y})$ if the product $L(\\boldsymbol{\\theta}; \\boldsymbol{y})p(\\boldsymbol{\\theta})$, after renormalization, is of the same parametric family as the prior.\n\n\nDistributions that are exponential family admit conjugate priors.^[A distribution is an exponential family if it's density can be written\n\\begin{align*}\nf(y; \\boldsymbol{\\theta}) = \\exp\\left\\{ \\sum_{k=1}^K Q_k(\\boldsymbol{\\theta}) t_k(y) + D(\\boldsymbol{\\theta})\\right\\}.\n\\end{align*}\nThe support of $f$ mustn't depend on $\\boldsymbol{\\theta}$.]\n\n\n\n## Conjugate priors for common exponential families\n\n| distribution | unknown parameter |  conjugate prior |\n|-------------------|:-----------------|:-----------------|\n| $Y \\sim \\mathsf{Exp}(\\lambda)$ | $\\lambda$ | $\\lambda \\sim \\mathsf{Ga}(\\alpha, \\beta)$ |\n|  $Y \\sim \\mathsf{Po}(\\mu)$ | $\\mu$|  $\\mu \\sim \\mathsf{Ga}(\\alpha, \\beta)$  |\n| $Y \\sim \\mathsf{Bin}(n, \\theta)$ | $\\theta$ | $\\theta \\sim \\mathsf{Be}(\\alpha, \\beta)$ |\n| $Y \\sim \\mathsf{No}(\\mu, \\sigma^2)$ | $\\mu$ | $\\mu \\sim \\mathsf{No}(\\nu, \\omega^2)$ |\n| $Y \\sim \\mathsf{No}(\\mu, \\sigma^2)$ | $\\sigma$ | $\\sigma^{-2} \\sim \\mathsf{Ga}(\\alpha, \\beta)$ |\n| $Y \\sim \\mathsf{No}(\\mu, \\sigma^2)$ | $\\mu, \\sigma$ | $\\mu \\mid \\sigma^2 \\sim \\mathsf{No}(\\nu, \\omega \\sigma^2)$, $\\sigma^{-2} \\sim \\mathsf{Ga}(\\alpha, \\beta)$ |\n\n## Conjugate prior for the Poisson \n\nIf $Y \\sim \\mathsf{Po}(\\mu)$ with density $f(y) = \\mu^x\\exp(-\\mu x)/x!$, then for $\\mu \\sim \\mathsf{Ga}(\\alpha, \\beta)$ with $\\alpha, \\beta$ fixed. \n\n$$\np(\\mu \\mid y) \\stackrel{\\mu}{\\propto} \\mu^x \\exp(-\\mu x) \\mu^{\\alpha-1} \\exp(-\\beta \\mu)\n$$\nso the posterior is gamma $\\mathsf{Ga}(x + \\alpha, x + \\beta)$.\n\n*Parameter interpretation*: $\\alpha$ events in $\\beta$ time intervals.\n\n## Conjugate prior for Gaussian (known variance) {.smaller}\n\nConsider an iid sample, $Y_i \\sim \\mathsf{No}(\\mu, \\sigma^2)$ and let $\\mu \\mid \\sigma \\sim \\mathsf{No}(\\nu, \\sigma^2\\tau^2)$. Then,\n\\begin{align*}\np(\\mu, \\sigma) &\\propto \\frac{p(\\sigma)}{\\sigma^{n+1}} \\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_{i}-\\mu)^2\\right\\} \\exp\\left\\{-\\frac{1}{2\\sigma^2\\tau^2}(\\mu - \\nu)^2\\right\\}\n\\\\&\\propto \\frac{p(\\sigma)}{\\sigma^{n+1}} \\exp\\left\\{\\left(\\sum_{i=1}^n y_{i} + \\frac{\\nu}{\\tau^2}\\right)\\frac{\\mu}{\\sigma^2} - \\left( \\frac{n}{2} +\\frac{1}{2\\tau^2}\\right)\\frac{\\mu^2}{\\sigma^2}\\right\\}.\n\\end{align*}\n\nThe conditional posterior $p(\\mu \\mid \\sigma)$ is Gaussian with\n\n- mean $(n\\overline{y}\\tau^2 + \\nu)/(n\\tau^2 + 1)$ and \n- precision (reciprocal variance) $(n + 1/\\tau^2)/\\sigma^2$.\n\n## Upworthy examples\n\n- The Upworthy Research Archive [@Matias:2021] contains results for 22743 experiments, with a click through rate of 1.58% on average and a standard deviation of 1.23%.\n- We consider an A/B test that compared four different headlines for a story.\n- We model the conversion **rate** for each using $\\texttt{click}_i \\sim \\mathsf{Po}(\\lambda_i\\texttt{impression}_i)$\n\n## A/B test: Sesame street example\n\n\n::: {.cell hash='bayesmod-slides2_cache/revealjs/upworthy_91b5fae631bc16c863b24ca97973b009'}\n::: {.cell-output-display}\n|headline | impressions| clicks|\n|:--------|-----------:|------:|\n|H1       |        3060|     49|\n|H2       |        2982|     20|\n|H3       |        3112|     31|\n|H4       |        3083|      9|\n:::\n:::\n\n\nConjugate prior: moment matching for $\\lambda \\sim \\mathsf{Ga}(\\alpha, \\beta )$ gives $\\alpha = 1.64$ and $\\beta = 0.01$, as $\\beta = \\mathsf{Va}_0(\\lambda)/\\mathsf{E}_0(\\lambda)$.\n\n## Posterior distributions for Sesame Street\n\n\n\n::: {.cell hash='bayesmod-slides2_cache/revealjs/fig-upworthy_ebd2d0786eece42bea0fc9ff7350a2b8'}\n::: {.cell-output-display}\n![Gamma posterior of the conversion rate for the Upworthy Sesame street headline.](bayesmod-slides2_files/figure-revealjs/fig-upworthy-1.png){#fig-upworthy width=768}\n:::\n:::\n\n\n## Proper priors\n\n\n:::{#thm-proper-priors}\n\nA sufficient condition for a prior to yield a proper (i.e., integrable) posterior density function is that it is (proportional) to a density function.\n\n:::\n\n- If we pick an improper prior, we need to check that the posterior is well-defined.\n- The answer to this question may depend on the sample size.\n\n\n## Proper posterior in a random effect model\n\n\nConsider a Gaussian random effect model with $n$ independent observations in $J$ groups \n\nThe $i$th observation  in group $j$ is\n\\begin{align*}\nY_{ij} &\\sim \\mathsf{No}(\\mu_{ij}, \\sigma^2) \\\\\n\\mu_{ij}&= \\mathbf{X}_i \\boldsymbol{\\beta} + \\alpha_j,  \\\\\n\\alpha_j &\\sim \\mathsf{No}(0, \\tau^2)\\\\\n...\n\\end{align*}\n\n## Conditions for a proper posterior\n\n- for $\\tau \\sim \\mathsf{U}(0, \\infty)$, we need at least $J \\geq 3$ 'groups' for the posterior to be proper.\n- if we take $p(\\tau) \\propto \\tau^{-1}$, the posterior is never proper. \n\nAs @Gelman:2006 states: \n\n> in a hierarchical model the data can never rule out a group-level variance of zero, and so [a] prior distribution cannot put an infinite mass in this area\n\n\n## Improper priors as limiting cases\n\nWe can view the improper prior as a limiting case $$\\sigma \\sim \\mathsf{U}(0, t), \\qquad t \\to \\infty.$$\n\nThe Haldane prior for $\\theta$ in a binomial model is $\\theta^{-1}(1-\\theta)^{-1}$, a limiting $\\mathsf{Be}(0,0)$ distribution. \n\nThe improper prior $p(\\sigma) \\propto \\sigma^{-1}$ is equivalent to an inverse gamma $\\mathsf{IGa}(\\epsilon, \\epsilon)$ when $\\epsilon \\to 0$. \n\nThe limiting posterior is thus improper for random effects scales, so the value of $\\epsilon$ matters.\n\n\n## MDI prior for generalized Pareto\n\nLet $Y_i \\sim \\mathsf{GP}(\\sigma, \\xi)$ be generalized Pareto with density $$f(x) = \\sigma^{-1}(1+\\xi x/\\sigma)_{+}^{-1/\\xi-1}$$ for $\\sigma>0$ and $\\xi \\in \\mathbb{R}$, and $x_{+} =\\max\\{0, x\\}$.\n\n\nConsider the maximum data information (MDI) $$p(\\xi) \\propto \\exp(-\\xi).$$\n\nSince $\\lim_{\\xi \\to -\\infty} \\exp(-\\xi) = \\infty$, the prior density increases without bound as $\\xi$ becomes smaller.\n\n## Truncated MDI for generalized Pareto distribution\n\nThe MDI prior leads to an improper posterior without modification.\n\n\n::: {.cell hash='bayesmod-slides2_cache/revealjs/fig-mdiprior_d475fbdd077e628062ee294f5ef5c14f'}\n::: {.cell-output-display}\n![Unscaled maximum data information (MDI) prior density.](bayesmod-slides2_files/figure-revealjs/fig-mdiprior-1.png){#fig-mdiprior width=100%}\n:::\n:::\n\n\nIf we restrict the range of the MDI prior $p(\\xi)$ to $\\xi \\geq -1$, then $p(\\xi + 1) \\sim \\mathsf{Exp}(1)$ and posterior is proper.\n\n## Flat priors\n\nUniform prior over the support of $\\theta$, $$p(\\theta) \\propto 1.$$ \n\nImproper prior unless $\\theta \\in [a,b]$ for finite $a, b$.\n\n## Flat priors for scale parameters\n\nConsider a scale parameter $\\sigma > 0$.\n\n- We could truncate the range, e.g., $\\sigma \\sim \\mathsf{U}(0, 50)$, but this is not 'uninformative', as extreme values of $\\sigma$ are as likely as small ones.\n- These priors are not invariant: if $p\\{\\log(\\sigma)\\} \\propto 1$ implies $p(\\sigma) \\propto \\sigma^{-1}$ so can be informative on another scale.\n\n## Vague priors\n\nVague priors are very diffuse proper prior.\n\nFor example, a vague Gaussian prior for regression coefficients on standardized data, $$\\boldsymbol{\\beta} \\sim \\mathsf{No}_p(\\mathbf{0}_p, 100\\mathbf{I}_p).$$ \n\n- if we consider a logistic regression with a binary variable $\\mathrm{X}_j \\in \\{0,1\\}$, then $\\beta_j =5$ gives odds ratios of 150, and $\\beta_j=10$ of around 22K... \n\n## Invariance and Jeffrey's prior\n\nIn single-parameter models, the **Jeffrey's prior** $$p(\\theta) \\propto |\\imath(\\theta)|^{1/2},$$ proportional to the square root of the determinant of the Fisher information matrix, is invariant to any (differentiable) reparametrization.\n\n\n## Jeffrey's prior for the binomial distribution\n\nConsider $Y \\sim \\mathsf{Bin}(1, \\theta)$. The negative of the second derivative of the log likelihood with respect to $p$ is \n$$\n\\jmath(\\theta) = - \\partial^2 \\ell(\\theta; y) / \\partial \\theta^2 = y/\\theta^2 + (1-y)/(1-\\theta)^2.\n$$\n\nSince $\\mathsf{E}(Y)=\\theta$, the Fisher information is $$\\imath(\\vartheta) = \\mathsf{E}\\{\\jmath(\\theta)\\}=1/\\theta + 1/(1-\\theta) = n/\\{\\theta(1-\\theta)\\}.$$\nJeffrey's prior is therefore $p(\\theta) \\propto \\theta^{-1/2}(1-\\theta)^{-1/2}$, a conjugate Beta prior $\\mathsf{Be}(0.5,0.5)$.\n\n\n## Invariant priors for location-scale families\n\nFor a location-scale family with location $\\mu$ and scale $\\sigma$, the independent priors\n\\begin{align*}\np(\\mu) &\\propto 1\\\\\np(\\sigma) &\\propto \\sigma^{-1}\n\\end{align*}\nare location-scale invariant.\n\nThe results are invariant to affine transformations of the units, $\\vartheta = a + b \\theta$.\n\n\n\n## Penalized complexity priors \n\n@Simpson:2017 consider a principled way of constructing priors that penalized model complexity for stable inference and limit over-specification.\n\nSuppose that the restriction of the parameter creates a simpler base version.\n\n- e.g., if we have a random effect $\\alpha \\sim \\mathsf{No}(0, \\zeta^2)$, the value $\\zeta=0$ corresponds to no group variability.\n \n## Ingredients of penalized complexity priors\n\nConsider a penalized complexity prior for parameter $\\zeta$.\n\n**Occam's razor** states that the simpler base model should be preferred if there is not enough evidence in favor of the full model.\n\nWe measure the complexity of the full model with density $f$ using the Kullback--Leibler divergence between $f$ and base model $f_0$ densities.\nThis is transformed into a distance $d=\\sqrt{2\\mathsf{KL}(f || f_0)}$.\n\n## Penalized complexity prior construction\n\nUsing a constant rate penalization from base model gives an exponential prior $p(d) = \\lambda \\exp(-\\lambda d)$ on the distance scale, with a mode at $d=0$, corresponding to the base model. \n\nBacktransform to parameter space to get $p(\\zeta)$, truncate above if $d$ is upper bounded,\n$$p(\\zeta) = \\lambda \\exp\\{-\\lambda \\cdot d(\\zeta)\\} \\left| \\frac{\\partial d(\\zeta)}{\\partial \\zeta}\\right|.$$\n\n## Fixing penalized complexity hyperparameter\n \nPick rate $\\lambda$ to control prior density in the tail, by specifying a value for (a transformation of) the parameter, say $g(\\zeta)$, which is interpretable. \n\nElicit values of $Q$ and small probability $\\alpha$ such that the tail probability \n$$\\Pr\\{g(\\zeta) > Q\\} = \\alpha.$$\n \n## Penalized complexity prior for random effect scale\n\nIf $\\alpha_j \\sim \\mathsf{No}(0, \\zeta^2)$, the penalized complexity prior is exponential with rate $\\lambda$.\n\nGiven $Q$ a high quantile of the standard deviation $\\zeta$, set $\\lambda = -\\ln(\\alpha/Q)$.\n\n\n \n## Priors for scale of random effects\n\nThe conjugate inverse gamma prior $p(1/\\zeta) \\sim \\mathsf{Ga}(\\alpha, \\beta)$ is such that the mode for $\\zeta$ is $\\beta/(1+\\alpha)$.\n\nOften, we take $\\beta=\\alpha = 0.01$ or $0.001$, but this leads to improper prior. So small values are not optimal for 'random effects', and this prior cannot provide shrinkage or allow for no variability between groups.\n \n## Priors for scale of random effects\n\nA popular suggestion, due to @Gelman:2006, is to take a centered Student-$t$ distribution with $\\nu$ degrees of freedoms, truncated over $[0, \\infty)$ with scale $s$.\n\n- since the mode is at zero, provides support for the base model\n- we want small degrees of freedom $\\nu$, preferable to take $\\nu=3$? Cauchy model ($\\nu=1$) still popular.\n \n## Prior sensitivity\n\nDoes the priors matter? As robustness check, one can fit the model with\n\n- different priors function \n- different hyperparameter values\n\nCostly, but may be needed to convince reviewers ;)\n\n## Distraction from smartwach\n\nWe consider an experimental study conducted at Tech3Lab on road safety. \n\n- In @Brodeur:2021, 31 participants were asked to drive in a virtual environment.\n- The number of road violation was measured for 4 different type of distractions (phone notification, phone on speaker, texting and smartwatch).\n- Balanced data, random order of tasks\n\n## Poisson mixed model\n\nWe model the number of violations, `nviolation` as a function of distraction type (`task`) and participant `id`.^[Specifically, $\\beta_j$ is the coefficient for `task` $j$ (distraction type) and $\\alpha_i$ is the random effect of participant $i$.]\n\\begin{align*}\n\\texttt{nviolation}_{ij} &\\sim \\mathsf{Po}(\\mu_{ij})\\\\\n\\mu_{ij} &= \\exp(\\beta_{j} + \\alpha_i),\\\\\n\\beta_j &\\sim \\mathsf{No}(0, 100), \\\\\n\\alpha_i &\\sim \\mathsf{No}(0, \\kappa^2).\n\\end{align*}\n\n## Priors for random effect scale\n\nConsider different priors for $\\kappa$\n\n- flat uniform prior $\\mathsf{U}(0,10)$\n- conjugate inverse gamma $\\mathsf{IG}(0.01, 0.01)$ prior\n- a Student-$t$ with $\\nu=3$ degrees of freedom\n- a penalized complexity prior such that the 0.95 percentile of the scale is 5, corresponding to $\\mathsf{Exp}(0.6)$. \n\n## Sensitivity analysis for smartwatch data\n\n\n::: {.cell hash='bayesmod-slides2_cache/revealjs/fig-posterior-kappa_4ea8bc26feb288556dbad518cac1d985'}\n::: {.cell-output-display}\n![Posterior density of $\\zeta$ for four different priors. The circle denotes the median and the bars the 50% and 95% percentile credible intervals.](fig/fig-sensitivity.png){#fig-posterior-kappa width=672}\n:::\n:::\n\n\nBasically indistinguishable results for the random scale..\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}