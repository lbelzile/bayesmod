{
  "hash": "accfe823992da1394d8630503e2bbfe1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"Léo Belzile\"\nsubtitle: \"Hamiltonian Monte Carlo and probabilistic programming\"\ndate: today\ndate-format: \"[Last compiled] dddd MMM D, YYYY\"\neval: true\necho: true\ncache: true\nbibliography: MATH80601A.bib\nformat:\n  revealjs:\n    slide-number: true\n    html-math-method: mathjax\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n    logo: \"fig/logo_hec_montreal_bleu_web.png\"\n---\n\n\n\n\n\n## Curse of dimensionality\n\nThis material is drawn from\n\n- @Neal:2011,\n- @Betancourt:2017\n\nCheck out these [animations](https://chi-feng.github.io/mcmc-demo/) by Chi Feng\n\n\n## Motivation\n\nWe are interested in calculating expectations of some function $g$ against the posterior.\n\\begin{align*}\n\\int_{\\mathbb{R}^d} g(\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}.\n\\end{align*}\n\nThe integral is determined by the product of the ``volume'' of $g(\\cdot)$ and the density.\n\n## Curse of dimensionality\n\nAs the dimension of the posterior, $d$, grows, the mass concentrates in a small region, the so-called **typical set**. The number of regions/directions to consider increases exponentially in $d$.\n\nIf we start at the stationary distribution, most proposals from a random walk Metropolis will fall outside of the typical set and get rejected.\n\nThis phenomenon also explains the decrease in performance of numerical integration schemes (quadrature).\n\n\n## Better informed proposals\n\nFor differentiable targets, we saw that we can do better than random walk Metropolis--Hastings.\n\n- Idea: use the gradient to make an informed proposal (e.g., in MALA)\n- There are two remaining challenges.\n     - it makes a single step from the current position. But why stop at one?\n     - the gradient needs not be aligned with the typical set (Betancourt analogy with satellite in orbit).\n\n\n## Hamiltonian Monte Carlo\n\nHamitonian Monte Carlo borrows ideas from Hamiltonian dynamics.\n\nConsider the evolution over time of a particle characterized by a\n\n- position $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ along with potential energy $U(\\boldsymbol{\\theta})=- \\log p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$\n- an added auxiliary vector $\\boldsymbol{s} \\in \\mathbb{R}^d$ of **momentum** (describing mass and velocity) with accompaying kinetic energy $K(\\boldsymbol{s})= -\\log p(\\boldsymbol{s})$.\n\n## Hamiltonian\n\nWrite the negative of the log of the joint density as\n$$H(\\boldsymbol{\\theta}, \\boldsymbol{s}) = -\\log p(\\boldsymbol{s}) - \\log p(\\boldsymbol{\\theta}) = U(\\boldsymbol{\\theta}) + K(\\boldsymbol{s}).$$\n\nThe partial derivatives of the Hamiltonian give the evolution over time of the system:\n\\begin{align*}\n\\frac{\\mathrm{d} \\theta_j}{\\mathrm{d} t} = \\frac{\\partial H}{\\partial s_j} &= \\frac{\\partial K}{\\partial s_j}\\\\\n \\frac{\\mathrm{d} s_j}{\\mathrm{d} t}= - \\frac{\\partial H}{\\partial \\theta_j} &= - \\frac{\\partial U}{\\partial \\theta_j}, \\quad j =1, \\ldots, d.\n\\end{align*}\n\nThere is no explicit solution to these differential equations in most settings.\n\n## Kinetic energy\n\nThe most popular choice of kinetic energy is the Gaussian,\n\\begin{align*}\nK(\\boldsymbol{s}) = \\frac{1}{2} \\boldsymbol{s}^\\top \\mathbf{M}^{-1}\\boldsymbol{s}\n\\end{align*}\nthe negative of a mean zero log Gaussian density with positive-definite covariance matrix $\\mathbf{M}.$\n\nTypically, we take $\\mathbf{M}=\\mathrm{diag}\\{m_1, \\ldots, m_d\\}$ diagonal, or else proportional $\\mathbf{M} = m \\mathbf{I}_d$.\n\n\n## Properties of Hamiltonian dynamics\n\nThe mapping $T_s$ from time $t$ at $(\\boldsymbol{\\theta}(t), \\boldsymbol{s}(t))$ to time $t + \\varepsilon$, $(\\boldsymbol{\\theta}(t + \\varepsilon), \\boldsymbol{s}(t + \\varepsilon))$ satisfies the following properties:\n\n\n- Reversible: MCMC will thus preserve the invariant target distribution\n- Conservation of energy: proposals from Hamiltonian dynamics would lead to acceptance probability of 1.\n- Symplecticness/volume preserving: the Jacobian of $T_s$ is one --- no need to calculate it.\n\n## A necessary discretization step\n\nThere is no explicit solution to the Hamiltonian differential equation. We must move away from continuous time...\n\n- For solving the differential equation numerically, Euler's method doesn't work because it does not preserve volume, and this leads to divergences.\n\n## Leapfrog integrator\n\nThe leapfrog integrator performs a half step for momentum, then does a full step for the position using the updated components, etc.\n\n\\begin{align*}\ns_j(t+\\varepsilon/2) &= s_j(t) - \\frac{\\varepsilon}{2}  \\left.\\frac{\\partial U(\\boldsymbol{\\theta})}{\\partial \\theta_j}\\right|_{\\boldsymbol{\\theta}(t)}\n\\\\\n\\theta_j(t+\\varepsilon) &= \\theta_j(t)  + \\varepsilon \\frac{s_j(t+\\varepsilon/2)}{m_j} \\\\\ns_j(t+\\varepsilon) &= s_j(t+\\varepsilon/2) - \\frac{\\varepsilon}{2}  \\left.\\frac{\\partial U(\\boldsymbol{\\theta})}{\\partial \\theta_j}\\right|_{\\boldsymbol{\\theta}(t + \\varepsilon)}\n\\end{align*}\n\n\n## Hamiltonian Monte Carlo algorithm\n\nConsider the joint distribution with positions $\\boldsymbol{\\theta}$ and momentum variables $\\boldsymbol{s}$, $p(\\boldsymbol{\\theta}, \\boldsymbol{s}) \\propto \\exp \\{- H(\\boldsymbol{\\theta}, \\boldsymbol{s})\\}.$\n\nWe start with a position vector $\\boldsymbol{\\theta}_{t-1}$ at step $t-1$:\n\n\n\n1. Sample a new momentum vector $\\boldsymbol{s}_{t-1} \\sim \\mathsf{Gauss}(\\boldsymbol{0}_d, \\mathbf{M}).$\n2. Use Verlet's (leapfrog) integrator to evolve the state vector for   $L=\\lfloor\\tau/\\varepsilon\\rfloor$ steps of size $\\varepsilon$ to get a proposal tuple $(\\boldsymbol{\\theta}_t^{\\star}, \\boldsymbol{s}_t^{\\star})$\n\n## Hamiltonian Monte Carlo algorithm\n\n3. Flip the momentum variable, $\\boldsymbol{s} \\mapsto - \\boldsymbol{s}.$\n4. Metropolis step: if $U \\sim \\mathsf{unif}(0,1) <R$, where\n$$\\log R = -H(\\boldsymbol{\\theta}^{\\star}, \\boldsymbol{s}^{\\star}_{t}) + H(\\boldsymbol{\\theta}_{t-1}, \\boldsymbol{s}_{t-1}),$$\nset $\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_t^{\\star}$, else keep the previous value and set $\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1}$.\n5. Discard the momentum vector\n\n\n## Tuning\n\nHamiltonian Monte Carlo (HMC) has numerous tuning parameters\n\n\n1. size of the leapfrog step $\\varepsilon$.\n2. length of the integration time $\\tau$ (or equivalently the number of steps $L=\\lfloor \\tau / \\varepsilon \\rfloor$).\n    - too small leads HMC to bear close resemblance to random walk, \n    - too large leads to wasteful calculations.\n3. choice of the mass matrix $\\mathbf{M}$ (pre-conditioner obtained during warmup period).\n\n\n\n## Leapfrog and error\n\n  The Störmer--Verlet (leapfrog) integrator is a second order method, so for step size $\\varepsilon$:\n\n- local error $\\mathrm{O}(\\varepsilon^3)$ and\n- global error of size $\\mathrm{O}(\\varepsilon^2)$ (accumulated error over $L$ steps).\n\nLeapfrog updates one variable at a time, a shear transformation.\n\nLeapfrog step should be $\\mathrm{O}(d^{-1/4})$ [@Beskos:2013]\n\n## Optimal acceptance rate\n\nIn practice, we use a Metropolis step to adjust for the discretization of the system.\n\n- This leads to acceptance rates less than the theoretical value of 1.\n- with optimal acceptance rate of $0.651$ [@Beskos:2013]; see @Neal:2011 for heuristics.\n- software like Stan tunes to around 0.8, but can be adjusted in settings.\n\n## It's nuts!\n\n- @Homan.Gelman:2014 propose the no $U$-turn sampler (NUTS), which continues the trajectory until the sampler turns back, to determine the number of steps $L$, along with tuning of $\\varepsilon.$ \n- Stan uses an adaptation of NUTS due to @Betancourt:2016\n\n## HMC and divergences\n\nIn theory, the energy of the Hamiltonian should stay constant, but the numerical scheme leads to small perturbations (hence the rejection step).\n\n- If the value of the Hamiltonian changes too much, this is identified as a **divergence**. These occur when the geometry of the posterior is heavily constrained (funnel shaped).\n- Reparametrization of the model can help improve this: see the [Stan  manual](https://mc-stan.org/docs/stan-users-guide/efficiency-tuning.html#reparameterization.section).\n\n## Neal's funnel\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](fig/fig-Neal-funnel.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n## Achieving independence\n\nWe have seen that for differentiable posterior $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$, using the gradient information can improve convergence by informing about the direction of the mode.\n\n- @Neal:2011 discusses how informally, random walk Metropolis requires $\\mathrm{O}(d^2)$ steps to get an independent draw, compared to $\\mathrm{O}(d^{4/3})$ for MALA.\n- HMC scales like $\\mathrm{O}(d^{5/4})$, a notable improvement in performance. \n- It however comes at the cost of repeated gradient evaluations ($L$ by update).\n\n## Take-home\n\n- HMC is more efficient than what we have seen, but not a silver bullet: it works very well for not overly complicated models and moderate sample sizes.\n- HMC works better than many MCMC, but requires special tuning best left to specialized implementations already available in software.\n- Most implementations don't cover the case of discrete random variables [@Nishimura:2020].\n\n\n## Probabilistic programming\n\nThere are several languages and interfaces that implement probabilistic programming where the user has only to specify the likelihood and prior.\n\nHistorically, [Bugs](https://www.mrc-bsu.cam.ac.uk/software/bugs-project) paved the way to practitioners.\n\nIt relies on Gibbs sampling (updating one parameter at the time), but is not actively developed. Still the source of many exercises and inspiration for the syntax of other implementations (e.g., Nimble, JAGS).\n\n# Stan\n\nThe programming language [Stan](https://mc-stan.org/) is written in C++ and offers cross-platform interfaces.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](fig/Stan.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n## Other MCMC software\n\n- [Nimble](https://r-nimble.org/), C++ with **R** interface\n- [Turing.jl](https://turinglang.org/docs/getting-started/), Julia\n- [PyMC](https://www.pymc.io/welcome.html), Python\n- [Pigeons](https://pigeons.run/stable/), Julia\n\n## Stochastic volatility model\n\nFinancial returns $Y_t$ typically exhibit time-varying variability. The **stochastic volatility** model is a parameter-driven model that specifies\n\\begin{align*}\nY_t = \\exp(h_t/2) Z_t \\\\\nh_t = \\gamma + \\phi (h_{t-1} - \\gamma) + \\sigma U_t\n\\end{align*}\nwhere $U_t \\stackrel{\\mathrm{iid}}{\\sim} \\mathsf{Gauss}(0,1)$ and $Z_t \\sim  \\stackrel{\\mathrm{iid}}{\\sim} \\mathsf{Gauss}(0,1).$\n\nIt is possible to introduce leverage by adding $\\mathsf{Cor}(Z_t, U_t = \\rho.$\n\n\n## Nimble code\nSee the package [vignette](https://r-nimble.org/nimbleExamples/stochastic_volatility.html)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstochVolCode <- nimbleCode({\n  x[1] ~ dnorm(mu, sd = sigma / sqrt(1-phi*phi))\n  y[1] ~ dnorm(0, sd = beta * exp(0.5 * x[1]))\n  for (t in 2:T){\n    x[t] ~ dnorm(mu + phi * (x[t-1] - mu), sd = sigma)\n    y[t] ~ dnorm(0, sd = beta * exp(0.5 * x[t]))\n  }\n  phi ~ dunif(-1, 1)\n  sigma ~ dt(mu = 0, sd = 5, df = 1)\n  mu ~ dt(mu = 0, sd = 10, df = 1)\n  beta <- exp(0.5*mu)\n})\n```\n:::\n\n\n\n## Stan code for stochastic volatility\n\ndata {\n  int<lower=0> T;   // # sample size\n  vector[T] y;      // mean zero return at time t\n}\nparameters {\n  real mu;                     // mean log volatility\n  real<lower=-1, upper=1> phi; // persistence of volatility, force stationarity\n  real<lower=0> sigma;         // white noise shock scale\n  vector[T] h;                 // log volatility at time t\n}\nmodel {\n  phi ~ uniform(-1, 1);\n  sigma ~ cauchy(0, 5);\n  mu ~ cauchy(0, 10);\n  h[1] ~ normal(mu, sigma / sqrt(1 - phi * phi)); // unconditional distribution\n  for (t in 2:T) {\n    h[t] ~ normal(mu + phi * (h[t - 1] -  mu), sigma);\n  }\n  for (t in 1:T) {\n    y[t] ~ normal(0, exp(h[t] / 2));\n  }\n}\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}