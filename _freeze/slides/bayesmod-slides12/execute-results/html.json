{
  "hash": "62e7414dead7d507c797183e9ebe0837",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"LÃ©o Belzile\"\nsubtitle: \"Final review\"\ndate: today\ndate-format: \"[Last compiled] dddd MMM D, YYYY\"\neval: true\necho: true\ncache: true\nbibliography: MATH80601A.bib\nformat:\n  revealjs:\n    slide-number: true\n    html-math-method: mathjax\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n    logo: \"fig/logo_hec_montreal_bleu_web.png\"\n---\n\n\n\n\n\n## Fundamentals\n\n- Bayesian inference uses likelihood based inference.\n- It complements the likelihood $p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})$ with a prior $p(\\boldsymbol{\\theta})$.\n- Provided that $p(\\boldsymbol{\\theta}, \\boldsymbol{y})$ is integrable, we get\n\\begin{align*}\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\stackrel{\\boldsymbol{\\theta}}{\\propto} p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}).\n\\end{align*}\n\n## Marginal likelihood\n\nThe normalizing constant \n\\begin{align*}\np(\\boldsymbol{y}) = \\int_{\\boldsymbol{\\Theta}} p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\nto make the posterior a valid density is termed **marginal likelihood**.\n\n## Marginal likelihood\n\nMoments of the posterior depend on $p(\\boldsymbol{y})$. \n\nIt is hard to compute because $\\boldsymbol{\\Theta} \\subseteq \\mathbb{R}^p$, and the integral is often high-dimensional.\n\n- Monte Carlo integration (does not typically work because prior need not align with likelihood)\n- Numerical integration performance degrades with $p$, numerical overflow.\n\n\n\n## Bayes factors \n\nThe $\\color{#6e948c}{\\text{Bayes factor}}$ is the ratio of marginal likelihoods, as\n\\begin{align*}\np(\\boldsymbol{y} \\mid \\mathcal{M}_i) = \\int p(y \\mid \\boldsymbol{\\theta}^{(i)}, \\mathcal{M}_i) p( \\boldsymbol{\\theta}^{(i)} \\mid \\mathcal{M}_i) \\mathrm{d}  \\boldsymbol{\\theta}^{(i)}.\n\\end{align*}\nValues of $\\mathsf{BF}_{ij}>1$ correspond to model $\\mathcal{M}_i$ being more likely than $\\mathcal{M}_j$.\n\n- Strong dependence on the prior $p(\\boldsymbol{\\theta}^{(i)} \\mid \\mathcal{M}_i)$.\n- Must use proper priors.\n\n## Predictive distributions\n\nDefine the $\\color{#D55E00}{\\text{posterior predictive}}$,\n\\begin{align*}\np(y_{\\text{new}}\\mid \\boldsymbol{y}) = \\int_{\\boldsymbol{\\Theta}} p(y_{\\text{new}} \\mid \\boldsymbol{\\theta}) \\color{#D55E00}{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})} \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\n\n## Bayesian inference\n\nIf we have samples from $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$ or an approximation of the joint/marginals, then we can\n\n- use the **posterior** distribution to answer any question that is a function of $\\boldsymbol{\\theta}$ alone.\n- use the **posterior predictive** $p(y_{\\text{new}}\\mid \\boldsymbol{y})$ for prediction or forecasting, and checks of model adequacy.\n\n## Point estimators and credible regions\n\nInterpretation is different from frequentist, but methods are similar:\n\n- point estimators (MAP, posterior mean and median, etc.) derive from consideration of **loss functions** that return a summary of the posterior.\n- credible interval or regions (interval for which the true parameter lies with a certain probability).\n\n## Stochastic approximations\n\nStochastic approximations rely on sampling methods (rejection sampling, MCMC)\n\n- returns (correlated) posterior samples.\n- Metropolis--Hastings acceptance ratio bypasses marginal likelihood calculation.\n- Marginalization is straightforward.\n\n## Markov chains\n\n- Need to assess convergence to the stationary distribution (traceplots)\n- Autocorrelation reduces precision of Monte Carlo estimates (**effective sample size**)\n\n## Markov chain Monte Carlo algorithms\n\nWe covered in class the following (in decreasing order of efficiency).\n\n- random walk Metropolis\n- Metropolis-adjusted Langevin algorithm (MALA)\n- Hamiltonian Monte Carlo\n\nBetter sampling performance, but the latter two require gradient and are more expensive to compute.\n\n## Model selection\n\n- Bernstein-von Mises ensures convergence in total variation of the posterior under weak conditions.\n- Distinguish between \n    - $\\mathcal{M}$-closed: true parameter is part of set considered or \n    - $\\mathcal{M}$-open: only misspecified models are considered.\n- The model that gets selected minimizes the Kullback--Leibler divergence with the truth.\n- In discrete parameter settings, we recover the truth with probability 1.\n\n## Priors\n\n- Priors don't matter in large sample on the data layer, as likelihood is $\\mathrm{O}(n)$ vs $\\mathrm{O}(1)$ for the prior.\n- Support constraints have an impact\n- Their impact depends largely on how far they are from the data.\n- Prior sensitivity check: compare posterior vs prior density\n\n## Type of priors\n\n- Different roles (expert opinion, simplification of calculations, regularization).\n- Conditional conjugacy mostly useful for Gibbs sampling, etc.\n- Careful with improper priors (unless they are known to yield valid posterior).\n- Prefer weak priors to near improper priors.\n\n## Prior selection\n\n- Moment matching\n- Prior predictive distribution: draw new observations from likelihood and plot\n\n## \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}