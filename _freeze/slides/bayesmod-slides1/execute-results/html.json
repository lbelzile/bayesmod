{
  "hash": "6143b2bc3de0e622f35e49cbdaaba53c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"Léo Belzile, HEC Montréal\"\nsubtitle: \"Introduction\"\ndate: today\ndate-format: YYYY\neval: true\ncache: true\necho: true\nstandalone: true\nwidth: 1200\nheight: 900\nbibliography: MATH80601A.bib\nformat:\n  revealjs:\n    slide-number: true\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n    logo: \"fig/logo_hec_montreal_bleu_web.png\"\n---\n\n\n\n\n\n\n\n## Distribution and density function\n\nLet $\\boldsymbol{X} \\in \\mathbb{R}^d$ be a random vector with distribution function\n\\begin{align*}\n F_{\\boldsymbol{X}}(\\boldsymbol{x}) = \\Pr(\\boldsymbol{X} \\leq \\boldsymbol{x}) = \\Pr(X_1 \\leq x_1, \\ldots, X_d \\leq x_d).\n\\end{align*}\n\nIf the distribution of $\\boldsymbol{X}$ is absolutely continuous, \n\\begin{align*}\n F_{\\boldsymbol{X}}(\\boldsymbol{x}) = \\int_{-\\infty}^{x_d} \\cdots \\int_{-\\infty}^{x_1} f_{\\boldsymbol{X}}(z_1, \\ldots, z_d) \\mathrm{d} z_1 \\cdots \\mathrm{d} z_d,\n\\end{align*}\nwhere $f_{\\boldsymbol{X}}(\\boldsymbol{x})$ is the joint **density function**.\n\n## Mass function\n\nBy abuse of notation, we denote the mass function in the discrete case \n$$0 \\leq f_{\\boldsymbol{X}}(\\boldsymbol{x}) = \\Pr(X_1 = x_1, \\ldots, X_d = x_d) \\leq 1.$$\n\nThe support is the set of non-zero density/probability total probability over all points in the support,\n$$\\sum_{\\boldsymbol{x} \\in \\mathsf{supp}(\\boldsymbol{X})} f_{\\boldsymbol{X}}(\\boldsymbol{x}) = 1.$$\n\n## Marginal distribution\n\nThe **marginal distribution** of a subvector $\\boldsymbol{X}_{1:k}=(X_1, \\ldots, X_k)^\\top$ is\n\\begin{align*}\n F_{\\boldsymbol{X}_{1:k}}(\\boldsymbol{x}_{1:k}) &= \\Pr(\\boldsymbol{X}_{1:k} \\leq \\boldsymbol{x}_{1:k}) \\\\&= F_{\\boldsymbol{X}}(x_1, \\ldots, x_k, \\infty, \\ldots, \\infty).\n\\end{align*}\n\n## Marginal density\n\nThe **marginal density** $f_{\\boldsymbol{X}_{1:k}}(\\boldsymbol{x}_{1:k})$ of an absolutely continuous subvector $\\boldsymbol{X}_{1:k}=(X_1, \\ldots, X_k)^\\top$ is\n\\begin{align*}\n \\int_{-\\infty}^\\infty \\cdots  \\int_{-\\infty}^\\infty  f_{\\boldsymbol{X}}(x_1, \\ldots, x_k, z_{k+1}, \\ldots, z_{d}) \\mathrm{d} z_{k+1} \\cdots \\mathrm{d}z_d.\n\\end{align*}\nthrough integration from the joint density.\n\n\n\n## Conditional distribution\n\nThe conditional distribution function of $\\boldsymbol{Y}$ given $\\boldsymbol{X}=\\boldsymbol{x}$, is \n\\begin{align*}\nf_{\\boldsymbol{Y} \\mid \\boldsymbol{X}}(\\boldsymbol{y}; \\boldsymbol{x}) = \\frac{f_{\\boldsymbol{X}, \\boldsymbol{Y}}(\\boldsymbol{x}, \\boldsymbol{y})}{f_{\\boldsymbol{X}}(\\boldsymbol{x})}\n\\end{align*}\nfor any value of $\\boldsymbol{x}$ in the support of $\\boldsymbol{X}$.\n\n\n\n## Conditional and marginal for contingency table\n\nConsider a bivariate distribution for $(Y_1, Y_2)$ supported on $\\{1,2,3\\} \\times \\{1, 2\\}$ whose joint probability mass function is given in @tbl-bivardiscrete\n\n\n\n::: {#tbl-bivardiscrete .cell tbl-cap='Bivariate mass function with probability of each outcome for $(Y_1, Y_2)$.' tbl-align='center'}\n::: {.cell-output-display}\n\n\n|           | $Y_1=1$| $Y_1=2$| $Y_1=3$| row total|\n|:----------|-------:|-------:|-------:|---------:|\n|$Y_2=1$    |    0.20|     0.3|    0.10|       0.6|\n|$Y_2=2$    |    0.15|     0.2|    0.05|       0.4|\n|col. total |    0.35|     0.5|    0.15|       1.0|\n\n\n:::\n:::\n\n\n\n## Calculations for the marginal distribution\n\nThe marginal distribution of $Y_1$ is obtain by looking at the total probability for each row/column, e.g., $$\\Pr(Y_1=i) = \\Pr(Y_1=i, Y_2=1)+ \\Pr(Y_1=i, Y_2=2).$$ \n\n- $\\Pr(Y_1=1)=0.35$, $\\Pr(Y_1=2)=0.5$, $\\Pr(Y_1=3) = 0.15$. \n- $\\Pr(Y_2=1)=0.6$ and $\\Pr(Y_2=2)=0.4$\n\n## Conditional distribution\n\nThe conditional distribution $$\\Pr(Y_2 = i \\mid Y_1=2) = \\frac{\\Pr(Y_1=2, Y_2=i)}{\\Pr(Y_1=2)},$$ so \n\\begin{align*}\n\\Pr(Y_2 = 1 \\mid Y_1=2) &= 0.3/0.5 = 0.6\n\\\\ \\Pr(Y_2=2 \\mid Y_1=2) &= 0.4.\n\\end{align*}\n\n\n## Independence\n\nVectors $\\boldsymbol{Y}$ and $\\boldsymbol{X}$ are independent if\n\\begin{align*}\nF_{\\boldsymbol{X}, \\boldsymbol{Y}}(\\boldsymbol{x}, \\boldsymbol{y}) = F_{\\boldsymbol{X}}(\\boldsymbol{x})F_{\\boldsymbol{Y}}(\\boldsymbol{y})\n\\end{align*}\nfor any value of $\\boldsymbol{x}$, $\\boldsymbol{y}$. \n\nThe joint density, if it exists, also factorizes\n\\begin{align*}\nf_{\\boldsymbol{X}, \\boldsymbol{Y}}(\\boldsymbol{x}, \\boldsymbol{y}) = f_{\\boldsymbol{X}}(\\boldsymbol{x})f_{\\boldsymbol{Y}}(\\boldsymbol{y}).\n\\end{align*}\n\nIf two subvectors $\\boldsymbol{X}$ and $\\boldsymbol{Y}$ are independent, then the conditional density $f_{\\boldsymbol{Y} \\mid \\boldsymbol{X}}(\\boldsymbol{y}; \\boldsymbol{x})$ equals the marginal $f_{\\boldsymbol{Y}}(\\boldsymbol{y})$.\n\n## Law of iterated expectation and variance\n\nLet $\\boldsymbol{Z}$ and $\\boldsymbol{Y}$ be random vectors. The expected value of $\\boldsymbol{Y}$ is\n\\begin{align*}\n\\mathsf{E}_{\\boldsymbol{Y}}(\\boldsymbol{Y}) = \\mathsf{E}_{\\boldsymbol{Z}}\\left\\{\\mathsf{E}_{\\boldsymbol{Y} \\mid \\boldsymbol{Z}}(\\boldsymbol{Y})\\right\\}.\n\\end{align*}\n\nThe **tower** property gives a law of iterated variance \n\\begin{align*}\n\\mathsf{Va}_{\\boldsymbol{Y}}(\\boldsymbol{Y}) = \\mathsf{E}_{\\boldsymbol{Z}}\\left\\{\\mathsf{Va}_{\\boldsymbol{Y} \\mid \\boldsymbol{Z}}(\\boldsymbol{Y})\\right\\} + \\mathsf{Va}_{\\boldsymbol{Z}}\\left\\{\\mathsf{E}_{\\boldsymbol{Y} \\mid \\boldsymbol{Z}}(\\boldsymbol{Y})\\right\\}.\n\\end{align*}\n\n\n## Poisson distribution\n\nThe Poisson distribution has mass\n\\begin{align*}\nf(x)=\\mathsf{Pr}(Y=x) = \\frac{\\exp(-\\lambda)\\lambda^y}{\\Gamma(y+1)}, \\quad x=0, 1, 2, \\ldots\n\\end{align*}\nwhere $\\Gamma(\\cdot)$ denotes the gamma function. \n\nThe parameter $\\lambda$ of the Poisson distribution is both the expectation and the variance of the distribution, meaning $$\\mathsf{E}(Y)=\\mathsf{Va}(Y)=\\lambda.$$\n\n\n## Gamma distribution\n\nA gamma distribution with shape $\\alpha>0$ and rate $\\beta>0$, denoted $Y \\sim \\mathsf{gamma}(\\alpha, \\beta)$, has density\n\\begin{align*}\nf(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}\\exp(-\\beta x), \\qquad x \\in (0, \\infty),\n\\end{align*}\nwhere $\\Gamma(\\alpha)=\\int_0^\\infty t^{\\alpha-1}\\exp(-t)\\mathrm{d} t$ is the gamma function.\n\n\n## Poisson with random scale\n\nTo handle overdispersion in count data, take\n\n\\begin{align*}\nY \\mid \\Lambda &= \\lambda \\sim \\mathsf{Poisson}(\\lambda)\\\\\n\\Lambda &\\sim \\mathsf{Gamma}(k\\mu, k).\n\\end{align*}\n\nThe joint density of $Y$ and $\\Lambda$ on $\\mathbb{N}=\\{0,1,\\ldots\\} \\times \\mathbb{R}_{+}$ is\n\\begin{align*}\nf(y, \\lambda) &= f(y \\mid \\lambda)f(\\lambda) \\\\\n&= \\frac{\\lambda^y\\exp(-\\lambda)}{\\Gamma(y+1)}  \\frac{k^{k\\mu}\\lambda^{k\\mu-1}\\exp(-k\\lambda)}{\\Gamma(k\\mu)}\n\\end{align*}\n\n## Conditional distribution\n\nThe conditional distribution of $\\Lambda \\mid Y=y$ can be found by considering only terms that are function of $\\lambda$, whence\n\\begin{align*}\nf(\\lambda \\mid Y=y) \\stackrel{\\lambda}{\\propto}\\lambda^{y+k\\mu-1}\\exp\\{-(k+1)\\lambda\\}\n\\end{align*}\nso $\\Lambda \\mid Y=y \\sim \\mathsf{gamma}(k\\mu + y, k+1)$.\n\n## Marginal density of Poisson mean mixture\n\n\\begin{align*}\nf(y) &= \\frac{f(y,  \\lambda)}{f(\\lambda \\mid y)} = \\frac{\\frac{\\lambda^y\\exp(-\\lambda)}{\\Gamma(y+1)}  \\frac{k^{k\\mu}\\lambda^{k\\mu-1}\\exp(-k\\lambda)}{\\Gamma(k\\mu)}}{ \\frac{(k+1)^{k\\mu+y}\\lambda^{k\\mu+y-1}\\exp\\{-(k+1)\\lambda\\}}{\\Gamma(k\\mu+y)}}\\\\\n&= \\frac{\\Gamma(k\\mu+y)}{\\Gamma(k\\mu)\\Gamma(y+1)}k^{k\\mu} (k+1)^{-k\\mu-y}\\\\&= \\frac{\\Gamma(k\\mu+y)}{\\Gamma(k\\mu)\\Gamma(y+1)}\\left(1-\\frac{1}{k+1}\\right)^{k\\mu} \\left(\\frac{1}{k+1}\\right)^y\n\\end{align*}\nMarginally, $Y \\sim \\mathsf{neg. binom}(p)$ where $p=(k+1)^{-1}.$\n\n## Change of variable formula\n\nConsider an injective (one-to-one) differentiable function $\\boldsymbol{g}: \\mathbb{R}^d \\to \\mathbb{R}^d,$ with inverse $g^{-1}.$ Then, if $\\boldsymbol{Y}=\\boldsymbol{g}(\\boldsymbol{X}),$\n\\begin{align*}\n\\Pr(\\boldsymbol{Y} \\leq \\boldsymbol{y}) = \\Pr\\{\\boldsymbol{g}(\\boldsymbol{X}) \\leq \\boldsymbol{y}\\} = \\Pr\\{\\boldsymbol{X} \\leq \\boldsymbol{x} = \\boldsymbol{g}^{-1}(\\boldsymbol{y})\\}.\n\\end{align*}\n\nUsing the chain rule, the density of $\\boldsymbol{Y}$ is\n\\begin{align*}\nf_{\\boldsymbol{Y}}(\\boldsymbol{y}) = f_{\\boldsymbol{X}}\\left\\{\\boldsymbol{g}^{-1}(\\boldsymbol{y})\\right\\} \\left| \\mathbf{J}_{\\boldsymbol{g}^{-1}}(\\boldsymbol{y})\\right| = f_{\\boldsymbol{X}}(\\boldsymbol{x}) \\left| \\mathbf{J}_{\\boldsymbol{g}}(\\boldsymbol{x})\\right|^{-1}\n\\end{align*}\nwhere $\\mathbf{J}_{\\boldsymbol{g}}(\\boldsymbol{x})$ is the Jacobian matrix with $i,j$th element $\\partial [\\boldsymbol{g}(\\boldsymbol{x})]_i / \\partial x_j.$\n\n## Gaussian location-scale\n\nConsider $d$ independent standard Gaussian variates $X_j \\sim \\mathsf{Gauss}(0, \\sigma^2)$ for $j=1, \\ldots, d,$ with joint density function\n\\begin{align*}\nf_{\\boldsymbol{X}}(\\boldsymbol{x})= (2\\pi)^{-d/2} \\exp \\left( - \\frac{\\boldsymbol{x}^\\top\\boldsymbol{x}}{2}\\right).\n\\end{align*}\nConsider the transformation $\\boldsymbol{Y} = \\mathbf{A}\\boldsymbol{X}+\\boldsymbol{b},$ with $\\mathbf{A}$ an invertible matrix.\n\n## Change of variable for Gaussian\n\n- The inverse transformation is $\\boldsymbol{g}^{-1}(\\boldsymbol{y}) = \\mathbf{A}^{-1}(\\boldsymbol{y}-\\boldsymbol{b}).$\n- The Jacobian $\\mathbf{J}_{\\boldsymbol{g}}(\\boldsymbol{x})$ is simply $\\mathbf{A},$ so the joint density of $\\boldsymbol{Y}$ is\n\\begin{align*}\n(2\\pi)^{-d/2} |\\mathbf{A}|^{-1}\\exp \\left\\{ - \\frac{(\\boldsymbol{y}-\\boldsymbol{b})^\\top\\mathbf{A}^{-\\top}\\mathbf{A}^{-1}(\\boldsymbol{y}-\\boldsymbol{b})}{2}\\right\\}.\n\\end{align*}\nSince $|\\mathbf{A}^{-1}| = |\\mathbf{A}|^{-1}$ and $\\mathbf{A}^{-\\top}\\mathbf{A}^{-1} = (\\mathbf{AA}^\\top)^{-1},$ we recover $\\boldsymbol{Y} \\sim \\mathsf{Gauss}_d(\\boldsymbol{b}, \\mathbf{AA}^\\top).$\n\n\n## Partitioning of covariance matrices\n\nLet $\\boldsymbol{\\Sigma}$ be a $d \\times d$ positive definite covariance matrix. We define the precision matrix $\\boldsymbol{Q} = \\boldsymbol{\\Sigma}^{-1}.$ Suppose the\nmatrices are partitioned into blocks, \n\\begin{align*}\n\\boldsymbol{\\Sigma}= \n\\begin{pmatrix}\n\\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12} \\\\\n\\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22}\n\\end{pmatrix} \\text{ and }\n\\boldsymbol{\\Sigma}^{-1}= \\boldsymbol{Q} = \n\\begin{pmatrix} \n\\boldsymbol{Q}_{11} &\\boldsymbol{Q}_{12}\n\\\\ \\boldsymbol{Q}_{21} & \\boldsymbol{Q}_{22}\n\\end{pmatrix}\n\\end{align*}\nwith $\\dim(\\boldsymbol{\\Sigma}_{11})=k\\times k$ and $\\dim(\\boldsymbol{\\Sigma}_{22})=(d-k) \\times (d-k).$\n\n## Matrix identities\n\nThe following relationships hold:\n\n- $\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}=-\\boldsymbol{Q}_{11}^{-1}\\boldsymbol{Q}_{12}$ \n- $\\boldsymbol{\\Sigma}_{11}-\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\boldsymbol{\\Sigma}_{21}=\\boldsymbol{Q}_{11}^{-1}$\n- $\\det(\\boldsymbol{\\Sigma})=\\det(\\boldsymbol{\\Sigma}_{22})\\det(\\boldsymbol{\\Sigma}_{1|2})$ \nwhere $\\boldsymbol{\\Sigma}_{1|2}=\\boldsymbol{\\Sigma}_{11}-\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\boldsymbol{\\Sigma}_{21}.$\n\n## Gaussian subvectors\n\nLet $\\boldsymbol{Y} \\sim \\mathsf{Gauss}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ and consider the partition\n\\begin{align*}\n \\boldsymbol{Y} = \\begin{pmatrix} \\boldsymbol{Y}_1 \\\\ \\boldsymbol{Y}_2\\end{pmatrix}, \\quad \n \\boldsymbol{\\mu} = \\begin{pmatrix} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2\\end{pmatrix}, \\quad \n \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\boldsymbol{\\Sigma}_{11} & \\boldsymbol{\\Sigma}_{12}\\\\ \\boldsymbol{\\Sigma}_{21} & \\boldsymbol{\\Sigma}_{22}\\end{pmatrix},\n\\end{align*}\nwhere $\\boldsymbol{Y}_1$ is a $k \\times 1$ and $\\boldsymbol{Y}_2$ is a $(d-k) \\times 1$ vector for some $1\\leq k < d.$\n\n\n\n## Conditional distribution of Gaussian vectors\n\nThen, we have the conditional distribution \n\\begin{align*}\n\\boldsymbol{Y}_1 \\mid \\boldsymbol{Y}_2 =\\boldsymbol{y}_2 &\\sim \\mathsf{Gauss}_k(\\boldsymbol{\\mu}_1+\\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1}(\\boldsymbol{y}_2-\\boldsymbol{\\mu}_2), \\boldsymbol{\\Sigma}_{1|2})\n\\\\& \\sim  \\mathsf{Gauss}_k(\\boldsymbol{\\mu}_1-\\boldsymbol{Q}_{11}^{-1}\\boldsymbol{Q}_{12}(\\boldsymbol{y}_2-\\boldsymbol{\\mu}_2), \\boldsymbol{Q}^{-1}_{11})\n\\end{align*}\nand $\\boldsymbol{\\Sigma}_{1|2}=\\boldsymbol{\\Sigma}_{11}-\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\boldsymbol{\\Sigma}_{21}$ is the Schur complement of $\\boldsymbol{\\Sigma}_{22}.$\n\n## Likelihood\n\nThe **likelihood** $L(\\boldsymbol{\\theta})$ is a function of the parameter vector $\\boldsymbol{\\theta}$ that gives the 'density' of a sample under a postulated distribution, treating the observations as fixed,\n\\begin{align*}\nL(\\boldsymbol{\\theta}; \\boldsymbol{y}) = f(\\boldsymbol{y}; \\boldsymbol{\\theta}).\n\\end{align*}\n\n## Likelihood for independent observations\n\nIf the joint density factorizes,\n\\begin{align*}\nL(\\boldsymbol{\\theta}; \\boldsymbol{y})=\\prod_{i=1}^n f_i(y_i; \\boldsymbol{\\theta}) = f_1(y_1; \\boldsymbol{\\theta}) \\times \\cdots \\times f_n(y_n; \\boldsymbol{\\theta}).\n\\end{align*}\nThe corresponding log likelihood function for independent and identically distributions observations is\n\\begin{align*}\n\\ell(\\boldsymbol{\\theta}; \\boldsymbol{y}) = \\sum_{i=1}^n \\ln f(y_i; \\boldsymbol{\\theta})\n\\end{align*}\n\n## Score\n\nLet $\\ell(\\boldsymbol{\\theta}),$ $\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta} \\subseteq \\mathbb{R}^p,$ be the log likelihood function. The gradient of the log likelihood, termed **score** is the $p$-vector\n$$U(\\boldsymbol{\\theta}) = \\frac{\\partial \\ell(\\boldsymbol{\\theta})}{ \\partial \\boldsymbol{\\theta}}.$$\n\n## Information matrix\n\n\nThe **observed information matrix** is the hessian of the negative log likelihood,\n\\begin{align*}\nj(\\boldsymbol{\\theta}; \\boldsymbol{y})=-\\frac{\\partial^2 \\ell(\\boldsymbol{\\theta}; \\boldsymbol{y})}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^\\top},\n\\end{align*}\nevaluated at the maximum likelihood estimate $\\widehat{\\boldsymbol{\\theta}},$ so $j(\\widehat{\\boldsymbol{\\theta}}).$\n\n## Expected information\n\nUnder regularity conditions, the **expected information**, also called **Fisher information** matrix, is\n\\begin{align*}\ni(\\boldsymbol{\\theta}) = \\mathsf{E}\\left\\{U(\\boldsymbol{\\theta}; \\boldsymbol{Y}) U(\\boldsymbol{\\theta}; \\boldsymbol{Y})^\\top\\right\\} = \\mathsf{E}\\left\\{j(\\boldsymbol{\\theta}; \\boldsymbol{Y})\\right\\}\n\\end{align*}\n\n## Note on information matrices \n\nInformation matrices are symmetric and provide information about the variability of $\\widehat{\\boldsymbol{\\theta}}.$\n\nThe information of an iid sample of size $n$is $n$ times that of a single observation \n\n- information accumulates at a linear rate.\n\n\n## Example: random right-censoring\n\nConsider a survival analysis problem for independent time-to-event data subject to (noninformative) random right-censoring. We observe\n\n-  failure times $Y_i (i=1, \\ldots, n)$ drawn from $F(\\cdot; \\boldsymbol{\\theta})$ supported on $(0, \\infty)$\n- independent binary censoring indicators $C_i \\in \\{0,1\\}$, with $0$ indicating right-censoring and $C_i=1$ observed failure time. \n\n## Likelihood contribution with censoring\n\nIf individual observation $i$ has not experienced the event at the end of the collection period, then the likelihood contribution is $\\Pr(Y > y) = 1-F(y; \\boldsymbol{\\theta})$, where $y_i$ is the maximum time observed for $Y_i$. We write the log likelihood\n\\begin{align*}\n\\ell(\\boldsymbol{\\theta}) = \\sum_{i: c_i=0} \\log \\{1- F(y_i; \\boldsymbol{\\theta})\\} + \\sum_{i: c_i=1} \\log f(y_i; \\boldsymbol{\\theta})\n\\end{align*}\n\n## Censoring and exponential data\n\nSuppose for simplicity that $Y_i \\sim \\mathsf{expo}(\\lambda)$ and let $m=c_1 + \\cdots + c_n$ denote the number of observed failure times. Then, the log likelihood and the Fisher information are\n\\begin{align*}\n\\ell(\\lambda) &= \\lambda \\sum_{i=1}^n y_i + \\log \\lambda m\\\\\ni(\\lambda) &= m/\\lambda^2\n\\end{align*}\nand the right-censored observations for the exponential model do not contribute to the information.\n\n\n## Information for the Gaussian distribution\n\nConsider $Y \\sim \\mathsf{Gauss}(\\mu, \\tau^{-1})$, parametrized in terms of precision $\\tau$. The likelihood contribution for an $n$ sample is, up to proportionality,\n\\begin{align*}\n\\ell(\\mu, \\tau) \\propto \\frac{n}{2}\\log(\\tau) - \\frac{\\tau}{2}\\sum_{i=1}^n(Y_i^2-2\\mu Y_i+\\mu^2)\n\\end{align*}\n\n## Gaussian information matrices\n\nThe observed and Fisher information matrices are \n\\begin{align*}\nj(\\mu, \\tau) &= \\begin{pmatrix}\nn\\tau & -\\sum_{i=1}^n (Y_i-\\mu)\\\\\n-\\sum_{i=1}^n (Y_i-\\mu) & \\frac{n}{2\\tau^2}\n\\end{pmatrix}, \\\\\ni(\\mu, \\tau) &= n\\begin{pmatrix}\n\\tau & 0\\\\\n0 & \\frac{1}{2\\tau^2}\n\\end{pmatrix}\n\\end{align*}\nSince $\\mathsf{E}(Y_i) = \\mu$, the expected value of the off-diagonal entries of the Fisher information matrix are zero.\n\n## Example: first-order autoregressive process\n\nConsider an $\\mathsf{AR}(1)$ model of the form\n$$Y_t = \\mu + \\phi(Y_{t-1} - \\mu) + \\varepsilon_t,$$ where \n\n- $\\phi$ is the lag-one correlation, \n- $\\mu$ the global mean and \n- $\\varepsilon_t$ is an iid innovation with mean zero and variance $\\sigma^2$. \n\nIf $|\\phi| < 1$, the process is stationary, and the variance does not increase with $t$.\n\n## Markov property and likelihood decomposition\n\nThe Markov property states that the current realization depends on the past, $Y_t \\mid Y_1, \\ldots, Y_{t-1},$ only through the most recent value $Y_{t-1}.$ The log likelihood thus becomes\n\\begin{align*}\n\\ell(\\boldsymbol{\\theta}) = \\ln f(y_1) + \\sum_{i=2}^n f(y_i \\mid y_{i-1}).\n\\end{align*}\n\n## Marginal of AR(1)\n\n\nThe $\\mathsf{AR}(1)$ stationarity process has unconditional moments\n$$\\mathsf{E}(Y_t) = \\mu, \\qquad \\mathsf{Var}(Y_t)=\\sigma^2/(1-\\phi^2).$$ \n\nThe $\\mathsf{AR}(1)$ process is first-order Markov since the conditional distribution $f(Y_t \\mid Y_{t-1}, \\ldots, Y_{t-p})$ equals $f(Y_t \\mid Y_{t-1})$. \n\n## Log likelihood of AR(1)\n\nIf innovations are Gaussian, we have\n$$Y_t \\mid Y_{t-1}=y_{t-1} \\sim \\mathsf{Gauss}\\{\\mu(1-\\phi)+ \\phi y_{t-1}, \\sigma^2\\}, \\qquad t>1.$$\nso the log-likelihood is\n\\begin{align*}\n&\\ell(\\mu, \\phi,\\sigma^2)= -\\frac{n}{2}\\log(2\\pi) - n\\log \\sigma + \\frac{1}{2}\\log(1-\\phi^2) \\\\&\\quad -\\frac{(1-\\phi^2)(y_1- \\mu)^2}{2\\sigma^2} - \\sum_{i=2}^n \\frac{(y_t - \\mu(1-\\phi)- \\phi y_{t-1})^2}{2\\sigma^2}\n\\end{align*}\n\n## Moments\n\nBy the laws of iterated expectation and iterative variance, \n\\begin{align*}\n\\mathsf{E}(Y) &= \\mathsf{E}_{\\Lambda}\\{\\mathsf{E}(Y \\mid \\Lambda\\} \\\\& = \\mathsf{E}(\\Lambda) = \\mu\\\\\n\\mathsf{Va}(Y) &= \\mathsf{E}_{\\Lambda}\\{\\mathsf{Va}(Y \\mid \\Lambda)\\} + \\mathsf{Va}_{\\Lambda}\\{\\mathsf{E}(Y \\mid \\Lambda)\\} \\\\&= \\mathsf{E}(\\Lambda) + \\mathsf{Va}(\\Lambda) \\\\&= \\mu + \\mu/k.\n\\end{align*}\nThe marginal distribution of $Y$, unconditionally, has a variance which exceeds its mean.\n\n## Estimation of integrals\n\nSuppose we can simulate $B$ i.i.d. variables with the same distribution, $x_1, \\ldots, x_B$ with distribution $F$.\n\nWe want to compute $\\mathsf{E}\\{g(X)\\}=\\int g(x) f(x) \\mathrm{d} x=\\mu_g$ for some functional $g(\\cdot)$\n\n- $g(x)=x$ (posterior mean)\n- $g(x) = \\mathsf{I}(x \\in A)$ (probability of event)\n- etc.\n\n## Vanilla Monte Carlo integration\n\nWe substitute expected value by sample average of\n\\begin{align*}\n\\widehat{\\mu}_g = \\frac{1}{B} \\sum_{b=1}^B g(x_b).\n\\end{align*}\n\n- law of large number guarantees convergence of $\\widehat{\\mu}_g \\to \\mu_g$ if the latter is finite.\n- Under finite second moments, central limit theorem gives $$\\sqrt{B}(\\widehat{\\mu}_g - \\mu_g) \\sim \\mathsf{No}(0, \\sigma^2_g).$$\n\n## Importance sampling\n\nConsider density $q$ instead with $\\mathrm{supp}(p) \\subseteq \\mathrm{supp}(q).$ Then,\n\\begin{align*}\n\\mathsf{E}\\{g(X)\\} = \\int_{\\mathcal{X}} g(x) \\frac{p(x)}{q(x)} q(x) \\mathrm{d} x\n\\end{align*}\nand we can proceed similarly by drawing samples from $q$. \n\n\n## Importance sampling estimator\n\nAn alternative Monte Carlo estimator uses the weighted average\n\\begin{align*}\n\\widetilde{\\mathsf{E}}\\{g(X)\\} =\\frac{B^{-1} \\sum_{b=1}^B w_b g(x_b) }{B^{-1}\\sum_{b=1}^B w_b}.\n\\end{align*}\nwith weights $w_b = p(x_b)/q(x_b)$. The latter equal 1 on average, so one could omit the denominator without harm. \n\n## Standard errors\n\nIf the variance of $g(X)$ is finite, we can approximate the latter by the sample variance of the simple random sample and obtain the Monte Carlo standard error of the estimator\n\\begin{align*}\n\\mathsf{se}^2[\\widehat{\\mathsf{E}}\\{g(X)\\}] = \\frac{1}{B(B-1)} \\sum_{b=1}^B \\left[ g(x_b) -  \\widehat{\\mathsf{E}}\\{g(X)\\} \\right]^2.\n\\end{align*}\n\n## Precision of Monte Carlo integration\n\nWe want to have an estimator as precise as possible.\n\n- but we can't control the variance of $g(X)$, say $\\sigma_g^2$\n- the more simulations $B$, the lower the variance of the mean. \n- sample average for i.i.d. data has variance $\\sigma^2_g/B$\n- to reduce the standard deviation by a factor 10, we need $100$ times more draws!\n\n\nRemember: the answer is **random**.\n\n## Example: functionals of gamma distribution\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Running mean trace plots for $g(x)=\\mathrm{I}(x<1)$ (left), $g(x)=x$ (middle) and $g(x)=1/x$ (right) for a Gamma distribution with shape 0.5 and rate 2, as a function of the Monte Carlo sample size.](bayesmod-slides1_files/figure-revealjs/fig-monte-carlo-path-1.png){#fig-monte-carlo-path width=960}\n:::\n:::\n\n\n\n## Simulation algorithms: inversion method\n\nIf $F$ is an absolutely continuous distribution function, then \n$$F(X) \\sim \\mathsf{U}(0,1).$$\nThe inversion method consists in applying the quantile function $F^{-1}$ to $U \\sim \\mathsf{U}(0,1)$, viz. $$F^{-1}(U) \\sim X.$$\n\n## Inversion method for truncated distributions\n\nConsider a random variable $Y$ with distribution function $F$.\n\nIf $X$ follows the same distribution as $Y$, but restricted over the interval $[a,b]$, then \n$$\\Pr(X \\leq x) = \\frac{F(x) - F(a)}{F(b)-F(a)}, \\qquad a \\leq x \\leq b,$$\n\nTherefore, $$F^{-1}[F(a) + \\{F(b)-F(a)\\}U] \\sim X$$\n\n\n\n## Simulation algorithms: accept-reject\n\n- **Target**: sample from density $p(x)$ (hard to sample from)\n- **Proposal**: find a density $q(x)$ with nested support, $\\mathrm{supp}(p) \\subseteq \\mathrm{supp}(q)$, such that \n$$\\frac{p(x)}{q(x)} \\leq C, \\quad C \\geq 1.$$\n\n## Rejection sampling algorithm\n\n1. Generate $X$ from proposal with density $q(x)$.\n2. Compute the ratio $R \\gets p(X)/ q(X)$.\n3. If $CU \\leq R$ for $U \\sim \\mathsf{U}(0,1)$, return $X$, else go back to step 1.\n\n\n## Remarks on rejection sampling\n\n- Acceptance rate is $1/C$\n   - we need on average $C$ draws from $q$ to get one from $p$\n- $q$ must be more heavy-tailed than $p$\n   - e.g., $q(x)$ Student-$t$ for $p(x)$ Gaussian\n- $q$ should be cheap and easy to sample from!\n\n## Designing a good proposal density\n\nGood choices must satisfy the following constraints: \n\n- pick a family $q(x)$ so that $$C = \\mathrm{sup}_x \\frac{p(x)}{q(x)}$$ is as close to 1 as possible.\n- you can use numerical optimization with $f(x) =\\log p(x) - \\log q(x)$ to find the mode $x^\\star$ and the upper bound $C = \\exp f(x^\\star)$.\n\n## Accept-reject illustration\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Target density (full) and scaled proposal density (dashed): the vertical segment at $x=1$ shows the percentage of acceptance for a uniform slice under the scaled proposal, giving an acceptance ratio of 0.58.](bayesmod-slides1_files/figure-revealjs/fig-acceptreject-1.png){#fig-acceptreject width=960}\n:::\n:::\n\n\n\n## Truncated Gaussian via accept-reject\n\nConsider sampling $Y \\sim \\mathsf{No}(\\mu, \\sigma^2)$, but truncated in the interval $(a, b)$. The target density is\n\\begin{align*}\np(x; \\mu, \\sigma, a, b) = \\frac{1}{\\sigma}\\frac{\\phi\\left(\\frac{x-\\mu}{\\sigma}\\right)}{\\Phi(\\beta)-\\Phi(\\alpha)}.\n\\end{align*}\nfor $\\alpha= (a-\\mu)/\\sigma$ and $\\beta = (b-\\mu)/\\sigma$.\nwhere $\\phi(\\cdot), \\Phi(\\cdot)$ are respectively the density and distribution function of the standard Gaussian distribution.\n\n## Accept-reject (crude version)\n\n1. Simulate $X \\sim \\mathsf{No}(\\mu, \\sigma^2)$\n2. reject any draw if $X < a$ or $X> b$. \n\nThe acceptance rate is $C^{-1} = \\{\\Phi(\\beta) - \\Phi(\\alpha)\\}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standard Gaussian truncated on [0,1]\ncandidate <- rnorm(1e5)\ntrunc_samp <- candidate[candidate >= 0 & candidate <= 1]\n# Acceptance rate\nlength(trunc_samp)/1e5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.34242\n```\n\n\n:::\n\n```{.r .cell-code}\n# Theoretical acceptance rate\npnorm(1)-pnorm(0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3413447\n```\n\n\n:::\n:::\n\n\n\n## Accept-reject for truncated Gaussian  {.smaller}\n\nSince the Gaussian is a location scale family, the inversion method gives\n\\begin{align*}\nX \\sim \\mu + \\sigma\\Phi^{-1}\\left[\\Phi(\\alpha) + \\{\\Phi(\\beta)-\\Phi(\\alpha)\\}U\\right]\n\\end{align*}\n\nWe however need to evaluate $\\Phi$ numerically (no closed-form expression).\n\nThe method fails for *rare event* simulation because the computer returns\n\n- $\\Phi(x) = 0$ for $x \\leq -39$\n- $\\Phi(x)=1$ for $x \\geq 8.3$,\n\nimplying that $a \\leq 8.3$ for this approach to work [@LEcuyer.Botev:2017].\n\n## Simulating tails of Gaussian variables {.smaller}\n\nWe consider simulation from a standard Gaussian truncated above $a>0$\n\nWrite the density of the truncated Gaussian as  [@Devroye:1986, p.381]$$f(x) = \\frac{\\exp(-x^2/2)}{\\int_{a}^{\\infty}\\exp(-z^2/2)\\mathrm{d} z}  =\\frac{\\exp(-x^2/2)}{c_1}.$$\n\nNote that, for $x \\geq a$, \n$$c_1f(x) \\leq \\frac{x}{a}\\exp\\left(-\\frac{x^2}{2}\\right)= a^{-1}\\exp\\left(-\\frac{a^2}{2}\\right)g(x);$$\nwhere $g(x)$ is the density of a Rayleigh variable shifted by $a$.^[The constant $C= \\exp(-a^2/2)(c_1a)^{-1}$ approaches 1 quickly as $a \\to \\infty$ (asymptotically optimality). ]\n\n## Accept-reject: truncated Gaussian with Rayleigh  {.smaller}\n\nThe shifted Rayleigh has distribution function $$G(x) = 1-\\exp\\{(a^2-x^2)/2\\}, x \\geq a.$$ \n\n\n\n\n:::{.callout-important}\n\n## Marsaglia algorithm\n1. Generate a shifted Rayleigh  above $a$, $X \\gets  \\{a^2 - 2\\log(U)\\}^{1/2}$ for $U \\sim \\mathsf{U}(0,1)$\n2. Accept $X$ if $XV \\leq a$, where $V \\sim \\mathsf{U}(0,1)$.\n:::\n\nFor sampling on $[a,b]$, propose from a Rayleigh truncated above at $b$ [@LEcuyer.Botev:2017].\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- 8.3\nniter <- 1000L\nX <- sqrt(a^2 + 2*rexp(niter))\nsamp <- X[runif(niter)*X <= a]\n```\n:::\n\n\n\n\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}