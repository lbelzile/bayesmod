{
  "hash": "12e61ee6ff6eae1e97691f3c690a7c17",
  "result": {
    "markdown": "---\ntitle: \"Bayesian modelling\"\nauthor: \"Léo Belzile, HEC Montréal\"\nsubtitle: \"Bayesics\"\ndate: today\ndate-format: YYYY\neval: true\ncache: true\necho: true\nstandalone: true\nbibliography: MATH80601A.bib\nformat:\n  revealjs:\n    slide-number: true\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#ff585d\"\n    logo: \"fig/logo_hec_montreal_bleu_web.png\"\n---\n\n\n\n\n\n## Probability vs frequency\n\nIn frequentist statistic, \"probability\" is synonym for \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n> long-term frequency under repeated sampling\n\n:::\n\n\n::: {.column width=\"40%\"}\n\n\n![](fig/dice.png)\n:::\n\n::::\n\n## What is probability?\n\nProbability reflects incomplete information.\n\nQuoting @deFinetti:1974\n\n> Probabilistic reasoning --- always to be understood as subjective --- merely stems from our being uncertain about something. \n\n## Why opt for the Bayesian paradigm?\n\n- Satisfies the likelihood principle\n- Generative approach naturally extends to complex settings (hierarchical models)\n- Uncertainty quantification and natural framework for prediction\n- Capability to incorporate subject-matter expertise\n\n\n## Bayesian versus frequentist\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n### Frequentist\n\n- Parameters treated as fixed, data as random\n  - true value of parameter $\\boldsymbol{\\theta}$ is unknown.\n- Target is point estimator\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n### Bayesian \n\n- **Both** parameters and data are random\n   - inference is conditional on observed data\n- Target is a distribution\n\n:::\n\n::::\n\n## Joint and marginal distribution\n\nThe joint density of data $\\boldsymbol{Y}$ and parameters $\\boldsymbol{\\theta}$ is\n\n\\begin{align*}\np(\\boldsymbol{Y}, \\boldsymbol{\\theta}) = p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) =  p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y}) p(\\boldsymbol{Y})\n\\end{align*}\nwhere the marginal $p(\\boldsymbol{Y}) = \\int_{\\boldsymbol{\\Theta}} p(\\boldsymbol{Y}, \\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}$.\n\n## Posterior\n\nUsing Bayes' theorem, the posterior density is \n\n\\begin{align*}\n\\color{#D55E00}{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})} = \\frac{\\color{#0072B2}{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})} \\times  \\color{#56B4E9}{p(\\boldsymbol{\\theta})}}{\\color{#E69F00}{\\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\\mathrm{d} \\boldsymbol{\\theta}}},\n\\end{align*}\n\nmeaning that \n$$\\color{#D55E00}{\\text{posterior}} \\propto \\color{#0072B2}{\\text{likelihood}} \\times \\color{#56B4E9}{\\text{prior}}$$\n\n::: aside\nEvaluating the **marginal likelihood** $\\color{#E69F00}{p(\\boldsymbol{Y})}$, is challenging when $\\boldsymbol{\\theta}$ is high-dimensional.\n\n:::\n\n<!--\n## Statistical inference {.smaller}\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n### Frequentist\n\n- Testing relies on asymptotic theory (NHST)\n- Unintuitive formulation \n   - (frequency-based)\n   - e.g., \"in repeated samples, 95% of the intervals would contain the true value\"\n\n:::\n\n::: {.column width=\"50%\"}\n\n### Bayesian \n\n- Comparison in terms of models\n- Any summary of the posterior distribution can be queried\n   - e.g., credible intervals, posterior mean\n\n:::\n\n::::\n\nBut Bayesian inference is often much more work than numerical optimization!\n\n-->\n## Updating beliefs and sequentiality\n\n\nBy Bayes' rule, we can consider *updating* the posterior by adding terms to the likelihood, noting that for independent $\\boldsymbol{y}_1$ and $\\boldsymbol{y}_2$,\n\\begin{align*}\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\boldsymbol{y}_2) \\propto p(\\boldsymbol{y}_2 \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1)\n\\end{align*}\nThe posterior is be updated in light of new information.\n\n## Binomial distribution\n\nA binomial variable with probability of success $\\theta \\in [0,1]$ has mass function\n\\begin{align*}\nf(y; \\theta) = \\binom{n}{y} \\theta^y (1-\\theta)^{n-y}, \\qquad y = 0, \\ldots, n.\n\\end{align*}\nMoments of the number of successes  out of $n$ trials are $$\\mathsf{E}(Y \\mid \\theta) = n \\theta, \\quad \\mathsf{Va}(Y \\mid \\theta) = n \\theta(1-\\theta).$$\n\n::: aside\n\nThe binomial coefficient $\\binom{n}{y}=n!/\\{(n-y)!y!\\}$, where $n!=\\Gamma(n+1)$.\n:::\n\n## Beta distribution\n\nThe beta distribution with shapes $\\alpha>0$ and $\\beta>0$, denoted $\\mathsf{Be}(\\alpha,\\beta)$, has density\n$$f(y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}y^{\\alpha - 1}(1-y)^{\\beta - 1}, \\qquad y \\in [0,1]$$\n\n\n- expectation:  $\\alpha/(\\alpha+\\beta)$;\n- mode $(\\alpha-1)/(\\alpha+\\beta-2)$ if $\\alpha, \\beta>1$, else, $0$, $1$ or none;\n- variance: $\\alpha\\beta/\\{(\\alpha+\\beta)^2(\\alpha+\\beta+1)\\}$.\n\n\n\n::: {.notes}\n\nIt is a continuous distribution over the unit interval. \n\nThe uniform is a special case when both shapes are unity.\n\n:::\n\n## Beta-binomial example\n\n\n\n\nWe write $Y \\sim \\mathsf{Bin}(n, \\theta)$ for $\\theta \\in [0,1]$; the likelihood is $$L(\\theta; y) = \\binom{n}{y} \\theta^y(1-\\theta)^{n-y}.$$\n\nConsider a beta prior, $\\theta \\sim \\mathsf{Be}(\\alpha, \\beta)$, with density\n$$\np(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta) }\\theta^{\\alpha-1}(1-\\theta)^{\\beta - 1}.\n$$\n\n## Density versus likelihood {.smaller}\n\nThe binomial distribution is discrete with support $0, \\ldots, n$, whereas the likelihood is continuous over $\\theta \\in [0,1]$.\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/fig-binom-massvslik_aba0213bf09a5e6a222dc1310cc7b0be'}\n::: {.cell-output-display}\n![Binomial density function (left) and scaled likelihood function (right).](bayesmod-slides1_files/figure-revealjs/fig-binom-massvslik-1.png){#fig-binom-massvslik width=70%}\n:::\n:::\n\n\n::: aside\nIf the density or mass function integrates to 1 over the range of $Y$, the integral of the likelihood over $\\theta$ does not.\n\n:::\n\n\n## Proportionality\n\nAny term not a function of $\\theta$ can be dropped, since it will absorbed by the normalizing constant. The posterior density is proportional to\n\n\\begin{align*}\nL(\\theta; y)p(\\theta) & \\stackrel{\\theta}{\\propto} \\theta^{y}(1-\\theta)^{n-y} \\times \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}\n\\\\& =\\theta^{y + \\alpha - 1}(1-\\theta)^{n-y + \\beta - 1}\n\\end{align*}\nthe kernel of a beta density with shape parameters $y + \\alpha$ and $n-y + \\beta$.\n\n::: aside\n\nThe symbol $\\propto$, for proportionality, means dropping all terms not an argument of the left hand side.\n\n:::\n\n## Experiments and likelihoods {.smaller}\n\nConsider the following sampling mechanism, which lead to $k$ successes out of $n$ independent trials, with the same probability of success $\\theta$.\n\n1. Bernoulli: sample fixed number of observations with $L(\\theta; y) =\\theta^k(1-\\theta)^{n-k}$\n2. binomial: same, but record only total number of successes so $L(\\theta; y) =\\binom{n}{k}\\theta^k(1-\\theta)^{n-k}$\n3. negative binomial: sample data until you obtain a predetermined number of successes, whence $L(\\theta; y) =\\binom{n-1}{k-1}\\theta^k(1-\\theta)^{n-k}$\n\n## Likelihood principle\n\nTwo likelihoods that are proportional, up to a constant not depending on unknown parameters, yield the same evidence. \n\n\nIn all cases, $L(\\theta; y) \\stackrel{\\theta}{\\propto} \\theta^k(1-\\theta)^{n-k}$, so these yield the same inference for Bayesian.\n\n\n::: aside\nFor a more in-depth discussion, see Section 6.3.2 of @Casella.Berger:2002\n:::\n\n\n## Integration\n\nWe could approximate the $\\color{#E69F00}{\\text{marginal likelihood}}$ through either\n\n- numerical integration (cubature)\n- Monte Carlo simulations\n\nIn more complicated models, we will try to sample observations by bypassing completely this calculation.\n\n::: aside\n\nThe likelihood terms can be small (always less than one and decreasing for discrete data), so watch out for numerical overflow when evaluating normalizing constants.\n\n:::\n\n## Numerical example of (Monte Carlo) integration\n\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/betabinom-calculate-marg-lik_b9f9cf58d9c0e6fa52c16a0c2ab60868'}\n\n```{.r .cell-code}\ny <- 6L # number of successes \nn <- 14L # number of trials\nalpha <- beta <- 1.5 # prior parameters\nunnormalized_posterior <- function(theta){\n  theta^(y+alpha-1) * (1-theta)^(n-y + beta - 1)\n}\nintegrate(f = unnormalized_posterior,\n          lower = 0,\n          upper = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.066906e-05 with absolute error < 1e-12\n```\n:::\n\n```{.r .cell-code}\n# Compare with known constant\nbeta(y + alpha, n - y + beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.066906e-05\n```\n:::\n\n```{.r .cell-code}\n# Monte Carlo integration\nmean(unnormalized_posterior(runif(1e5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.064055e-05\n```\n:::\n:::\n\n\n\n<!-- ## Marginal posterior -->\n\n<!-- In multi-parameter models, additional integration is needed to get the marginal posterior -->\n\n<!-- $$p(\\theta_j \\mid \\boldsymbol{y}) = \\int p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}_{-j}.$$ -->\n\n\n<!-- ::: aside -->\n<!-- Marginalization is trivial when we have a joint sample: simply keep the column corresponding to $\\theta_j$. -->\n<!-- ::: -->\n\n\n## Prior, likelihood and posterior\n\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/fig-betabinom-likpost_721759bfb045c25d9283772939230566'}\n::: {.cell-output-display}\n![Scaled Binomial likelihood for six successes out of 14 trials, $\\mathsf{Beta}(3/2, 3/2)$ prior and corresponding posterior distribution from a beta-binomial model.](bayesmod-slides1_files/figure-revealjs/fig-betabinom-likpost-1.png){#fig-betabinom-likpost width=960}\n:::\n:::\n\n\n\n## Proper prior\n\nWe could define the posterior simply as the normalized product of the likelihood and some prior function.\n\nThe prior function need not even be proportional to a density function (i.e., integrable as a function of $\\boldsymbol{\\theta}$).\n\nFor example, \n\n- $p(\\theta) \\propto \\theta^{-1}(1-\\theta)^{-1}$ is improper because it is not integrable.\n- $p(\\theta) \\propto 1$ is a proper prior over $[0,1]$ (uniform).\n\n## Validity of the posterior\n\n- The marginal likelihood does not depend on $\\boldsymbol{\\theta}$\n   - (a normalizing constant)\n- For the posterior density to be *proper*,\n   - the marginal likelihood must be a finite!\n   - in continuous models, the posterior is proper whenever the prior function is proper.\n\n\n## Different priors give different posteriors\n\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/fig-betabinom_060f52d3538c6d678be1ffeecd914646'}\n::: {.cell-output-display}\n![Scaled binomial likelihood for six successes out of 14 trials, with $\\mathsf{Beta}(3/2, 3/2)$ prior (left), $\\mathsf{Beta}(1/4, 1/4)$ (middle) and truncated uniform on $[0,1/2]$ (right), with the corresponding posterior distributions.](bayesmod-slides1_files/figure-revealjs/fig-betabinom-1.png){#fig-betabinom width=960}\n:::\n:::\n\n\n## Role of the prior\n\n\n\nThe posterior is beta, with expected value\n \\begin{align*}\n \\mathsf{E}(\\theta \\mid y) &= w\\frac{y}{n} + (1-w) \\frac{\\alpha}{\\alpha + \\beta}, \\\\ w&=\\frac{n}{n+\\alpha+\\beta}\n \\end{align*} \na weighted average of \n\n- the maximum likelihood estimator and\n- the prior mean. \n\n\n\n\n\n::: {.notes}\n\nWe can think of the parameter $\\alpha$ (respectively $\\beta$) as representing the fixed prior number of success (resp. failures).\n\n:::\n\n## Posterior concentration\n\nExcept for stubborn priors, the likelihood contribution dominates in large samples. The impact of the prior is then often negligible.\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/fig-sequential_0bbf3ad21ec623833f1b723f2777f254'}\n::: {.cell-output-display}\n![Beta posterior and binomial likelihood with a uniform prior for increasing number of observations (from left to right).](bayesmod-slides1_files/figure-revealjs/fig-sequential-1.png){#fig-sequential width=1152}\n:::\n:::\n\n\n\n## Summarizing posterior distributions\n\n<!-- In frequentist statistics, we focus on a point estimator $\\widehat{\\boldsymbol{\\theta}}$, such as the maximum likelihood estimator, and attempt to derive it's distribution, often relying on approximate large-sample distributions. In contrast, t -->\n\nThe output of the Bayesian learning will be either of:\n\n1. a fully characterized distribution (in toy examples).\n2. a numerical approximation to the posterior distribution.\n3. an exact or approximate sample drawn from the posterior distribution.\n\n\n::: {.notes}\n\nThe first case, which we have already encountered, allows us to query moments (mean, median, mode) directly provided there are analytical expressions for the latter, or else we could simulate from the model.\n\n:::\n\n\n## Bayesian inference in practice {.smaller}\n\n\nMost of the field revolves around the creation of algorithms that either \n\n- circumvent the calculation of the normalizing constant \n   - (Monte Carlo and Markov chain Monte Carlo methods)\n- provide accurate numerical approximation, including for marginalizing out all but one parameter.   \n  - (integrated nested Laplace approximations, variational inference, etc.)\n\n## Predictive distributions\n\nDefine the $\\color{#D55E00}{\\text{posterior predictive}}$,\n\\begin{align*}\np(y_{\\text{new}}\\mid \\boldsymbol{y}) = \\int_{\\boldsymbol{\\Theta}} p(y_{\\text{new}} \\mid \\boldsymbol{\\theta}) \\color{#D55E00}{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})} \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\nand the $\\color{#56B4E9}{\\text{prior predictive}}$ \n\\begin{align*}\np(y_{\\text{new}}) = \\int_{\\boldsymbol{\\Theta}} p(y_{\\text{new}} \\mid \\boldsymbol{\\theta}) \\color{#56B4E9}{p(\\boldsymbol{\\theta})} \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\nis useful for determining whether the prior is sensical.\n\n## Analytical derivation of predictive distribution\n\nGiven the $\\mathsf{Be}(a, b)$ prior or posterior, the predictive for $n_{\\text{new}}$ trials is beta-binomial with density\n\\begin{align*}\np(y_{\\text{new}}\\mid y) &= \\int_0^1 \\binom{n_{\\text{new}}}{y_{\\text{new}}} \\frac{\\theta^{a + y_{\\text{new}}-1}(1-\\theta)^{b + k - y_{\\text{new}}-1}}{\n\\mathrm{Be}(a, b)}\\mathrm{d} \\theta\n\\\\&= \\binom{n_{\\text{new}}}{y_{\\text{new}}} \\frac{\\mathrm{Be}(a + y_{\\text{new}}, b + n_{\\text{new}} - y_{\\text{new}})}{\\mathrm{Be}(a, b)}\n\\end{align*}\n\nReplace $a=y + \\alpha$ and $b=n-y + \\beta$ to get the posterior predictive distribution.\n\n\n## Posterior predictive distribution\n\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/fig-betabinompostpred_bc186d2dd0963b0d55de7cd601845a0f'}\n::: {.cell-output-display}\n![Beta-binomial posterior predictive distribution with corresponding binomial mass function evaluated at the maximum likelihood estimator.](bayesmod-slides1_files/figure-revealjs/fig-betabinompostpred-1.png){#fig-betabinompostpred width=960}\n:::\n:::\n\n\n\n## Posterior predictive distribution via simulation\n\n\nThe posterior predictive carries over the parameter uncertainty so will typically be wider and overdispersed relative to the corresponding distribution.\n\nGiven a draw $\\theta^*$ from the posterior, simulate a new observation from the distribution $f(y_{\\text{new}}; \\theta^*)$.\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/post-samp-betabinom_06a307d06cb94271a470f1f7765ffd9e'}\n\n```{.r .cell-code}\nnpost <- 1e4L\n# Sample draws from the posterior distribution\npost_samp <- rbeta(n = npost, y + alpha, n - y + beta)\n# For each draw, sample new observation\npost_pred <- rbinom(n = npost, size = n, prob = post_samp)\n```\n:::\n\n\n\n\n\n::: aside\n\n\nThe beta-binomial is used to model overdispersion in binary regression models.\n\n:::\n\n## Summarizing posterior distributions\n\nThe output of a Bayesian procedure is a **distribution** for the parameters given the data.\n\nWe may wish to return different numerical summaries (expected value, variance, mode, quantiles, ...)\n\nThe question: which point estimator to return?\n\n## Decision theory and loss functions\n\nA loss function $c(\\boldsymbol{\\theta}, \\boldsymbol{\\upsilon}): \\boldsymbol{\\Theta} \\mapsto \\mathbb{R}^k$ assigns a weight to each value $\\boldsymbol{\\theta}$, corresponding to the regret or loss. \n\n\nThe point estimator $\\widehat{\\boldsymbol{\\upsilon}}$ is the minimizer of the expected loss\n\\begin{align*}\n\\widehat{\\boldsymbol{\\upsilon}} &= \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\upsilon}}\\mathsf{E}_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y}}\\{c(\\boldsymbol{\\theta}, \\boldsymbol{v})\\} \\\\&=\\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\upsilon}} \\int_{\\boldsymbol{\\Theta}} c(\\boldsymbol{\\theta}, \\boldsymbol{\\upsilon})p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\n\n\n## Point estimators and loss functions\n\nIn a univariate setting, the most widely used point estimators are\n\n- mean: quadratic loss $c(\\theta, \\upsilon) = (\\theta-\\upsilon)^2$\n- median: absolute loss $c(\\theta, \\upsilon)=|\\theta - \\upsilon|$\n- mode: 0-1 loss $c(\\theta, \\upsilon) = 1-\\mathrm{I}(\\upsilon = \\theta)$\n\nThe posterior mode $\\boldsymbol{\\theta}_{\\mathrm{map}} = \\mathrm{argmax}_{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$ is the **maximum a posteriori** or MAP estimator.\n\n## Measures of central tendency\n\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/fig-central-moments_71e5e114d424431cf9b9c343f6b1efe9'}\n::: {.cell-output-display}\n![Point estimators from a right-skewed distribution (left) and from a multimodal distribution (right).](bayesmod-slides1_files/figure-revealjs/fig-central-moments-1.png){#fig-central-moments width=960}\n:::\n:::\n\n\n## Example of loss functions\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/fig-losses_3842261c99591970d9b519a8660cc734'}\n::: {.cell-output-display}\n![Posterior density with mean, mode and median point estimators (left) and corresponding loss functions, scaled to have minimum value of zero (right).](bayesmod-slides1_files/figure-revealjs/fig-losses-1.png){#fig-losses width=768}\n:::\n:::\n\n\n\n## Credible regions\n\n\nThe freshman dream comes true! A $1-\\alpha$ credible region give a set of parameter values which contains the \"true value\" of the parameter $\\boldsymbol{\\theta}$ with probability $1-\\alpha$.\n\n\nCaveat: @McElreath:2020 suggests the term 'compatibility', as it\n\n> returns the range of parameter values compatible with the model and data.\n\n## Which credible intervals?\n\nMultiple $1-\\alpha$ intervals, most common are \n\n- equitailed: region $\\alpha/2$ and $1-\\alpha/2$ quantiles and \n- **highest posterior density interval** (HPDI), which gives the smallest interval $(1-\\alpha)$ probability\n\n::: aside\n\nIf we accept to have more than a single interval, the highest posterior density region can be a set of disjoint intervals. The HDPI is more sensitive to the number of draws and more computationally intensive (see **R** package `HDinterval`)\n\n:::\n\n## Illustration of credible regions\n\n\n\n::: {.cell hash='bayesmod-slides1_cache/revealjs/fig-credible-intervals_976f49a9ee4d0ccc003f120feddc3088'}\n::: {.cell-output-display}\n![Density plots with 89% (top) and 50% (bottom) equitailed or central credible (left) and highest posterior density (right) regions for two data sets, highlighted in grey.](bayesmod-slides1_files/figure-revealjs/fig-credible-intervals-1.png){#fig-credible-intervals width=960}\n:::\n:::\n\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}