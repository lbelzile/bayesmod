{
  "hash": "5dbb62b2164b1e5ee9fd1c6ff5f859a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Solution 9\"\ndraft: false\n---\n\n\n\n\n\n\n## Exercise 9.1\n\nWe consider the accuracy of the Gaussian approximation to the posterior of a Bernoulli likelihood $Y_i \\sim \\mathsf{binom}(1, \\theta)$ with $y =\\sum_{i=1}^n y_i = \\lfloor 0.1n\\rfloor$ successes out of $n$ trials, i.e., if 10% of the realizations are successes. To do so,\n\n\n\na. Obtain a closed-form expression maximum a posteriori and the hessian of the log posterior for a conjugate $\\mathsf{beta}(a,b)$ prior.\nb. Repeat this with $\\vartheta = \\log(\\theta) - \\log(1-\\theta)$, using a change of variable. \nc. Plug in the approximations with a $\\theta \\sim \\mathsf{beta}(1/2, 1/2)$ prior for $n\\in\\{25, 50, 100\\}$ and plot the Gaussian approximation along with the true posterior. Is the approximation for $\\vartheta$ better for small $n$? Discuss.\nd. Compute the marginal likelihood and compare the approximation with the true value.\n\n:::{.solution}\n\na. Consider the beta kernel $k(\\theta) = \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}$ for $\\alpha, \\beta>0.$ \nThe unnormalized log posterior to optimize and the negative of it's Hessian matrix are\n\\begin{align*}\n\\log k(\\theta) &\\propto (\\alpha-1)\\log \\theta + (\\beta-1) \\log (1-\\theta),\\\\\n\\frac{\\partial \\log k(\\theta)}{\\partial \\theta} &\\propto \\frac{\\alpha-1}{\\theta} - \\frac{\\beta-1}{1-\\theta},\\\\\n-\\frac{\\partial^2 \\log k(\\theta)}{\\partial \\theta^2} &= \\frac{\\alpha-1}{\\theta^2} + \\frac{\\beta-1}{(1-\\theta)^2}.\n\\end{align*}\nBy equating the gradient to zero, we find that the posterior mode is $\\widehat{\\theta} = (\\alpha-1)/(\\alpha+\\beta-2)$ if $\\alpha>1, \\beta>1.$ \nb. If we write now instead $\\theta = \\mathrm{expit}(\\vartheta) \\equiv \\exp(\\vartheta)/\\{1+\\exp(\\vartheta)\\}$ for $\\vartheta \\in \\mathbb{R},$ gradient and Hessian matrix become\n\\begin{align*}\n\\frac{\\partial k(\\vartheta)}{\\partial \\vartheta} &= \n\\frac{\\partial}{\\partial \\vartheta} (\\alpha-1)\\vartheta - (\\alpha + \\beta-2)\\log\\{1+\\exp(\\vartheta)\\}\n\\\\&=(\\alpha-1) - \\mathrm{expit}(\\vartheta)(\\alpha + \\beta-2)\n\\end{align*}\nso that, thanks to invariance, the mode is simply the corresponding inverse transformation $\\widehat{\\vartheta}_{\\mathrm{MAP}}=\\mathrm{logit}(\\widehat{\\theta}) = \\mathrm{logit}\\{(\\alpha-1)/(\\alpha +\\beta-2)\\},$ and \n\\begin{align*}\n\\jmath_{\\vartheta}(\\vartheta) = -\\frac{\\partial^2 k(\\vartheta)}{\\partial \\vartheta^2} &= (\\alpha +\\beta-2)\\frac{\\mathrm{expit}(\\vartheta)}{1+\\exp(\\vartheta)}.\n\\end{align*}\nWe can also get the Hessian by multiplying by the square of the Jacobian.\nThe Jacobian of the change of variables gives \n\\begin{align*}\nf_{\\theta}(\\theta) = \\phi\\{\\log(\\theta)-\\log(1-\\theta); \\widehat{\\vartheta}_{\\mathrm{MAP}}, -1/\\jmath_{\\vartheta}(\\widehat{\\vartheta}_{\\mathrm{MAP}})\\} \\theta^{-1}(1-\\theta)^{-1}\n\\end{align*}\nc. See @fig-approx: the (truncated) Gaussian approximation is not particularly good, but the one for $\\vartheta$ has too much mass close to zero and markedly worst for small $n$. The arcsin variance stabilization transformation would probably be a better choice here.\nd. The marginal likelihood is $\\mathrm{Beta}(\\alpha, \\beta)/\\mathrm{Beta}(0.5, 0.5).$ We can simply plug-in the approximation in the formula to get the results of @tbl-laplace-approx.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngauss_approx_beta <- function(n){\n  y <- floor(0.1*n)\n  alpha <- 0.5 + y\n  beta <- n - y + 0.5\n  # Gaussian approximation for theta\n  map <- (alpha - 1)/(alpha + beta - 2)\n  neghessian <- function(x, alpha, beta){\n    (alpha-1)/x^2 + (beta-1)/(1-x)^2\n  }\n  jm <- neghessian(x = map, alpha = alpha, beta = beta)\n  # Gaussian approximation for logit(theta) = vartheta\n  map_v <- qlogis(map)\n  jac <- plogis(map_v)/(1+exp(map_v))\n  neghessian_v <- function(x, alpha, beta){\n    (alpha + beta - 2) * plogis(x) / (1 + exp(x))\n  }\n  jm_v <- neghessian_v(map_v, alpha = alpha, beta = beta)\n  # Check the hessian is correct\n  stopifnot(isTRUE(all.equal(jac^2*jm, jm_v)))\n  dens_logit_gauss <- function(x){\n    exp(-log(x) - log(1-x) +\n        dnorm(x = qlogis(x), mean = map_v, sd = sqrt(1/jm_v), log = TRUE))\n  }\n  # Check that Jacobian is okay\n  # integrate(dens_logit_gauss, 0, 1)\n  xs <- seq(0, 1, length.out = 1001)\n  data.frame(x = xs, \n             posterior = dbeta(x = xs, shape1 = alpha, shape2 = beta), \n             gaussian = dnorm(x = xs, mean = map, sd = sqrt(1/jm)) / diff(pnorm(c(0,1), mean = map, sd = sqrt(1/jm))),\n             gaussian_logit = dens_logit_gauss(xs)) |>\n    tidyr::pivot_longer(cols = -1, \n                        names_to = \"model\", \n                        values_to = \"density\") |>\n    dplyr::mutate(model = factor(model))\n}\n# Marginal approximation to Bernoulli sample + beta(0.5, 0.5) prior\nlaplace_marginal <- function(n){\n  y <- floor(0.1*n)\n  alpha <- 0.5 + y\n  beta <- n - y + 0.5\n  # Gaussian approximation for theta\n  map <- (alpha - 1)/(alpha + beta - 2)\n  neghessian <- function(x, alpha, beta){\n    (alpha-1)/x^2 + (beta-1)/(1-x)^2\n  }\n lapl_marglik <- dbinom(x = y, size = n, prob = map, log = TRUE) - lchoose(n,y) + \n   dbeta(map, 0.5, 0.5, log = TRUE) + \n   0.5*log(2*pi) - 0.5*log(neghessian(map, alpha, beta))\n true_marglik <- lbeta(alpha, beta)-lbeta(0.5,0.5)\n c(laplace = lapl_marglik, true = true_marglik)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(patchwork)\ntheme_set(theme_classic())\ng1 <- ggplot(mapping = aes(x = x, y = density, \n                           linetype = model, \n                           color = model)) +\n  geom_line(data = gauss_approx_beta(10)) +\n  scale_y_continuous(limits = c(0, NA), expand = expansion()) +\n  MetBrewer::scale_color_met_d(\"Hiroshige\") +\n  labs(caption = \"n=25\")\ng2 <- ggplot(mapping = aes(x = x, y = density, \n                           linetype = model, \n                           color = model)) +\n  geom_line(data = gauss_approx_beta(25)) +\n  scale_y_continuous(limits = c(0, NA), expand = expansion()) +\n  scale_x_continuous(limits = c(0,0.3), expand = expansion()) +\n  MetBrewer::scale_color_met_d(\"Hiroshige\") +\n  labs(caption = \"n=50\")\ng3 <- ggplot(mapping = aes(x = x, y = density, \n                           linetype = model, \n                           color = model)) +\n  geom_line(data = gauss_approx_beta(50)) +\n  scale_y_continuous(limits = c(0, NA), expand = expansion()) +\n  scale_x_continuous(limits = c(0.05,0.2), expand = expansion()) +\n  MetBrewer::scale_color_met_d(\"Hiroshige\") +\n  labs(caption = \"n=100\")\n\ng1 + g2 + g3 + plot_layout(guides = 'collect') & theme(legend.position = \"top\") \n```\n\n::: {.cell-output-display}\n![Density of Gaussian approximations for $\\theta$ and $\\vartheta$ (log odds scale) versus true posterior as a function of the sample size for $n=25$ (left), $n=50$ (middle) and $n=100$ (right).](09-solution_files/figure-html/fig-approx-1.png){#fig-approx width=768}\n:::\n:::\n\n::: {#tbl-laplace-approx .cell tbl-cap='Log of marginal likelihood based on the Laplace approximation and the exact value'}\n::: {.cell-output-display}\n\n\n|        |     10|     25|     100|\n|:-------|------:|------:|-------:|\n|laplace | -4.729| -8.844| -35.041|\n|true    | -4.681| -8.830| -35.042|\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n## Exercise 9.2\n\nConsider a simple random sample from a Bernoulli of size $n$ with $y$ successes and a $\\mathsf{beta}(a, b)$ conjugate prior. Compute the Laplace approximation to the posterior mean for samples of size $n=10, 20, 50, 100$ and $y/n \\in \\{0.1, 0.25, 0.5,1\\}$ and $a=b=1.$\n\n\n:::{.solution}\n\n<!--Example 8.3 of Bove and Held (2020) -->\nWe can apply Laplace's approximation and write\n\\begin{align*}\n \\mathsf{E}_{{\\Theta} \\mid {Y}}(\\theta) &=  \\frac{\\int g({\\theta}) p({y} \\mid {\\theta}) p( {\\theta}) \\mathrm{d} {\\theta}}{\\int p({y} \\mid {\\theta})p({\\theta}) \\mathrm{d} {\\theta}}\n\\end{align*}\n\nThe Hessian evaluated at the mode gives $\\mathbf{H}=(\\alpha+\\beta-2)^3/\\{(\\alpha-1)(\\beta-1)\\}.$ To simplify notation, take $\\alpha = y+a$ and $\\beta = n-y+b$: the Laplace approximation is therefore\n\\begin{align*}\n \\widehat{\\mathsf{E}}_{{\\Theta} \\mid {Y}}(\\theta)&= \\left( \\frac{(\\alpha + \\beta-2)^{3}}{(\\alpha-1)(\\beta-1)}\\frac{\\alpha(\\beta-1)}{(\\alpha + \\beta-1)^{3}}\\right)^{1/2} \\widehat{\\theta}_g \\left( \\frac{\\widehat{\\theta}_g}{\\widehat{\\theta}_{\\mathrm{MAP}}}\\right)^{\\alpha-1} \\left( \\frac{1-\\widehat{\\theta}_g}{1-\\widehat{\\theta}_{\\mathrm{MAP}}}\\right)^{\\beta-1} \n \\\\& =\n\\frac{\\alpha^{\\alpha+1/2}}{(\\alpha-1)^{\\alpha-1/2}}\\frac{(\\alpha + \\beta-2)^{\\alpha + \\beta-1/2}}{(\\alpha + \\beta-1)^{\\alpha + \\beta+1/2}}\n \\end{align*}\nand taking $a=b=1$, we retrieve\n\\begin{align*}\n\\widehat{\\mathsf{E}}_{{\\Theta} \\mid {Y}}(\\theta)  = \\frac{(y+1)^{y+3/2}n^{n+3/2}}{(y+1/2)^{y+1/2}(n+1)^{n+5/2}}.\n\\end{align*}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior mean of beta-binomial\nlap_post_mean <- function(alpha, beta){\n  exp((alpha+0.5)*log(alpha) - (alpha-0.5)*log(alpha-1) + \n        (alpha + beta - 0.5)*log(alpha + beta -2) - \n        (alpha + beta + 0.5)*log(alpha +  beta - 1))\n}\nns <- c(10L, 20L, 50L, 100L)\nyfrac <- c(0.1, 0.25, 0.5,1)\napprox <- array(dim = c(4,4,2), dimnames = list(n = ns, \"y/n\" = yfrac, approx = c(\"laplace\",\"exact\")))\nfor(i in seq_along(ns)){\n  for(j in seq_along(yfrac)){\n    alpha <- ns[i]*yfrac[j] + 1\n    beta <- ns[i]*(1-yfrac[j]) + 1\n approx[i,j,] <-  c(lap_post_mean(alpha, beta), alpha/(alpha+beta)) \n  }\n}\napprox\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n, , approx = laplace\n\n     y/n\nn           0.1      0.25       0.5         1\n  10  0.1718567 0.2917720 0.4968663 0.9090909\n  20  0.1378987 0.2728091 0.4991449 0.9523810\n  50  0.1156562 0.2596352 0.4998555 0.9803922\n  100 0.1079133 0.2549075 0.4999632 0.9900990\n\n, , approx = exact\n\n     y/n\nn           0.1      0.25 0.5         1\n  10  0.1666667 0.2916667 0.5 0.9166667\n  20  0.1363636 0.2727273 0.5 0.9545455\n  50  0.1153846 0.2596154 0.5 0.9807692\n  100 0.1078431 0.2549020 0.5 0.9901961\n```\n\n\n:::\n\n```{.r .cell-code}\n# Percentage relative error\nround(100*(approx[,,1]-approx[,,2])/approx[,,2], 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     y/n\nn       0.1  0.25    0.5      1\n  10  3.114 0.036 -0.627 -0.826\n  20  1.126 0.030 -0.171 -0.227\n  50  0.235 0.008 -0.029 -0.038\n  100 0.065 0.002 -0.007 -0.010\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n",
    "supporting": [
      "09-solution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}