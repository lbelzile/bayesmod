{
  "hash": "75a3e297e6f621a1ee69454a0422369e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Solution 6\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Exercise 6.1\n\nConsider the eight school data from Educational Testing Service (ETS), featured prominently in @Gelman:2013. The data shows results of randomized experiments in eight different schools to estimate the effect of coaching on SAT-V scores.\nThe data consist of estimated treatment mean difference and their standard errors. The data size are large, so the average treatment effect (ATE) can be considered Gaussian and the standard errors are treated as a known quantity.\n\n\n\n1. Derive the Gibbs sampler for the two models with improper priors. Note that, to sample jointly from $p(\\mu, \\boldsymbol{\\alpha} \\mid \\boldsymbol{y}, \\sigma^2_{\\alpha}, \\boldsymbol{\\sigma}^2_{\\boldsymbol{y}})$, you can draw from the marginal of $\\boldsymbol{\\alpha}$, then the conditional $\\mu \\mid \\boldsymbol{\\alpha}, \\cdots$.\n2. Fit the three algorithms described in @Gelman.vanDyk:2008 and compare their efficiency via effective sample size (ESS). In particular, draw traceplots and calculate effective sample sizes for \n    \n    - $\\mu$\n    - $\\alpha_1$, \n    - the sum $\\mu + \\alpha_1$.\n\nas well as pairs plot of $\\alpha_i, \\log \\sigma_{\\alpha}$. The latter should display a very strong funnel.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06-solution_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\nConsider a one-way ANOVA with a single observation per group, where for $i=1, \\ldots, n,$ we have\n\\begin{align*}\nY_{i} &\\sim \\mathsf{Gauss}(\\mu + \\alpha_i, \\sigma^2_i) \\\\\n\\alpha_i &\\sim \\mathsf{Gauss}(0, \\sigma^2_\\alpha)\n\\end{align*}\nand an improper prior for the mean $p(\\mu) \\propto 1.$\nIf there are $K$ groups, then the group-specific mean is $\\mu+\\alpha_k$, and there is a redundant parameter. In the Bayesian setting, the parameters are weakly identifiable because of the prior on $\\boldsymbol{\\alpha}$, but the geometry is such that $\\alpha_k \\to 0$ as $\\sigma^2_{\\alpha} \\to 0.$ See [Section 5.5](https://users.aalto.fi/~ave/BDA3.pdf) on p. 119 of @Gelman:2013 for more details.\n\nWe fit the model by adding redundant parameter to improve mixing [@Gelman.vanDyk:2008]\n\\begin{align*}\nY_{i} &\\sim \\mathsf{Gauss}(\\mu + \\alpha\\xi_i, \\sigma^2_i) \\\\\n\\xi_i &\\sim \\mathsf{Gauss}(0, \\sigma^2_\\xi)\n\\end{align*}\nso that $\\sigma_\\alpha = |\\alpha|\\sigma_\\xi.$\n\n\nSee this [post](https://xuwd11.github.io/am207/wiki/gelmanschools.html) for display of the pathological behaviour that can be captured by divergence in Hamiltonian Monte Carlo.\n\n\n:::{.solution}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Eight school data\nes <- data.frame(school = LETTERS[1:8], \n                 y = c(28, 8, -3, 7, -1, 1, 18, 12), \n                 se = c(15, 10, 16, 11, 9, 11, 10, 18))\nn <- nrow(es)\nB <- 1e3L\nset.seed(80601)\n# Starting values\nalpha <- rnorm(0, sd = 0.01)\nmu <- mean(es$y)\nsigma_al <- sqrt(var(es$y) - mean(es$se))\npars1 <- matrix(NA, nrow = B, ncol = n + 2)\nvar_mu <- 1/sum(1/es$se^2)\nfor(b in seq_len(B)){\n  var_al <- 1/(1/es$se^2 + 1/sigma_al^2)\n  pars1[b, 1:n] <- alpha <- rnorm(n = n, mean = var_al * (es$y - mu)/es$se^2, sd = sqrt(var_al))\n  pars1[b, n + 1] <- mu <- rnorm(n = 1, mean = var_mu * sum((es$y - alpha)/es$se^2), sd = sqrt(var_mu)) \n  pars1[b, n + 2] <- sigma_al <- sum(alpha^2) / rchisq(1, df = n - 1) \n}\n\n## Sampler 2\n# Same model, with joint updates\nset.seed(80601)\n# Starting values\nalpha <- rnorm(0, sd = 0.01)\nmu <- mean(es$y)\nsigma_al <- sqrt(var(es$y) - mean(es$se))\npars2 <- matrix(NA, nrow = B, ncol = n + 2)\nfor(b in seq_len(B)){\n  var_al <- 1/(1/es$se^2 + 1/sigma_al^2)\n  # Sample from joint of alpha, mu\n  # by taking p(alpha | sigma_al, y) * p(mu | alpha, sigma_al, y)\n  pars2[b, 1:n] <- alpha <- rnorm(n = n, mean = var_al * es$y/es$se^2, sd = sqrt(var_al))\n  pars2[b, n + 1] <- mu <- rnorm(n = 1, mean = var_mu * sum((es$y - alpha)/es$se^2), sd = sqrt(var_mu)) \n  pars2[b, n + 2] <- sigma_al <- sum(alpha^2) / rchisq(1, df = n - 1) \n}\n\n# Parameter expansion - V+PX sampler\na <- 1\npars3 <- matrix(NA, nrow = B, ncol = n + 3)\nfor(b in seq_len(B)){\n  var_al <- 1/(1/es$se^2 + 1/sigma_al^2)\n  # Sample from joint of alpha, mu\n  # by taking p(alpha | sigma_al, y) * p(mu | alpha, sigma_al, y)\n  alpha_st <- rnorm(n = n, mean = var_al * es$y/es$se^2, sd = sqrt(var_al))\n  pars3[b, n + 1] <- mu <- rnorm(n = 1, mean = var_mu * sum((es$y - alpha)/es$se^2), sd = sqrt(var_mu)) \n  pars3[b, n + 2] <- sigma_al_st <- sum(alpha_st^2) / rchisq(1, df = n - 1) \n  pars3[b, n + 3] <- a <- rnorm(n = 1, mean = sum(alpha_st*(es$y-mu)/es$se^2)/sum(alpha_st^2/es$se^2), \n                                sd = sum(alpha_st^2/es$se^2))\n  pars3[b, 1:n] <- alpha_st*a\n  pars3[b, n + 2] <- sigma_al <- abs(a)*sigma_al_st\n}\n```\n:::\n\n\n\n\nWe can write the joint density of $\\mu$ and $\\boldsymbol{\\alpha}$ and integrate out $\\mu$. The precision for $\\alpha_j$ is $Q_{\\alpha_j} = 1/\\sigma^2_j + 1/\\sigma_{\\alpha}^2$, and the unconditional mean of $p(\\alpha_j \\mid \\sigma_{\\alpha})$ is $Q^{-1}_{\\alpha_j}y_j/\\sigma^2_j$. The conditional distribution $p(\\mu, \\mid \\boldsymbol{\\alpha}, \\sigma_{\\alpha})$ is Gaussian with precision $Q_{\\mu} = \\sum_{j=1}^{8} \\sigma^{-2}_j$ and mean $Q_{\\mu}^{-1}\\sum_{j=1}^8 \\sigma^{-2}_j(y_j-\\alpha_j).$ The other steps are detailed in @Gelman.vanDyk:2008.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare trace plots for mu + alpha\nplot(pars1[, 2] + pars1[, n+1])\n```\n\n::: {.cell-output-display}\n![](06-solution_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(pars1[, 2])\n```\n\n::: {.cell-output-display}\n![](06-solution_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Funnel behaviour\nggplot(\n  data = data.frame(\n   beta2 = c(pars1[,2], pars3[,2]), \n   sigma = c(pars1[,n+2], pars3[,n+2]),\n   parametrization = rep(c(\"block\",\"expansion\"), each = B)),\n  mapping = aes(x = beta2, y = log(sigma), color = parametrization)) +\n  geom_point() +\n  scale_color_grey() +\n  labs(x = expression(alpha[2]), y = expression(\"log\"~sigma[alpha])) \n```\n\n::: {.cell-output-display}\n![](06-solution_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n:::\n\n\n\n\n:::\n\n<!--\nRun four Markov chains to estimate the parameters with different starting values and store the posterior parameters. For each posterior draw, sample a corresponding data set from the posterior predictive with the same size as the original.\n\n1. Plot the chain including burn in and warmup period. How many iterations does it take for the chains to stabilize?\n2. Estimate the quartiles and report adjusted Monte Carlo standard errors based on the effective sample size.\n3. Calculate the $\\widehat{R}$ factor for each parameter.\n4. **Posterior predictive check**: for each simulated dataset from the posterior predictive,\n     a. plot their density and compare with that of the original data.\n     b. Compare the posterior std. deviation and the range.\n     c. Compute the leave-one-out cross validated probability integral transform\n5. Compute WAIC\n-->\n\n## Exercise 6.2\n\nLet $Y_{ij1}$ ($Y_{ij2}$) denote the score of the home (respectively visitor) team for a soccer match opposing teams $i$ and $j$. @Maher:1982 suggested modelling the scores as\n\\begin{align*}\nY_{ij1} \\mid \\delta, \\alpha_i, \\beta_j &\\sim \\mathsf{Poisson}\\{\\exp(\\delta + \\alpha_i +\\beta_j)\\},\n\\\\ Y_{ij2} \\mid \\delta, \\alpha_j, \\beta_i &\\sim \\mathsf{Poisson}\\{\\exp(\\alpha_j+\\beta_i)\\},\n\\end{align*}\nwhere \n\n- $\\alpha_i$ represent the offensive strength of the team, \n- $\\beta_j$ the defensive strength of team $j$ and \n- $\\delta$ is the common home advantage. \n\nThe scores in a match and between matches are assumed to be conditionally independent of one another given $\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\delta$. The data set `efl` contains the results of football (soccer) matches for the 20232-2024 season of the English Football Ligue (EFL) and contains the following variables\n\n- `score`: number of goals of `team` during a match\n- `team`: categorical variable giving the name of the team which scored the goals\n- `opponent`: categorical variable giving the name of the adversary\n- `home`: binary variable, 1 if `team` is playing at home, 0 otherwise.\n\n\n1. Specify suitable priors for the regression parameters that shrink $\\alpha_i$ and $\\beta_j$ to zero.\n2. Fit the model: \n   - Using the posterior distribution, give the expected number of goals for a match between Manchester United (at home) against Liverpool.\n   - Report and interpret the estimated posterior mean home advantage $\\widehat{\\delta}$. \n   - Calculate the probability that the home advantage $\\delta$ is positive?\n3. Comment on the adequacy of the fit by using a suitable statistic for the model fit (e.g. the deviance statistic\n4. Maher also suggested more complex models, including one in which the offensive and defensive strength of each team changed depending on whether they were at home or visiting another team, i.e.\n\\begin{align}\nY_{ij1} \\sim \\mathsf{Poisson}\\{\\exp(\\alpha_i +\\beta_j + \\delta)\\}, \nY_{ij2} \\sim \\mathsf{Poisson}\\{\\exp(\\gamma_j+\\omega_i)\\},\n\\end{align}\nDoes the second model fit significantly better than  than the first? Compare the models using WAIC and Bayes factors.\n\n:::{.solution}\n\nWe use the `brms` package to generate Stan code following **R** syntax. Model 1 can be fit by adding random effects for `team` and `opponent`. These are assigned penalized complexity priors for the scale such that their 0.99 quantile is 1, giving $1-\\exp(-\\lambda_0) = 0.99$ and thus $\\lambda_0=-\\log (0.01).$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(efl, package = \"hecbayes\")\n# Model 1\nfit_brms1 <- brms::brm(formula = score ~ home + (1 | team) + (1 | opponent),\n                 data = efl, \n                 prior = c(\n                   set_prior(\"normal(0, 10)\", class = \"b\", coef = \"home\"),\n                   set_prior(\"exponential(4.605)\", class = \"sd\", group = \"team\"),\n                   set_prior(\"exponential(4.605)\", class = \"sd\", group = \"opponent\")\n                 ),\n                 family = poisson, \n                 silent = 0, \n                 save_model = \"Maher1.stan\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000141 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.41 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 2.475 seconds (Warm-up)\nChain 1:                1.589 seconds (Sampling)\nChain 1:                4.064 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9.8e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.98 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.887 seconds (Warm-up)\nChain 2:                1.303 seconds (Sampling)\nChain 2:                3.19 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.89 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.829 seconds (Warm-up)\nChain 3:                1.235 seconds (Sampling)\nChain 3:                3.064 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 7.8e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.78 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.755 seconds (Warm-up)\nChain 4:                1.23 seconds (Sampling)\nChain 4:                2.985 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit_brms1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: poisson \n  Links: mu = log \nFormula: score ~ home + (1 | team) + (1 | opponent) \n   Data: efl (Number of observations: 760) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~opponent (Number of levels: 20) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.24      0.05     0.15     0.36 1.00     1521     2414\n\n~team (Number of levels: 20) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.25      0.05     0.16     0.36 1.00     1627     2333\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.34      0.09     0.15     0.51 1.00     1610     1759\nhome          0.20      0.06     0.09     0.31 1.00     6890     2708\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n\n```{.r .cell-code}\n# Use S3 generic for prediction, with new data frame\npredict(fit_brms1, newdata = data.frame(home = 1, team = \"Man Utd\", opponent = \"Liverpool\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimate Est.Error Q2.5 Q97.5\n[1,]  1.27025  1.142603    0     4\n```\n\n\n:::\n\n```{.r .cell-code}\n# Manual prediction from posterior predictive\n# 1) extract simulations\nsims <- fit_brms1$fit@sim$samples[[1]]\nnsim <- length(sims[[1]])\n# 2) calculate the posterior mean for the match for each team (Poisson with log link)\npost_mean1 <- exp(sims$b_Intercept + sims$b_home + sims$`r_opponent[Liverpool,Intercept]` + sims$`r_team[Man.Utd,Intercept]`)\npost_mean2 <- exp(sims$b_Intercept + sims$`r_team[Liverpool,Intercept]` + sims$`r_opponent[Man.Utd,Intercept]`)\n# 3) generate the number of goals from the posterior predictive\npost_pred1 <- rpois(n = nsim, lambda = post_mean1)\npost_pred2 <- rpois(n = nsim, lambda = post_mean2)\n# Plot the posterior predictive (bar plot)\ng1 <- ggplot(data = data.frame(x = c(post_pred1, post_pred2),\n                         team = rep(c(\"Manchester United\", \"Liverpool\"), each = nsim)),\n       aes(x = x, fill = team)) + \n  geom_bar(position = position_dodge()) + \n  labs(x = \"number of goals\") \ng2 <- ggplot(data = data.frame(delta = exp(sims$b_home)),\n             mapping = aes(x = delta)) +\n  geom_density() + \n  labs(x = expression(\"home advantage\"~exp(delta))) \ng1 + g2\n```\n\n::: {.cell-output-display}\n![](06-solution_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n# Model 2\nfit_brms2 <- brms::brm(formula = score ~ home + (1 | team) + (1 | opponent),\n                 data = efl, \n                 prior = c(\n                   set_prior(\"normal(0, 10)\", class = \"b\", coef = \"home\"),\n                   set_prior(\"exponential(4.605)\", class = \"sd\", group = \"team\"),\n                   set_prior(\"exponential(4.605)\", class = \"sd\", group = \"opponent\")\n                 ),\n                 family = poisson, save_model = \"Maher2.stan\")\nsummary(fit_brms2)\n\nwaic(fit_brms1); waic(fit_brms2)\n```\n\n\nWe can also fit the model using integrated nested Laplace approximations.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(INLA)\n# Default prior for random effects\n# inla.models()$latent$iid$hyper$theta\nprec_prior <- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)))\nfit_inla <- INLA::inla(\n  score ~ home + f(team, model = \"iid\", hyper = prec_prior) +\n    f(opponent, model = \"iid\", hyper = prec_prior),\n  family = \"poisson\",\n    data = efl)\nmarg_delta <- fit_inla$marginals.fixed[[2]]\ninla.pmarginal(q = 0, marginal = inla.smarginal(marg_delta))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002693049\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data = data.frame(inla.smarginal(marg_delta)), mapping = aes(x = x, y = y)) + \n  geom_line() + \n  labs(x = expression(delta), y  = \"\", subtitle = \"Marginal density of home advantage\") + \n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](06-solution_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n:::\n\n\n## Exercise 6.3\n\nThe 2000 US presidential election opposed Georges W. Bush (GOP) and Albert A. Gore (Democrat), as well as marginal third party candidates. The tipping state was Florida, worth 25 electors, which Bush won by a narrow margin of 537 votes.  There have been many claims that the design of so-called [butterfly ballot](https://www.palmbeachpost.com/storyimage/LK/20151105/NEWS/812069858/AR/0/AR-812069858.jpg) used in poor neighborhoods of Palm Beach county led to confusion among voters and that this deprived Gore of some thousands of votes that were instead assigned to a paleoconservative third-party candidate, Patrick Buchanan (Reform). @Smith:2002 \\href{https://projecteuclid.org/euclid.ss/1049993203}{Smith (2002)} analysed the election results in Palm Beach country, in which a unusually high number of ballots (3407) were cast for Buchanan. \n\nWe are interested in building a model to predict the expected number of votes for Buchanan in Palm Beach county, based only on the information from other county votes. The data set `buchanan` contains sociodemographic information and votes per county of different candidates.\n\n1. Plot the percentage of votes obtained by Buchanan, `buch`/(`buch`+`totmb`), against $\\log$`popn` and comment.\n2. Fit a binomial logistic model for the votes of Bush versus Gore with $\\log$ `popn`, `black`, `hisp`, `geq65` `highsc` as covariates. \n    - Interpret the estimated coefficients $\\widehat{\\boldsymbol{\\beta}}$ for `highsc` and `hisp` on the odds scale.\n    - Calculate the total posterior proportion of votes for Buchanan in Florida.\n3. Look at the posterior predictive checks and leave-one-out probability plot.  Is there evidence of overdispersion? Consider how to account for the latter and compare the model with a beta-binomial regression.\n4. Rerun the model **excluding** the results of Palm Beach county and predict the expected number of Buchanan votes in Palm Beach county. Comment hence on the discrepancy between this forecast and the number of votes received in the election.\n",
    "supporting": [
      "06-solution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}