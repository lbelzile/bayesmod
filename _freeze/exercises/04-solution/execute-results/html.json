{
  "hash": "a92d61123740146f48588f5af3379b4e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Solution 4\"\n---\n\n\n\n\n## Exercise 4.1\n\nConsider the Laplace family of distribution, $\\mathsf{Laplace}(\\nu, \\tau)$, with density\n\\begin{align*}\ng(x; \\nu, \\tau) = \\frac{1}{2\\tau} \\exp\\left(- \\frac{|x-\\nu|}{\\tau}\\right), \\qquad \\nu \\in \\mathbb{R}, \\tau > 0\n\\end{align*}\nas a candidate distribution for rejection sampling from $\\mathsf{Gauss}(0,1)$.\n\n1. Provide an inversion sampling algorithm to generate from $\\mathsf{Laplace}(\\nu, \\tau)$.\n2. Can you use the proposal to generate from a standard Gaussian? for Student-$t$ with 1 degree of freedom? Justify your answer.\n3. Consider as proposal a location-scale version of the Student-t with $\\nu=3$\n degrees of freedom. Find the optimal location and scale parameters and the upper bound $C$ for your choice.\n4. Use the accept-reject to simulate 1000 independent observations and compute the empirical acceptance rate.\n\n\n\n::: {.solution}\n\n1. The distribution function is \n\\begin{align*}\nF(x) = \\begin{cases}\n\\frac{1}{2} \\exp\\left(\\frac{x-\\nu}{\\tau}\\right) & x \\leq \\nu\\\\\n1 - \\frac{1}{2} \\exp\\left(-\\frac{x-\\nu}{\\tau}\\right) & x >\\nu\\\\\n\\end{cases}\n\\end{align*}\nand using the quantile transform, set $X=\\nu + \\tau \\log(2U)$ if $U \\leq 0.5$ and $X=\\nu - \\tau\\log(2-2U)$ if $U > 0.5$ for $U \\sim \\mathsf{unif}(0,1)$.\n2. The Gaussian has lighter tail than the Laplace, so this won't work. The Cauchy distribution would be a suitable candidate, albeit too heavy tailed.\n3. The optimal value for the location of the Student-$t$ would be $\\nu$ (here zero for the standard Laplace). We compute the optimal scale via in the code below:\n$$\n\\mathrm{argmax}_{\\sigma \\in \\mathbb{R}_{+}}\\mathrm{argmin}_{x \\in \\mathbb{R}} \\{\\log f(x) - \\log g(x; \\sigma)\\}\n$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Laplace density\ndlaplace <- function(x, loc = 0, scale = 1, log = FALSE){\n stopifnot(scale > 0)\n logdens <-  -log(2*scale) - abs(x-loc)/scale\n if(log){\n return(logdens)\n } else{\n return(exp(logdens))\n }\n}\ndstudent <- function(x, loc = 0, scale = 1, df = 1, log = FALSE){\n  logdens <- -log(scale) + dt(x = (x - loc)/scale, df = df, log = TRUE)\n   if(log){\n    return(logdens)\n   } else{\n   return(exp(logdens))\n   }\n}\n# For each value of the scale sigma,\n# find the minimum value of x (typically at zero)\nopt <- optimize(f = function(sigma){\n  optimize(f = function(x){\n  dlaplace(x, log = TRUE) - \n    dstudent(x, scale = sigma, df = 3, log = TRUE)}, \n  maximum = TRUE, \n  interval = c(-100, 100))$objective\n}, interval = c(0.1,10))\n(C <- exp(opt$objective))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.261368\n```\n\n\n:::\n\n```{.r .cell-code}\n(sigma <- opt$minimum)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9272381\n```\n\n\n:::\n\n```{.r .cell-code}\n# Simulate from accept-reject\nntrials <- 1.1*C*1000\n# Simulate from location-scale student\ncandidate <- sigma*rt(n = ntrials, df = 3)\n# Compute log of acceptance rate\nlogR <- dlaplace(candidate, log = TRUE) - \n  dstudent(candidate, scale = sigma, df = 3, log = TRUE) \nsamp <- candidate[logR >= log(C) + -rexp(ntrials)]\n# Monte Carlo estimator of the acceptance rate\nntrials/length(samp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.274109\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot density\nlibrary(ggplot2)\nggplot(data = data.frame(x = samp),\n       mapping = aes(x = x)) +\n  geom_density() +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](04-solution_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n4. The Monte Carlo acceptance rate is 1.27, compared with the analytical bound found via numerical optimization of 1.26, to two significant digits.\n\n:::\n\n\n## Exercise 4.2\n\nWe revisit [Exercise 2.3](/exercises/02-exercise.html#exercice-2.3), which used a half-Cauchy prior for the exponential waiting time of buses.\n\nThe ratio-of-uniform method, implemented in the [`rust` **R** package](https://paulnorthrop.github.io/rust/index.html), can be used to simulate independent draws from the posterior of the rate $\\lambda$.\nThe following code produces \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnobs <- 10L # number of observations\nybar <- 8   # average waiting time\nB <- 1e4L  # number of draws\n# Un-normalized log posterior: scaled log likelihood + log prior\nupost <- function(x){ \n  dgamma(x = x, shape = nobs + 1L, rate = nobs*ybar, log = TRUE) +\n    log(2) + dt(x = x, df = 1, log = TRUE)}\npost_samp <- rust::ru(logf = upost, \n                      n = B, \n                      d = 1,  # dimension of parameter (scalar)\n                      init = nobs/ybar)$sim_vals # initial value of mode\n```\n:::\n\n\n\nEstimate using the independent Monte Carlo samples:\n\n1. the probability that the average waiting time $1/\\lambda$ is between 3 and 15 minutes\n2. the average waiting time\n3. the standard deviation of the average waiting time.\n\n\nNext, implement a random walk Metropolis--Hastings algorithm to sample draws from the posterior and re-estimate the quantities. Compare the values.\n\n::: {.solution}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Monte Carlo for waiting time\n# Lambda is the reciprocal mean (1/minute)\ntimes_mc <- 1/post_samp\nmean(times_mc > 3 & times_mc < 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.98\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(times_mc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.642588\n```\n\n\n:::\n\n```{.r .cell-code}\n# posterior mean\nmean(times_mc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.978362\n```\n\n\n:::\n\n```{.r .cell-code}\n# Standard error of posterior mean\nsd(times_mc)/sqrt(length(post_samp))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02642588\n```\n\n\n:::\n\n```{.r .cell-code}\n# Metropolis-Hastings algorithm\nB <- 1e4L\ncurr <- 8/10 # prior mean\nchains <- numeric(B) # container\nsd_prop <- 0.1 # proposal standard deviation\nfor(b in seq_len(B)){\n  prop <- rnorm(n = 1, mean = curr, sd = sd_prop)\n  if(upost(prop) - upost(curr) > -rexp(1)){\n    curr <- prop\n  }\n  chains[b] <- curr\n}\n# Discard burn-in\ntimes <- 1/chains[-(1:100)]\n# Estimate quantities using Monte Carlo\nmean(times > 3 & times < 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9773737\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(times)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.010612\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(times)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.676302\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary of MCMC\nmcmc <- coda::mcmc(times)\nsummary(mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIterations = 1:9900\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 9900 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n       8.01061        2.67630        0.02690        0.05787 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n 4.283  6.170  7.538  9.203 14.691 \n```\n\n\n:::\n:::\n\n\nWe can see that the standard error for the mean is roughly twice as big. We would thus need to inflate the sample size by a factor four to get the same precision.\n\n:::\n\n## Solution 4.3\n\n\nConsider the following code which implements a Metropolis--Hastings algorithm to simulate observations from a $\\mathsf{beta}(0.5, 0.5)$ density.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_f <- function(par){\n  dbeta(x = par, shape1 = 0.5, shape2 = 0.5, log = TRUE)\n}\nmetropo <- function(B, sd_prop = 0.2){\n  chain <- rep(0, B)\n  # Draw initial value\n  cur <- runif(1)\n  for(b in seq_len(B)){\n    repeat {\n        # Simulate proposal from Gaussian random walk proposal\n        prop <- cur + rnorm(1, sd = sd_prop)\n        # check admissibility for probability of success\n        if (prop >= 0 & prop <= 1)\n          break\n    }\n    # Compute (log) acceptance ratio\n    logR <- log_f(prop) - log_f(cur) \n    # Accept the move if R > u\n    if(isTRUE(logR > log(runif(1)))){\n     cur <- prop \n    }\n    chain[b] <- cur\n  }\n  return(chain)\n}\n# Run MCMC for 10K iterations\nmc <- metropo(1e4L)\n```\n:::\n\n\n\nTo see if the algorithm works:\n\na. Plot the density of the Markov chain draws along with the beta density curve. \nb. Check that empirical moments match the theoretical ones\n\nIf the algorithm is incorrect, provide a fix and explain the reason for the problem.\n\n:::{.solution}\n\nThe sampler is incorrect because it tacitly draws from Gaussian variables that are restricted to $[0,1]$ via the accept-reject step. This means in particular that the acceptance ratio involves constants for the probability given a Gaussian centered at the current value or proposal truncated on the unit interval\n$$\\frac{q(x^{\\text{cur}\\hphantom{c}} \\mid x^{\\text{prop}})}{q(x^{\\text{prop}} \\mid x^{\\text{cur}\\hphantom{c}})} = \\frac{\\Phi\\{(1-x^{\\text{cur}})/\\sigma^{\\text{prop}}\\} - \\Phi(-x^{\\text{cur}}/\\sigma^{\\text{prop}})}{\\Phi\\{(1-x^{\\text{prop}})/\\sigma^{\\text{prop}}\\} - \\Phi(-x^{\\text{prop}}/\\sigma^{\\text{prop}})}.$$\n\nThis exercise was inspired by [this blog post](https://darrenjw.wordpress.com/2012/06/04/metropolis-hastings-mcmc-when-the-proposal-and-target-have-differing-support/) by Darren Wilkinson.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_f <- function(par){\n  dbeta(x = par, shape1 = 0.5, shape2 = 0.5, log = TRUE)\n}\nmetropo_fixed <- function(B, sd_prop = 0.2){\n  chain <- rep(0, B)\n  # Draw initial value\n  cur <- runif(1)\n  for(b in seq_len(B)){\n    repeat {\n        # Simulate proposal from Gaussian random walk proposal\n        prop <- cur + rnorm(1, sd = sd_prop)\n        # check admissibility for probability of success\n        if (prop >= 0 & prop <= 1)\n          break\n    }\n    # Compute (log) acceptance ratio\n    logR <- log_f(prop) - log_f(cur) + \n      log(pnorm(1, mean = cur, sd = sd_prop) - pnorm(0, mean = cur, sd = sd_prop)) -\n      log(pnorm(1, mean = prop, sd = sd_prop) - pnorm(0, mean = prop, sd = sd_prop))\n    # Accept the move if R > u\n    if(isTRUE(logR > log(runif(1)))){\n     cur <- prop \n    }\n    chain[b] <- cur\n  }\n  return(chain)\n}\n# Run MCMC for 10K iterations\nmc2 <- metropo_fixed(1e4L)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of histogram for 10K draws from the incorrect sampler (left) and the correct sampler (right).](04-solution_files/figure-html/fig-comparison-1.png){#fig-comparison width=90%}\n:::\n:::\n\n\n\n\n:::\n\n<!--\n## Exercise 4.3\n\nRepeat the simulations in Example 3.6, this time with a parametrization in terms of log rates $\\lambda_i$ $(i=1,2)$, with the same priors. Use a Metropolis--Hastings algorithm with a Gaussian random walk proposal, updating parameters one at a time. Run four chains in parallel.\n\n1. Tune the variance to reach an approximate acceptance rate of 0.44.\n2. Produce diagnostic plots (scatterplots of observations, marginal density plots, trace plots and correlograms). See [`bayesplot`](http://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html) or `coda`. Comment on  the convergence and mixing of your Markov chain Monte Carlo.\n3. Report summary statistics of the chains.\n\n:::{ .solution}\n\nThe only thing we need to do is change the parametrization, and add tuning of the variance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(upworthy_question, package = \"hecbayes\")\n# Compute sufficient statistics\ndata <- upworthy_question |>\n  dplyr::group_by(question) |>\n  dplyr::summarize(ntot = sum(impressions),\n                   y = sum(clicks))\n# Code log posterior as sum of log likelihood and log prior\nloglik <- function(par, counts = data$y, offset = data$ntot, ...){\n  lambda <- exp(c(par[1] + log(offset[1]), par[2] + log(offset[2])))\n sum(dpois(x = counts, lambda = lambda, log = TRUE))\n}\nlogprior <- function(par, ...){\n  sum(dnorm(x = par, mean = log(0.01), sd = 1.5, log = TRUE))\n}\nlogpost <- function(par, ...){\n  loglik(par, ...) + logprior(par, ...)\n}\n# Compute maximum a posteriori (MAP)\nmap <- optim(\n  par = rep(-4, 2),\n  fn = logpost,\n  control = list(fnscale = -1),\n  offset = data$ntot,\n  counts = data$y,\n  hessian = TRUE)\n# Use MAP as starting value\ncur <- map$par\n# Compute logpost_cur - we can keep track of this to reduce calculations\nlogpost_cur <- logpost(cur)\n# Proposal covariance\ncov_map <- -2*solve(map$hessian)\nchol <- chol(cov_map)\n\nset.seed(80601)\nniter <- 1e4L\nnchains <- 4L\nnpar <- 2L\nchain <- array(0, dim = c(niter, nchains, npar))\nnaccept <- 0L\nfor(j in 1:nchains){\n  for(i in seq_len(niter)){\n    # Multivariate normal proposal - symmetric random walk\n    prop <- chol %*% rnorm(n = 2)/1.05 + cur\n    logpost_prop <- logpost(prop)\n    # Compute acceptance ratio (no q because the ratio is 1)\n    logR <- logpost_prop - logpost_cur\n    if(logR > -rexp(1)){\n      cur <- prop\n      logpost_cur <- logpost_prop\n      naccept <- naccept + 1L\n    }\n    chain[i,j,] <- cur\n  }\n}\n# Acceptance rate\nnaccept/(nchains * niter)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.44355\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create some summary graphs\nlibrary(bayesplot)\n# Name chains so that the graphs can be labelled\ndimnames(chain)[[3]] <- c(\"lambda[1]\",\"lambda[2]\")\n\n# Trace plots, correlograms, density plots and bivariate scatterplot\nbayesplot::mcmc_trace(chain)\n```\n\n::: {.cell-output-display}\n![](04-solution_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_acf_bar(chain)\n```\n\n::: {.cell-output-display}\n![](04-solution_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_dens(chain)\n```\n\n::: {.cell-output-display}\n![](04-solution_files/figure-html/unnamed-chunk-7-3.png){width=672}\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_scatter(chain)\n```\n\n::: {.cell-output-display}\n![](04-solution_files/figure-html/unnamed-chunk-7-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# Posterior summaries with coda\n# Need to create a list of mcmc objects...\nmcmc_list <- coda::as.mcmc.list(\n    lapply(seq_len(nchains),\n           function(ch) coda::as.mcmc(chain[, ch, ])))\nsummary(mcmc_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 4 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean       SD  Naive SE Time-series SE\nlambda[1] -4.513 0.001730 8.648e-06      2.384e-05\nlambda[2] -4.442 0.001192 5.962e-06      1.670e-05\n\n2. Quantiles for each variable:\n\n            2.5%    25%    50%    75%  97.5%\nlambda[1] -4.516 -4.514 -4.513 -4.511 -4.509\nlambda[2] -4.444 -4.443 -4.442 -4.441 -4.440\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n-->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}