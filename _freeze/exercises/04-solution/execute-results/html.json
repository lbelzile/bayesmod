{
  "hash": "65c85bcf796ec2fb8814d4803c292d6a",
  "result": {
    "markdown": "---\ntitle: \"Solution 4\"\n---\n\n\n\n## Exercise 4.1\n\nThe Pareto distribution with shape $\\alpha>0$ and scale $\\tau>0$ has density\n$$\nf(x; \\alpha, \\tau) = \\alpha x^{-\\alpha-1}\\tau^\\alpha \\mathsf{I}(x > \\tau).\n$$\nIt can be used to model power laws in insurance and finance, or in demography. The `uscitypopn` data set in the `hecbayes` package contains the population size of cities above 200K inhabitants in the United States, from the 2020 census.\n\n1. Using improper priors, write the joint posterior for a simple random sample of size $n$ and derive the conditional distributions $p(\\alpha \\mid \\boldsymbol{y}, \\tau)$ and $p(\\tau \\mid \\alpha, \\boldsymbol{y})$.\n2. The mononomial distribution $\\mathsf{Mono}(a,b)$ has density $p(x) \\propto x^{a-1}\\mathsf{I}(0 \\leq x \\leq b)$. Find the normalizing constant for the distribution and obtain the quantile function to derive a sampler.\n3. Implement Gibbs sampling for this problem for the `uscitypopn` data. Draw enough observations to obtain an effective sample size of at least 1000 observations. Calculate the accuracy of your estimates?\n\n\n\n::: {.solution}\n\nWith improper prior, the joint posterior is the product of the likelihood contributions so\n$$\np(\\alpha, \\tau \\mid \\boldsymbol{y}) \\propto \\alpha^n \\left(\\prod_{i=1}^n y_i\\right)^{-\\alpha-1} \\tau^{-n\\alpha} \\mathsf{I}(\\min_i y_i > \\tau).\n$$\nUsing the hint, write the conditional density for $\\alpha$ given the rest as\n\\begin{align*}\np(\\alpha \\mid \\boldsymbol{y}, \\tau) \\propto \\alpha^n \\left( \\frac{\\prod_{i=1}^n y_i}{\\tau^n}\\right)^{-\\alpha} = \\alpha^{(n+1)-1} \\exp\\left\\{-\\alpha \\left(\\sum_{i=1}^n\\log y_i - n\\log \\tau\\right) \\right\\}\n\\end{align*}\nwhich is $\\mathsf{Gamma}\\big(n+1, \\sum_{i=1}^n \\log y_i - n \\log \\tau \\big)$. For the second, we have\n\\begin{align*}\np(\\tau \\mid \\alpha, \\boldsymbol{y}) \\propto \\tau^{n\\alpha} \\mathsf{I}(\\min_{i} y_i > \\tau),\n\\end{align*}\na mononomial distribution with parameters $a=n\\alpha+1$ and $b = \\min_{i} y_i$.\n\nTo find the normalizing constant of the mononomial distribution, we simply integrate the unnormalized density to obtain the reciprocal constant: if $c = \\int g(x) \\mathrm{d} x$ for $c < \\infty$ and $g(x) \\geq 0$ for all $x$, then $g(x)/c$ integrates to one and is a valid density. Thus, we find \n$$c= \\int_0^b x^{a-1}\\mathrm{d} x = \\left[\\frac{x^{a}}{a}\\right]_{0}^b= \\frac{b^{a}}{a}.$$\nThe distribution function is $G(x) = (x/b)^{a}$ for $x \\in [0,b]$ and the quantile function $G^{-1}(u) = u^{1/a}b$.\n\n\n::: {.cell hash='04-solution_cache/html/unnamed-chunk-1_eb5272bfed2ebf22bdbaaea97ac0245f'}\n\n```{.r .cell-code}\nqmono <- function(u, a, b, log = FALSE){\n  stopifnot(isTRUE(all(a > 0, b > 0, u >= 0, u <= 1)))\n logq <-   log(u)/(a+1) + log(b)\n if(log){ return(logq)} else { return(exp(logq)) }\n}\n\n\n# Load data\ndata(\"uscitypopn\", package = \"hecbayes\")\ny <- uscitypopn$population\nn <- length(y)\n# Summary statistics appearing in the posterior distribution\nsumlogy <- sum(log(y))\nminy <- min(y)\n# MCMC via Gibbs sampling\nB <- 1e4L\nchains <- matrix(0, nrow = B, ncol = 2)\ncolnames(chains) <- c(\"alpha\", \"tau\")\ncurr <- c(2, 2e5)\nfor(b in seq_len(B)){\n  chains[b,1] <- curr[1] <- rgamma(n = 1, shape = n+1, rate = sumlogy - n*log(curr[2]))\n  chains[b,2] <- curr[2] <- qmono(runif(1), a = n*curr[1]+1, b = miny)\n}\nchains <- coda::as.mcmc(chains)\n# Compute effective sample size\ncoda::effectiveSize(chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nalpha   tau \n10000 10000 \n```\n:::\n\n```{.r .cell-code}\nsummary(chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean        SD  Naive SE Time-series SE\nalpha 1.391e+00    0.1309  0.001309       0.001309\ntau   1.991e+05 1240.4467 12.404467      12.404467\n\n2. Quantiles for each variable:\n\n           2.5%      25%       50%       75%     97.5%\nalpha 1.149e+00      1.3 1.387e+00 1.478e+00 1.662e+00\ntau   1.958e+05 198626.3 1.995e+05 2.000e+05 2.004e+05\n```\n:::\n:::\n\nWe can see that the autocorrelation is minimal, so the sampler is quite efficient.\n\n\n:::\n\n## Exercise 4.2\n\nImplement the Bayesian LASSO for the `diabetes` cancer surgery from package `lars`.  Check @Park.Casella:2008 for the details of the Gibbs sampling.\n\n1. Fit the model for a range of values of $\\lambda=0.237$ and produce parameter estimate paths to replicate Figure 2 of the paper.\n2. Check the effective sample size and comment on the mixing. Is it impacted by the tuning parameter?\n3. Implement the method of section 3.1 from @Park.Casella:2008 by adding $\\lambda$ as a parameter.\n4. For three models with different values of $\\lambda$, compute the widely applicable information criterion (WAIC) and use it to assess predictive performance.\n\n\n::: {.solution}\n\nWe first setup a Gibbs sampler for a given value of $\\lambda$, or using the empirical Bayes estimator provided in section 3.1. The effective sampling size for fixed $\\lambda$ is good. If we let the parameter varies, the performance degrades and we obtain an effective size shy of 1000 for 10K iterations for $\\lambda$, and comfortably above 5000 for others. \n\n\n::: {.cell hash='04-solution_cache/html/unnamed-chunk-2_3ce40774ee57677fbf39bd707f4b57ac'}\n\n```{.r .cell-code}\ndata(diabetes, package = \"lars\")\nbayeslasso <- function(lambda = NULL, \n                       B = 1e4L,\n                       x = diabetes$x, \n                       y = diabetes$y){\n  stopifnot(is.matrix(x), is.vector(y))\n  # Scale inputs in case\n  x <- scale(x, center = TRUE, scale = FALSE)\n  y <- y - mean(y)\n  # Check method\n  if(is.null(lambda)){\n    method <- \"empbayes\"\n  } else{\n    method <- \"fixed\" \n  }\n  # Precompute quantities and dimensions\n  xtx <- crossprod(x)\n  p <- ncol(x)\n  n <- nrow(x)\n  # Obtain initial estimates\n  linmod <- lm(y ~ x - 1)\n  betaols <- coef(linmod)\n  beta.curr <- betaols\n  sigmasq.curr <- mean(residuals(linmod)^2)\n  tausqinv.curr <- rep(1, p)\n  # Value reported in the text for the optimal parameter: lambda = 0.237\n  beta.ind <- 1:p\n  sigmasq.ind <- p + 1L\n  tausq.ind <- seq(from = p + 2L, length.out = p, by = 1L)\n  chains <- matrix(0, nrow = B, ncol = p + 1 + p + \n                     ifelse(method == \"fixed\", 0,1))\n  if(method == \"fixed\"){\n    colnames(chains) <- c(paste0(\"beta\", 1:p), \"sigmasq\",\n                          paste0(\"tausq\", 1:p))\n    lambdasq.curr <- lambda[1]^2\n  } else{\n    colnames(chains) <- c(paste0(\"beta\", 1:p), \"sigmasq\", \n                          paste0(\"tausq\", 1:p), \"lambda\")\n    lambdasq.curr <- p*sqrt(sigmasq.curr)/sum(abs(betaols))\n    lambdasq.ind <- ncol(chains)\n  }\n# MCMC loop\nfor(b in seq_len(B)){\n  Ainv <- solve(xtx + diag(tausqinv.curr))\n  beta.curr <- chains[b,beta.ind] <- as.numeric(\n    mvtnorm::rmvnorm(\n      n = 1, \n      mean = as.numeric(Ainv %*% t(x) %*% y), \n      sigma = sigmasq.curr*Ainv))\n  sigmasq.curr <- chains[b, sigmasq.ind] <- 1/rgamma(\n    n = 1, \n    shape = (n-1+p)/2,\n    rate = sum((y-x %*% beta.curr)^2)/2 + \n      sum(beta.curr^2*tausqinv.curr)/2)\n  # Compute marginal posterior mean for lambda, using section 3.1\n  sumexpect <- 0\n  for(j in 1:p){\n    tausqinv.curr[j] <- actuar::rinvgauss(\n      n = 1, \n      mean = sqrt(lambdasq.curr*sigmasq.curr)/abs(beta.curr[j]),\n      dispersion = 1/lambdasq.curr)\n    if(method != \"fixed\"){\n    sumexpect <- sumexpect + mean(1/actuar::rinvgauss(\n      n = 1000, \n      mean = sqrt(lambdasq.curr*sigmasq.curr)/abs(beta.curr[j]),\n      dispersion = 1/lambdasq.curr))\n    }\n  }\n  chains[b, tausq.ind] <- 1/tausqinv.curr\n  if(method != \"fixed\"){\n    lambdasq.curr <- chains[b, lambdasq.ind] <- 2*p/sumexpect\n  }\n}\n  if(method != \"fixed\"){\n  chains[, lambdasq.ind] <- sqrt(chains[, lambdasq.ind])\n}\n# Cast Markov chains to mcmc class object.\nchains.mcmc <- coda::as.mcmc(chains)\n# Effective sample size\ness <- as.integer(round(coda::effectiveSize(chains.mcmc), 0))\n\n# Compute WAIC from log pointwise density\nlppd <- 0\npenalty <- 0\nfor(i in seq_len(n)){\n  lppd_i <- dnorm(\n    x = y[i], \n    mean = as.numeric(chains[,beta.ind] %*% c(x[i,])), \n    sd = chains[,sigmasq.ind], \n    log = TRUE)\n  lppd <- lppd + mean(lppd_i)\n  penalty <- penalty + var(lppd_i)\n}\nwaic <- (-lppd + penalty)/(n*B)\n\n# Parameter estimates and 95% equitailed credible intervals\nquant <- t(apply(chains, 2, \n                quantile, prob = c(0.025, 0.5, 0.975)))\nregpar <- as.data.frame(cbind(quant,\n                colMeans(chains),\n                coda::batchSE(chains.mcmc),\n                ess))\nregpar$pars <- rownames(quant)\nrownames(regpar) <- NULL\ncolnames(regpar) <- c(\"lower\", \"median\", \"upper\", \n                      \"mean\", \"se\", \"ess\", \"par\")\nregpar <- regpar[,c(7,5,4,1:3,6)]\nattr(regpar, \"waic\") <- waic\n return(regpar)\n}\n\n# Call the MCMC sampler\nset.seed(2023)\nlasso_empbayes <- bayeslasso(lambda = NULL)\n# Extract the value of WAIC\nwaic <- attr(lasso_empbayes, \"waic\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Standardized median posterior estimates of the coefficients for the Bayesian LASSOwith 95 percent equitailed credible intervals, with $\\lambda$ estimated using empirical Bayes. Ordinary least square estimates are denoted by crosses.](04-solution_files/figure-html/fig-parkcasellaf2-1.png){#fig-parkcasellaf2 width=672}\n:::\n:::\n\nThe plot corresponds to Figure 2 of @Park.Casella:2008 and the posterior summaries, reported in @tbl-coefs, are also in line with those of the paper.\n\n\n\n::: {#tbl-coefs .cell tbl-cap='Estimates posterior summaries from the Bayesian LASSO, based on 10K draws. Posterior means and adjusted standard errors, posterior median and equitailed 95 percent credible intervals, effective sample.'}\n::: {.cell-output-display}\n|par     |   se|    mean|   lower|  median|   upper|   ess|\n|:-------|----:|-------:|-------:|-------:|-------:|-----:|\n|beta1   | 0.51|   -3.16| -111.94|   -2.55|  105.93| 10000|\n|beta2   | 0.66| -213.98| -331.39| -213.45|  -98.14|  9104|\n|beta3   | 0.70|  523.35|  393.62|  523.61|  657.08|  9097|\n|beta4   | 0.71|  307.85|  177.72|  308.03|  439.28|  8971|\n|beta5   | 2.50| -186.87| -586.53| -170.73|  125.37|  5017|\n|beta6   | 1.93|    7.63| -273.46|   -1.62|  341.75|  5947|\n|beta7   | 1.58| -154.24| -384.29| -153.15|   72.40|  5193|\n|beta8   | 1.57|   96.64| -131.61|   87.19|  353.40|  6412|\n|beta9   | 1.30|  523.87|  331.23|  520.85|  726.63|  6637|\n|beta10  | 0.61|   64.17|  -50.03|   61.84|  188.68|  8837|\n|sigmasq | 1.92| 2952.77| 2585.66| 2945.04| 3370.20|  9603|\n|tausq1  | 0.26|   20.60|    0.24|   11.16|   93.83| 10000|\n|tausq2  | 0.33|   34.77|    3.44|   25.18|  122.06|  8896|\n|tausq3  | 0.41|   58.43|   13.38|   49.04|  156.54|  9353|\n|tausq4  | 0.38|   42.00|    6.10|   32.31|  134.62|  8917|\n|tausq5  | 0.45|   34.26|    1.04|   24.36|  123.81|  5145|\n|tausq6  | 0.38|   26.72|    0.57|   17.08|  104.90|  7369|\n|tausq7  | 0.37|   31.02|    1.03|   21.38|  117.65|  8443|\n|tausq8  | 0.33|   27.39|    0.54|   17.80|  107.05|  8572|\n|tausq9  | 0.46|   58.91|   12.89|   49.63|  159.97|  7744|\n|tausq10 | 0.30|   23.26|    0.40|   13.65|  101.97|  9276|\n|lambda  | 0.00|    0.24|    0.21|    0.24|    0.26|  1040|\n:::\n:::\n\n\nFor the last part, we can simply run the MCMC and find the value of $\\lambda$ that yields the lowest value of WAIC. \n\n\n::: {.cell hash='04-solution_cache/html/unnamed-chunk-5_4c45bcab79eba3bb538513ee490a1331'}\n\n```{.r .cell-code}\nset.seed(2023)\nblasso1 <- bayeslasso(lambda = 0.1)\nblasso2 <- bayeslasso(lambda = 0.2)\nblasso3 <- bayeslasso(lambda = 1)\n2*length(diabetes$y)*c(attr(blasso1, \"waic\"), attr(blasso2, \"waic\"), attr(blasso3, \"waic\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7865253 0.7874587 0.7934965\n```\n:::\n:::\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}