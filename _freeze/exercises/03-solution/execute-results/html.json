{
  "hash": "3124b749f8cb411169e416b7935fb7b0",
  "result": {
    "markdown": "---\ntitle: \"Solution 3\"\n---\n\n\n\n## Exercise 3.1\n\nConsider the Laplace family of distribution, $\\mathsf{La}(\\nu, \\tau)$, with density\n\\begin{align*}\ng(x; \\nu, \\tau) = \\frac{1}{2\\tau} \\exp\\left(- \\frac{|x-\\nu|}{\\tau}\\right), \\qquad \\nu \\in \\mathbb{R}, \\tau > 0\n\\end{align*}\nas a candidate distribution for rejection sampling from $\\mathsf{No}(0,1)$.\n\n1. Provide an inversion sampling algorithm to generate from $\\mathsf{La}(\\nu, \\tau)$.\n2. Can you use the proposal to generate from a standard Gaussian? for Student-$t$ with 1 degree of freedom? Justify your answer.\n3. Consider as proposal a location-scale version of the Student-t with $\\nu=3$\n degrees of freedom. Find the optimal location and scale parameters and the upper bound $C$ for your choice.\n4. Use the accept-reject to simulate 1000 independent observations and compute the empirical acceptance rate.\n\n\n\n::: {.solution}\n\n1. The distribution function is \n\\begin{align*}\nF(x) = \\begin{cases}\n\\frac{1}{2} \\exp\\left(\\frac{x-\\nu}{\\tau}\\right) & x \\leq \\nu\\\\\n1 - \\frac{1}{2} \\exp\\left(-\\frac{x-\\nu}{\\tau}\\right) & x >\\nu\\\\\n\\end{cases}\n\\end{align*}\nand using the quantile transform, set $X=\\nu + \\tau \\log(2U)$ if $U \\leq 0.5$ and $X=\\nu - \\tau\\log(2-2U)$ if $U > 0.5$ for $U \\sim \\mathsf{U}(0,1)$.\n2. The Gaussian has lighter tail than the Laplace, so this won't work. The Cauchy distribution would be a suitable candidate, albeit too heavy tailed.\n3. The optimal value for the location of the Student-$t$ would be $\\nu$ (here zero for the standard Laplace). We compute the optimal scale via in the code below:\n$$\n\\mathrm{argmax}_{\\sigma \\in \\mathbb{R}_{+}}\\mathrm{argmin}_{x \\in \\mathbb{R}} \\{\\log f(x) - \\log g(x; \\sigma)\\}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Laplace density\ndlaplace <- function(x, loc = 0, scale = 1, log = FALSE){\n stopifnot(scale > 0)\n logdens <-  -log(2*scale) - abs(x-loc)/scale\n if(log){\n return(logdens)\n } else{\n return(exp(logdens))\n }\n}\ndstudent <- function(x, loc = 0, scale = 1, df = 1, log = FALSE){\n  logdens <- -log(scale) + dt(x = (x - loc)/scale, df = df, log = TRUE)\n   if(log){\n    return(logdens)\n   } else{\n   return(exp(logdens))\n   }\n}\n# For each value of the scale sigma,\n# find the minimum value of x (typically at zero)\nopt <- optimize(f = function(sigma){\n  optimize(f = function(x){\n  dlaplace(x, log = TRUE) - \n    dstudent(x, scale = sigma, df = 3, log = TRUE)}, \n  maximum = TRUE, \n  interval = c(-100, 100))$objective\n}, interval = c(0.1,10))\n(C <- exp(opt$objective))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.261368\n```\n:::\n\n```{.r .cell-code}\n(sigma <- opt$minimum)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9272381\n```\n:::\n\n```{.r .cell-code}\n# Simulate from accept-reject\nntrials <- 1.1*C*1000\n# Simulate from location-scale student\ncandidate <- sigma*rt(n = ntrials, df = 3)\n# Compute log of acceptance rate\nlogR <- dlaplace(candidate, log = TRUE) - \n  dstudent(candidate, scale = sigma, df = 3, log = TRUE) \nsamp <- candidate[logR >= log(C) + -rexp(ntrials)]\n# Monte Carlo estimator of the acceptance rate\nntrials/length(samp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.252261\n```\n:::\n\n```{.r .cell-code}\n# Plot density\nlibrary(ggplot2)\nggplot(data = data.frame(x = samp),\n       mapping = aes(x = x)) +\n  geom_density() +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](03-solution_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n4. The Monte Carlo acceptance rate is 1.25, compared with the analytical bound found via numerical optimization of 1.26, to two significant digits.\n\n:::\n\n\n## Exercise 3.2\n\nWe revisit [Exercise 1.3](/exercises/01-exercise.html#exercice-1.3), which used a half-Cauchy prior for the exponential waiting time of buses.\n\nThe ratio-of-uniform method, implemented in the [`rust` **R** package](https://paulnorthrop.github.io/rust/index.html), can be used to simulate independent draws from the posterior of the rate $\\lambda$.\nThe following code produces \n\n::: {.cell}\n\n```{.r .cell-code}\nnobs <- 10L # number of observations\nybar <- 8   # average waiting time\nB <- 1000L  # number of draws\n# Un-normalized log posterior: scaled log likelihood + log prior\nupost <- function(x){ \n  dgamma(x = x, shape = nobs + 1L, rate = nobs*ybar, log = TRUE) +\n    log(2) + dt(x = x, df = 1, log = TRUE)}\npost_samp <- rust::ru(logf = upost, \n                      n = B, \n                      d = 1,  # dimension of parameter (scalar)\n                      init = nobs/ybar)$sim_vals # initial value of mode\n```\n:::\n\n\nEstimate using the Monte Carlo sample:\n\n1. the probability that the waiting time is between 3 and 15 minutes\n2. the average waiting time\n3. the standard deviation of the waiting time\n\n\n::: {.solution}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Lambda is the reciprocal mean (1/minute)\ntimes <- 1/post_samp\nmean(times > 3 & times < 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.987\n```\n:::\n\n```{.r .cell-code}\nmean(times)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.031375\n```\n:::\n\n```{.r .cell-code}\nsd(times)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.53749\n```\n:::\n:::\n\n\n:::\n\nNext, implement a random walk Metropolis--Hastings algorithm to sample draws from the posterior and re-estimate the quantities. Compare the values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 1e4L\ncurr <- 8/10 # prior mean\nchains <- numeric(B) # container\nsd_prop <- 0.1\nfor(b in seq_len(B)){\n  prop <- rnorm(n = 1, mean = curr, sd = sd_prop)\n  if(upost(prop) - upost(curr) > -rexp(1)){\n    curr <- prop\n  }\n  chains[b] <- curr\n}\n# Discard burn-in\ntimes <- 1/chains[-(1:100)]\nmean(times > 3 & times < 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9764646\n```\n:::\n\n```{.r .cell-code}\nmean(times)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.034626\n```\n:::\n\n```{.r .cell-code}\nsd(times)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.619788\n```\n:::\n\n```{.r .cell-code}\n# Summary of MCMC\nmcmc <- coda::mcmc(chains)\nsummary(mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n     0.1373035      0.0445814      0.0004458      0.0012284 \n\n2. Quantiles for each variable:\n\n   2.5%     25%     50%     75%   97.5% \n0.06726 0.10784 0.13248 0.16139 0.23015 \n```\n:::\n:::\n\n\n\n## Exercise 3.3\n\nRepeat the simulations in Example 3.6, this time with a parametrization in terms of log rates $\\lambda_i$ $(i=1,2)$, with the same priors. Use a Metropolis--Hastings algorithm with a Gaussian random walk proposal, updating parameters one at a time. Run four chains in parallel.\n\n1. Tune the variance to reach an approximate acceptance rate of 0.44.\n2. Produce diagnostic plots (scatterplots of observations, marginal density plots, trace plots and correlograms). See [`bayesplot`](http://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html) or `coda`. Comment on  the convergence and mixing of your Markov chain Monte Carlo.\n3. Report summary statistics of the chains.\n\n:::{ .solution}\n\nThe only thing we need to do is change the parametrization, and add tuning of the variance.\n\n::: {.cell hash='03-solution_cache/html/unnamed-chunk-5_020d9a6ee1e1e15986d4ed0e8b80ddc4'}\n\n```{.r .cell-code}\ndata(upworthy_question, package = \"hecbayes\")\n# Compute sufficient statistics\ndata <- upworthy_question |>\n  dplyr::group_by(question) |>\n  dplyr::summarize(ntot = sum(impressions),\n                   y = sum(clicks))\n# Code log posterior as sum of log likelihood and log prior\nloglik <- function(par, counts = data$y, offset = data$ntot, ...){\n  lambda <- exp(c(par[1] + log(offset[1]), par[2] + log(offset[2])))\n sum(dpois(x = counts, lambda = lambda, log = TRUE))\n}\nlogprior <- function(par, ...){\n  sum(dnorm(x = par, mean = log(0.01), sd = 1.5, log = TRUE))\n}\nlogpost <- function(par, ...){\n  loglik(par, ...) + logprior(par, ...)\n}\n# Compute maximum a posteriori (MAP)\nmap <- optim(\n  par = rep(-4, 2),\n  fn = logpost,\n  control = list(fnscale = -1),\n  offset = data$ntot,\n  counts = data$y,\n  hessian = TRUE)\n# Use MAP as starting value\ncur <- map$par\n# Compute logpost_cur - we can keep track of this to reduce calculations\nlogpost_cur <- logpost(cur)\n# Proposal covariance\ncov_map <- -2*solve(map$hessian)\nchol <- chol(cov_map)\n\nset.seed(80601)\nniter <- 1e4L\nnchains <- 4L\nnpar <- 2L\nchain <- array(0, dim = c(niter, nchains, npar))\nnaccept <- 0L\nfor(j in 1:nchains){\n  for(i in seq_len(niter)){\n    # Multivariate normal proposal - symmetric random walk\n    prop <- chol %*% rnorm(n = 2)/1.05 + cur\n    logpost_prop <- logpost(prop)\n    # Compute acceptance ratio (no q because the ratio is 1)\n    logR <- logpost_prop - logpost_cur\n    if(logR > -rexp(1)){\n      cur <- prop\n      logpost_cur <- logpost_prop\n      naccept <- naccept + 1L\n    }\n    chain[i,j,] <- cur\n  }\n}\n# Acceptance rate\nnaccept/(nchains * niter)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.44355\n```\n:::\n\n```{.r .cell-code}\n# Create some summary graphs\nlibrary(bayesplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is bayesplot version 1.10.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- Online documentation and vignettes at mc-stan.org/bayesplot\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- bayesplot theme set to bayesplot::theme_default()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n   * Does _not_ affect other ggplot2 plots\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n   * See ?bayesplot_theme_set for details on theme setting\n```\n:::\n\n```{.r .cell-code}\n# Name chains so that the graphs can be labelled\ndimnames(chain)[[3]] <- c(\"lambda[1]\",\"lambda[2]\")\n\n# Trace plots, correlograms, density plots and bivariate scatterplot\nbayesplot::mcmc_trace(chain)\n```\n\n::: {.cell-output-display}\n![](03-solution_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_acf_bar(chain)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `facets` argument of `facet_grid()` is deprecated as of ggplot2 2.2.0.\nℹ Please use the `rows` argument instead.\nℹ The deprecated feature was likely used in the bayesplot package.\n  Please report the issue at <https://github.com/stan-dev/bayesplot/issues/>.\n```\n:::\n\n::: {.cell-output-display}\n![](03-solution_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_dens(chain)\n```\n\n::: {.cell-output-display}\n![](03-solution_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n\n```{.r .cell-code}\nbayesplot::mcmc_scatter(chain)\n```\n\n::: {.cell-output-display}\n![](03-solution_files/figure-html/unnamed-chunk-5-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# Posterior summaries with coda\n# Need to create a list of mcmc objects...\nmcmc_list <- coda::as.mcmc.list(\n    lapply(seq_len(nchains),\n           function(ch) coda::as.mcmc(chain[, ch, ])))\nsummary(mcmc_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 4 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean       SD  Naive SE Time-series SE\nlambda[1] -4.513 0.001730 8.648e-06      2.384e-05\nlambda[2] -4.442 0.001192 5.962e-06      1.670e-05\n\n2. Quantiles for each variable:\n\n            2.5%    25%    50%    75%  97.5%\nlambda[1] -4.516 -4.514 -4.513 -4.511 -4.509\nlambda[2] -4.444 -4.443 -4.442 -4.441 -4.440\n```\n:::\n:::\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}