{
  "hash": "6127f5a4f849e2b690174367c85ea4c0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Solution 10\n---\n\n\n\n\n\n## Exercise 10.1\n\nSuppose that $p(\\boldsymbol{\\theta})$ is a density with $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ and that we consider a Gaussian approximation $q(\\boldsymbol{\\theta})$ with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$. Show that the parameters that minimize the Kullback--Leibler $\\mathsf{KL}\\{p(\\boldsymbol{\\theta} \\parallel q(\\boldsymbol{\\theta})\\}$ are the expectation and variance under $p(\\boldsymbol{\\theta})$.^[Note that this is not a variational approximation!]\n      \n:::{.solution}\n\nRefer to the [Matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)\n\nConsider $g \\equiv \\phi_d(\\cdot; \\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ a Gaussian density and $p(\\boldsymbol{\\theta})$ the target. Optimization of the parameters of $g$ can be performed by differentiation of the target. The Kullback--Leibler divergence involves two terms: the negative entropy, which is constant, and the expected value of log density, so\n\\begin{align*}\n\\mathsf{KL}(p \\parallel g) \\stackrel{\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}}{\\propto} -\\frac{1}{2} \\mathsf{E}_{p}\\left\\{ (\\boldsymbol{\\theta}-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})\\right\\} - \\frac{1}{2} \\log |\\boldsymbol{\\Sigma}|\n\\end{align*}\nTaking gradients (formulas 57 and 61 of the Matrix Cookbook) and interchanging the order of integration and differentiation, assuming that the first two moments of $p(\\boldsymbol{\\theta})$ exists, gives \n\\begin{align*}\n\\frac{\\partial}{\\partial \\boldsymbol{\\mu}} \\mathsf{KL}(p \\parallel g) &= \\mathsf{E}_{p}\\left\\{(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})^\\top\\right\\}\\boldsymbol{\\Sigma}^{-1} \\\\\n\\frac{\\partial}{\\partial \\boldsymbol{\\Sigma}} \\mathsf{KL}(p \\parallel g) &= \\frac{1}{2} \\boldsymbol{\\Sigma}^{-1} \n\\mathsf{E}_{p}\\left\\{(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})^\\top\\right\\}\\boldsymbol{\\Sigma}^{-1}  - \\frac{1}{2}\\boldsymbol{\\Sigma}^{-1}\n\\end{align*}\nEquating these to zero yields $\\mathsf{E}_p(\\boldsymbol{\\theta}) = \\boldsymbol{\\mu}$ and $\\mathsf{E}_p\\left\\{(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})^\\top\\right\\} = \\boldsymbol{\\Sigma}$ provided $\\boldsymbol{\\Sigma}$ is invertible.\n\n:::\n      \n      \n## Exercise 10.2\n\nConsider a finite mixture model of $K$ univariate Gaussian $\\mathsf{Gauss}(\\mu_k, \\tau_k^{-1})$ with $K$ fixed, whose density is\n\\begin{align*}\n\\sum_{k=1}^{K} w_k \\left(\\frac{\\tau_k}{2\\pi}\\right)^{1/2}\\exp \\left\\{-\\frac{\\tau_k}{2}(y_i-\\mu_k)^2\\right\\}\n\\end{align*}\nwhere $\\boldsymbol{w} \\in \\mathbb{S}_{K-1}$ are positive weights that sum to one, meaning $w_1 + \\cdots + w_K=1.$ We use conjugate priors \n$\\mu_k \\sim \\mathsf{Gauss}(0, 100)$, $\\tau_k \\sim \\mathsf{Gamma}(a, b)$ for $k=1, \\ldots, K$ and $\\boldsymbol{w} \\sim \\mathsf{Dirichlet}(\\alpha)$ for $a, b, \\alpha>0$ fixed hyperparameter values.\n\nTo help with inference, we introduce auxiliary variables $\\boldsymbol{U}_1, \\ldots, \\boldsymbol{U}_n$ where $\\boldsymbol{U}_i \\sim \\mathsf{multinom}(1, \\boldsymbol{w})$ that indicates which cluster component the model belongs to, and $w_k = \\Pr(U_{ik}=1).$\n\nThe parameters and latent variables for the posterior of interest are $\\boldsymbol{\\mu}, \\boldsymbol{\\tau}, \\boldsymbol{w}$ and $\\boldsymbol{U}.$ Consider a factorized decomposition in which each component is independent of the others, $g_{\\boldsymbol{w}}(\\boldsymbol{w})g_{\\boldsymbol{\\mu}}(\\boldsymbol{\\mu})g_{\\boldsymbol{\\tau}}(\\boldsymbol{\\tau}) g_{\\boldsymbol{U}}(\\boldsymbol{u}).$\n\n1. Apply the coordinate ascent algorithm to obtain the distribution for the optimal components.\n2. Write down an expression for the ELBO.\n3. Run the algorithm for the `geyser` data from `MASS` **R** package for $K=2$ until convergence with $\\alpha=0.1, a=b=0.01.$ Repeat multiple times with different initializations and save the ELBO for each run.\n4. Repeat these steps for $K=2, \\ldots, 6$ and plot the ELBO as a function of $K$. Comment on the optimal number of cluster components suggested by the approximation to the marginal likelihood.\n\n\n:::{.solution}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#############################################################\n## CAVI algorithm for K-components Gaussian mixture models\n#############################################################\n# Parameters are\n# 1) weights w (probability of components)\n# 2) cluster means mu\n# 3) cluster variance sigma_sq\n# 4) binary indicators of clusters \"a\" (data augmentation)\nCAVI_gauss_mixt <- function(K = 2L, y,\n                prior_mean_mu = rep(0, K),\n                prior_var_mu = rep(1e8, K),\n                prior_shape_var = rep(0.01, K),\n                prior_rate_var = rep(0.01, K),\n                prior_shape_weights = 0.01,\n                maxiter = 1000L,\n                tol = 1e-8\n                ){\n  stopifnot(K >= 1)\n  K <- as.integer(K)\n  n <- length(y)\n  # ELBO normalizing constant (only depends on hyperparameters)\nlcst <- 0.5*K*(1-n*log(2*pi)) +\n  lgamma(K*prior_shape_weights) -\n  K*lgamma(prior_shape_weights) -\n  lgamma(n + K*prior_shape_weights)  +\n  sum(prior_shape_var*log(prior_rate_var)) -\n  sum(lgamma(prior_shape_var))\n# Initialization\nmu <- runif(K)*diff(range(y))\nsigma_sq <- alpha <- rep(1, K)\nA <- B <- rep(1, K) # K vector\nELBO <- numeric(maxiter)\nnu <- matrix(NA, nrow = n, ncol = K)\n# CAVI runs\nfor(b in seq_len(maxiter)){\n  for(k in seq_len(K)){\n    nu[, k] <- exp(digamma(alpha[k]) + 0.5*digamma(A[k]) - 0.5*log(B[k]) -\n                   0.5*A[k]/B[k]*((y-mu[k])^2+ sigma_sq[k]))\n  }\n  omega <- nu/rowSums(nu) # Probability of components for each obs\n  om <- colSums(omega) # sample size in each cluster\n  sigma_sq <- 1/(1/prior_var_mu + A*om/B) # posterior variance of cluster\n  mu <- sigma_sq*(prior_mean_mu/prior_var_mu + A/B*colSums(omega * y)) # posterior mean of cluster\n  alpha <- prior_shape_weights + om\n  A <- prior_shape_var + 0.5*om\n  B <- prior_rate_var + 0.5*(colSums(omega*(outer(y, mu, FUN = \"-\")^2)) + om*sigma_sq)\n  # Compute ELBO\n  ELBO[b] <- lcst + sum(lgamma(A) - A*log(B) +\n    lgamma(alpha) + 0.5*(log(sigma_sq) - log(prior_var_mu)) -\n    0.5*((mu-prior_mean_mu)^2 + sigma_sq)/prior_var_mu) -\n    sum(omega*log(omega+1e-80))\n  if(b >=2){\n    if((ELBO[b]-ELBO[b-1]) < tol){\n      break\n    }\n  }\n}\nlist(elbo = ELBO[1:b],\n     mu_mean = mu,\n     mu_var = sigma_sq,\n     sigmasg_shape = A,\n     sigmasg_rate = B,\n     weights_alpha = alpha,\n     probs = omega,\n     mean_probs = alpha/sum(alpha),\n     cluster_probs = colSums(omega)/sum(omega)\n)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Fit the model\ndata(geyser, package = \"MASS\")\nmixt <- CAVI_gauss_mixt(K = 2, y = geyser$duration)\nplot(mixt$elbo, \n     xlab = \"number of iterations\", \n     ylab = \"evidence lower bound\", \n     type = \"b\")\n```\n\n::: {.cell-output-display}\n![](10-solution_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Compute posterior mean of mean, variance and probability of each cluster\npost_prob <- mixt$mean_probs\npost_mean <- mixt$mu_mean\npost_sd <- sqrt(mixt$sigmasg_rate/(mixt$sigmasg_shape-1))\n# Plot the mixture density at posterior mean\ncurve(post_prob[1]*dnorm(x, mean = post_mean[1], sd = post_sd[1]) +\n        post_prob[2]*dnorm(x, mean = post_mean[2], sd = post_sd[2]),\n      from = 1,\n      to = 6,\n      n = 1001,\n      xlab = \"duration of geyser eruption (in minutes)\",\n      ylab = \"density\")\nrug(geyser$duration)\n```\n\n::: {.cell-output-display}\n![](10-solution_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n",
    "supporting": [
      "10-solution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}