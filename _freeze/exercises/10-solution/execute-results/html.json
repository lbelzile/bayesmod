{
  "hash": "b8a6872bf1e5654eb0ff0d97809d5cd9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Solution 10\n---\n\n\n\n\n\n## Exercise 10.1\n\nSuppose that $p(\\boldsymbol{\\theta})$ is a density with $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ and that we consider a Gaussian approximation $q(\\boldsymbol{\\theta})$ with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$. Show that the parameters that minimize the Kullback--Leibler $\\mathsf{KL}\\{p(\\boldsymbol{\\theta} \\parallel q(\\boldsymbol{\\theta})\\}$ are the expectation and variance under $p(\\boldsymbol{\\theta})$.^[Note that this is not a variational approximation!]\n      \n:::{.solution}\n\nRefer to the [Matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)\n\nConsider $g \\equiv \\phi_d(\\cdot; \\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ a Gaussian density and $p(\\boldsymbol{\\theta})$ the target. Optimization of the parameters of $g$ can be performed by differentiation of the target. The Kullback--Leibler divergence involves two terms: the negative entropy, which is constant, and the expected value of log density, so\n\\begin{align*}\n\\mathsf{KL}(p \\parallel g) \\stackrel{\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}}{\\propto} -\\frac{1}{2} \\mathsf{E}_{p}\\left\\{ (\\boldsymbol{\\theta}-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})\\right\\} - \\frac{1}{2} \\log |\\boldsymbol{\\Sigma}|\n\\end{align*}\nTaking gradients (formulas 57 and 61 of the Matrix Cookbook) and interchanging the order of integration and differentiation, assuming that the first two moments of $p(\\boldsymbol{\\theta})$ exists, gives \n\\begin{align*}\n\\frac{\\partial}{\\partial \\boldsymbol{\\mu}} \\mathsf{KL}(p \\parallel g) &= \\mathsf{E}_{p}\\left\\{(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})^\\top\\right\\}\\boldsymbol{\\Sigma}^{-1} \\\\\n\\frac{\\partial}{\\partial \\boldsymbol{\\Sigma}} \\mathsf{KL}(p \\parallel g) &= \\frac{1}{2} \\boldsymbol{\\Sigma}^{-1} \n\\mathsf{E}_{p}\\left\\{(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})^\\top\\right\\}\\boldsymbol{\\Sigma}^{-1}  - \\frac{1}{2}\\boldsymbol{\\Sigma}^{-1}\n\\end{align*}\nEquating these to zero yields $\\mathsf{E}_p(\\boldsymbol{\\theta}) = \\boldsymbol{\\mu}$ and $\\mathsf{E}_p\\left\\{(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})(\\boldsymbol{\\theta}-\\boldsymbol{\\mu})^\\top\\right\\} = \\boldsymbol{\\Sigma}$ provided $\\boldsymbol{\\Sigma}$ is invertible.\n\n:::\n      \n      \n## Exercise 10.2\n\nConsider a finite mixture model of $K$ univariate Gaussian $\\mathsf{Gauss}(\\mu_k, \\tau_k^{-1})$ with $K$ fixed, whose density is\n\\begin{align*}\n\\sum_{k=1}^{K} w_k \\left(\\frac{\\tau_k}{2\\pi}\\right)^{1/2}\\exp \\left\\{-\\frac{\\tau_k}{2}(y_i-\\mu_k)^2\\right\\}\n\\end{align*}\nwhere $\\boldsymbol{w} \\in \\mathbb{S}_{K-1}$ are positive weights that sum to one, meaning $w_1 + \\cdots + w_K=1.$ We use conjugate priors \n$\\mu_k \\sim \\mathsf{Gauss}(0, 100)$, $\\tau_k \\sim \\mathsf{Gamma}(a, b)$ for $k=1, \\ldots, K$ and $\\boldsymbol{w} \\sim \\mathsf{Dirichlet}(\\alpha)$ for $a, b, \\alpha>0$ fixed hyperparameter values.\n\nTo help with inference, we introduce auxiliary variables $\\boldsymbol{U}_1, \\ldots, \\boldsymbol{U}_n$ where $\\boldsymbol{U}_i \\sim \\mathsf{multinom}(1, \\boldsymbol{w})$ that indicates which cluster component the model belongs to, and $w_k = \\Pr(U_{ik}=1).$\n\nThe parameters and latent variables for the posterior of interest are $\\boldsymbol{\\mu}, \\boldsymbol{\\tau}, \\boldsymbol{w}$ and $\\boldsymbol{U}.$ Consider a factorized decomposition in which each component is independent of the others, $g_{\\boldsymbol{w}}(\\boldsymbol{w})g_{\\boldsymbol{\\mu}}(\\boldsymbol{\\mu})g_{\\boldsymbol{\\tau}}(\\boldsymbol{\\tau}) g_{\\boldsymbol{U}}(\\boldsymbol{u}).$\n\n1. Apply the coordinate ascent algorithm to obtain the distribution for the optimal components.\n2. Write down an expression for the ELBO.\n3. Run the algorithm for the `geyser` data from `MASS` **R** package for $K=2$ until convergence with $\\alpha=0.1, a=b=0.01.$ Repeat multiple times with different initializations and save the ELBO for each run.\n4. Repeat these steps for $K=2, \\ldots, 6$ and plot the ELBO as a function of $K$. Comment on the optimal number of cluster components suggested by the approximation to the marginal likelihood.\n\n\n:::{.solution}\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](10-solution_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](10-solution_files/figure-revealjs/unnamed-chunk-2-2.png){width=960}\n:::\n:::\n\n\n\n\n\n:::\n",
    "supporting": [
      "10-solution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}