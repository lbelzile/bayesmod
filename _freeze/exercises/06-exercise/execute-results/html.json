{
  "hash": "36a89eaea23aa3c49956241707641bf0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercises 6\"\n---\n\n\n\n\n\n## Exercise 6.1\n\nConsider the eight school data from Educational Testing Service (ETS), featured prominently in @Gelman:2013. The data shows results of randomized experiments in eight different schools to estimate the effect of coaching on SAT-V scores.\nThe data consist of estimated treatment mean difference and their standard errors. The data size are large, so the average treatment effect (ATE) can be considered Gaussian and the standard errors are treated as a known quantity.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06-exercise_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\n\nConsider a one-way ANOVA with a single observation per group, where for $i=1, \\ldots, n,$ we have\n\\begin{align*}\nY_{i} &\\sim \\mathsf{Gauss}(\\mu + \\alpha_i, \\sigma^2_i) \\\\\n\\alpha_i &\\sim \\mathsf{Gauss}(0, \\sigma^2_\\alpha)\n\\end{align*}\nand an improper prior for the mean $p(\\mu) \\propto 1.$\nIf there are $K$ groups, then the group-specific mean is $\\mu+\\alpha_k$, and there is a redundant parameter. In the Bayesian setting, the parameters are weakly identifiable because of the prior on $\\boldsymbol{\\alpha}$, but the geometry is such that $\\alpha_k \\to 0$ as $\\sigma^2_{\\alpha} \\to 0.$ See [Section 5.5](https://users.aalto.fi/~ave/BDA3.pdf) on p. 119 of @Gelman:2013 for more details.\n\nWe fit the model by adding redundant parameter to improve mixing [@Gelman.vanDyk:2008]\n\\begin{align*}\nY_{i} &\\sim \\mathsf{Gauss}(\\mu + \\alpha\\xi_i, \\sigma^2_i) \\\\\n\\xi_i &\\sim \\mathsf{Gauss}(0, \\sigma^2_\\xi)\n\\end{align*}\nso that $\\sigma_\\alpha = |\\alpha|\\sigma_\\xi.$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Eight school data\nes <- data.frame(school = LETTERS[1:8], \n                 y = c(28, 8, -3, 7, -1, 1, 18, 12), \n                 se = c(15, 10, 16, 11, 9, 11, 10, 18))\n```\n:::\n\n\n\n\n\n1. Derive the Gibbs sampler for the two models with improper priors. Note that, to sample jointly from $p(\\mu, \\boldsymbol{\\alpha} \\mid \\boldsymbol{y}, \\sigma^2_{\\alpha}, \\boldsymbol{\\sigma}^2_{\\boldsymbol{y}})$, you can draw from the marginal of $\\boldsymbol{\\alpha}$, then the conditional $\\mu \\mid \\boldsymbol{\\alpha}, \\cdots$.\n2. Fit the three algorithms described in @Gelman.vanDyk:2008 and compare their efficiency via effective sample size (ESS). In particular, draw traceplots and calculate effective sample sizes for \n    \n    - $\\mu$\n    - $\\alpha_1$, \n    - the sum $\\mu + \\alpha_1$.\n\nas well as pairs plot of $\\alpha_i, \\log \\sigma_{\\alpha}$. The latter should display a very strong funnel.\n\nSee this [post](https://xuwd11.github.io/am207/wiki/gelmanschools.html) for display of the pathological behaviour that can be captured by divergence in Hamiltonian Monte Carlo.\n\n## Exercise 6.2\n\nLet $Y_{ij1}$ ($Y_{ij2}$) denote the score of the home (respectively visitor) team for a soccer match opposing teams $i$ and $j$. @Maher:1982 suggested modelling the scores as\n\\begin{align*}\nY_{ij1} \\mid \\delta, \\alpha_i, \\beta_j &\\sim \\mathsf{Poisson}\\{\\exp(\\delta + \\alpha_i +\\beta_j)\\},\n\\\\ Y_{ij2} \\mid \\delta, \\alpha_j, \\beta_i &\\sim \\mathsf{Poisson}\\{\\exp(\\alpha_j+\\beta_i)\\},\n\\end{align*}\nwhere \n\n- $\\alpha_i$ represent the offensive strength of the team, \n- $\\beta_j$ the defensive strength of team $j$ and \n- $\\delta$ is the common home advantage. \n\nThe scores in a match and between matches are assumed to be conditionally independent of one another given $\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\delta$. The data set [`efl`](/files/data/efl.csv) contains the results of football (soccer) matches for the 2015 season of the English Premier Ligue (EPL) and contains the following variables\n\n- `score`: number of goals of `team` during a match\n- `team`: categorical variable giving the name of the team which scored the goals\n- `opponent`: categorical variable giving the name of the adversary\n- `home`: binary variable, 1 if `team` is playing at home, 0 otherwise.\n\n\n1. Specify penalized complexity priors for the regression parameters scale $\\sigma$ that shrink $\\alpha_i \\sim \\mathsf{Gauss}(0, \\sigma^2)$ and $\\beta_j \\sim \\mathsf{Gauss}(0, \\sigma^2)$ to zero, with a probability of 1% of exceeding 1.\n2. Fit the model and answer the following questions: \n   - Using the posterior distribution, give the expected number of goals for a match between Manchester United (at home) against Liverpool.\n   - Report and interpret the estimated posterior mean home advantage $\\widehat{\\delta}$. \n   - Calculate the probability that the home advantage $\\delta$ is positive?\n3. Comment on the adequacy of the fit by using a suitable statistic for the model fit (e.g. the deviance statistic\n4. Maher also suggested more complex models, including one in which the offensive and defensive strength of each team changed depending on whether they were at home or visiting another team, i.e.\n\\begin{align}\nY_{ij1} \\sim \\mathsf{Poisson}\\{\\exp(\\alpha_i +\\beta_j + \\delta)\\}, \nY_{ij2} \\sim \\mathsf{Poisson}\\{\\exp(\\gamma_j+\\omega_i)\\},\n\\end{align}\nDoes the second model fit significantly better than  than the first? Compare the models using WAIC and Bayes factors.\n\n\n",
    "supporting": [
      "06-exercise_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}