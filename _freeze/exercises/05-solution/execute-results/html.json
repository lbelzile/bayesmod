{
  "hash": "afc0fbd8213eecf3775d45c5ab982e9b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Solution 5\"\n---\n\n\n\n\n\n## Exercise 5.1\n\nThe Pareto distribution with shape $\\alpha>0$ and scale $\\tau>0$ has density\n$$\nf(x; \\alpha, \\tau) = \\alpha x^{-\\alpha-1}\\tau^\\alpha \\mathrm{I}(x > \\tau).\n$$\nIt can be used to model power laws in insurance and finance, or in demography. The `uscitypopn` data set in the `hecbayes` package contains the population size of cities above 200K inhabitants in the United States, from the 2020 census.\n\n1. Using improper priors $p(\\alpha, \\tau) \\propto 1,$ write the joint posterior for a simple random sample of size $n$ and derive the conditional distributions $p(\\alpha \\mid \\boldsymbol{y}, \\tau)$ and $p(\\tau \\mid \\alpha, \\boldsymbol{y})$.\n2. The mononomial distribution $\\mathsf{Mono}(a,b)$ has density $p(x) \\propto x^{a-1}\\mathrm{I}(0 \\leq x \\leq b)$. Find the normalizing constant for the distribution and obtain the quantile function to derive a sampler.\n3. Implement Gibbs sampling for this problem for the `uscitypopn` data. Draw enough observations to obtain an effective sample size of at least 1000 observations. Calculate the accuracy of your estimates?\n\n\n\n::: {.solution}\n\nWith improper prior, the joint posterior is the product of the likelihood contributions so\n$$\np(\\alpha, \\tau \\mid \\boldsymbol{y}) \\propto \\alpha^n \\left(\\prod_{i=1}^n y_i\\right)^{-\\alpha-1} \\tau^{n\\alpha} \\mathrm{I}(\\min_i y_i > \\tau).\n$$\nUsing the hint, write the conditional density for $\\alpha$ given the rest as\n\\begin{align*}\np(\\alpha \\mid \\boldsymbol{y}, \\tau) \\propto \\alpha^n \\left( \\frac{\\prod_{i=1}^n y_i}{\\tau^n}\\right)^{-\\alpha} = \\alpha^{(n+1)-1} \\exp\\left\\{-\\alpha \\left(\\sum_{i=1}^n\\log y_i - n\\log \\tau\\right) \\right\\}\n\\end{align*}\nwhich is $\\mathsf{Gamma}\\big(n+1, \\sum_{i=1}^n \\log y_i - n \\log \\tau \\big)$. For the second, we have\n\\begin{align*}\np(\\tau \\mid \\alpha, \\boldsymbol{y}) \\propto \\tau^{n\\alpha} \\mathrm{I}(\\min_{i} y_i > \\tau),\n\\end{align*}\na mononomial distribution with parameters $a=n\\alpha+1$ and $b = \\min_{i} y_i$.\n\nTo find the normalizing constant of the mononomial distribution, we simply integrate the unnormalized density to obtain the reciprocal constant: if $c = \\int g(x) \\mathrm{d} x$ for $c < \\infty$ and $g(x) \\geq 0$ for all $x$, then $g(x)/c$ integrates to one and is a valid density. Thus, we find \n$$c= \\int_0^b x^{a-1}\\mathrm{d} x = \\left[\\frac{x^{a}}{a}\\right]_{0}^b= \\frac{b^{a}}{a}.$$\nThe distribution function is $G(x) = (x/b)^{a}$ for $x \\in [0,b]$ and the quantile function $G^{-1}(u) = u^{1/a}b$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqmono <- function(u, a, b, log = FALSE){\n  stopifnot(isTRUE(all(a > 0, b > 0, u >= 0, u <= 1)))\n logq <-   log(u)/(a+1) + log(b)\n if(log){ return(logq)} else { return(exp(logq)) }\n}\n\n\n# Load data\ndata(\"uscitypopn\", package = \"hecbayes\")\ny <- uscitypopn$population\nn <- length(y)\n# Summary statistics appearing in the posterior distribution\nsumlogy <- sum(log(y))\nminy <- min(y)\n# MCMC via Gibbs sampling\nB <- 1e4L\nchains <- matrix(0, nrow = B, ncol = 2)\ncolnames(chains) <- c(\"alpha\", \"tau\")\ncurr <- c(2, 2e5)\nfor(b in seq_len(B)){\n  chains[b,1] <- curr[1] <- rgamma(n = 1, shape = n+1, rate = sumlogy - n*log(curr[2]))\n  chains[b,2] <- curr[2] <- qmono(runif(1), a = n*curr[1]+1, b = miny)\n}\nchains <- coda::as.mcmc(chains)\n# Compute effective sample size\ncoda::effectiveSize(chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    alpha       tau \n 9718.142 10000.000 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean        SD  Naive SE Time-series SE\nalpha 1.391e+00    0.1306  0.001306       0.001324\ntau   1.991e+05 1247.1410 12.471410      12.471410\n\n2. Quantiles for each variable:\n\n           2.5%       25%       50%       75%     97.5%\nalpha 1.143e+00 1.303e+00 1.385e+00 1.476e+00 1.657e+00\ntau   1.958e+05 1.987e+05 1.995e+05 2.000e+05 2.004e+05\n```\n\n\n:::\n:::\n\n\n\nWe can see that the autocorrelation is minimal, so the sampler is quite efficient.\n\n\n:::\n\n## Exercise 5.2\n\nImplement the Bayesian LASSO for the `diabetes` cancer surgery from package `lars`.  Check @Park.Casella:2008 for the details of the Gibbs sampling (p. 682, right column).\n\n1. Fit the model for a range of values of $\\lambda$ and produce parameter estimate paths to replicate Figure 2 of the paper.\n2. Check the effective sample size and comment on the mixing. Is it impacted by the tuning parameter?\n3. Implement the method of section 3.1 from @Park.Casella:2008 by adding $\\lambda$ as a parameter.\n4. For three models with different values of $\\lambda$, compute the widely applicable information criterion (WAIC) and use it to assess predictive performance.\n\n\n::: {.solution}\n\nWe first setup a Gibbs sampler for a given value of $\\lambda$, or using the empirical Bayes estimator provided in section 3.1. The effective sampling size for fixed $\\lambda$ is good. If we let the parameter varies, the performance degrades and we obtain an effective size shy of 1000 for 10K iterations for $\\lambda$, and comfortably above 5000 for others. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(diabetes, package = \"lars\")\nbayeslasso <- function(lambda = NULL, \n                       B = 1e4L,\n                       x = diabetes$x, \n                       y = diabetes$y){\n  stopifnot(is.matrix(x), is.vector(y))\n  # Scale inputs in case\n  x <- scale(x, center = TRUE, scale = FALSE)\n  y <- y - mean(y)\n  # Check method\n  if(is.null(lambda)){\n    method <- \"empbayes\"\n  } else{\n    method <- \"fixed\" \n  }\n  burnin <- 250L\n  # Precompute quantities and dimensions\n  xtx <- crossprod(x)\n  p <- ncol(x)\n  n <- nrow(x)\n  # Obtain initial estimates\n  linmod <- lm(y ~ x - 1)\n  betaols <- coef(linmod)\n  beta.curr <- betaols\n  sigmasq.curr <- mean(residuals(linmod)^2)\n  tausqinv.curr <- rep(1, p)\n  # Value reported in the text for the optimal parameter: lambda = 0.237\n  beta.ind <- 1:p\n  sigmasq.ind <- p + 1L\n  tausq.ind <- seq(from = p + 2L, length.out = p, by = 1L)\n  chains <- matrix(0, nrow = B, ncol = p + 1 + p + \n                     ifelse(method == \"fixed\", 0,1))\n  if(method == \"fixed\"){\n    colnames(chains) <- c(paste0(\"beta\", 1:p), \"sigmasq\",\n                          paste0(\"tausq\", 1:p))\n    lambdasq.curr <- lambda[1]^2\n  } else{\n    colnames(chains) <- c(paste0(\"beta\", 1:p), \"sigmasq\", \n                          paste0(\"tausq\", 1:p), \"lambda\")\n    lambdasq.curr <- p*sqrt(sigmasq.curr)/sum(abs(betaols))\n    lambdasq.ind <- ncol(chains)\n  }\n# MCMC loop\nfor(b in seq_len(B + burnin)){\n  ind <- pmax(1, b-burnin)\n  Ainv <- solve(xtx + diag(tausqinv.curr))\n  beta.curr <- chains[ind,beta.ind] <- as.numeric(\n    mvtnorm::rmvnorm(\n      n = 1, \n      mean = as.numeric(Ainv %*% t(x) %*% y), \n      sigma = sigmasq.curr*Ainv))\n  sigmasq.curr <- chains[ind, sigmasq.ind] <- 1/rgamma(\n    n = 1, \n    shape = (n-1+p)/2,\n    rate = sum((y-x %*% beta.curr)^2)/2 + \n      sum(beta.curr^2*tausqinv.curr)/2)\n  # Compute marginal posterior mean for lambda, using section 3.1\n  sumexpect <- 0\n  for(j in 1:p){\n    tausqinv.curr[j] <- actuar::rinvgauss(\n      n = 1, \n      mean = sqrt(lambdasq.curr*sigmasq.curr)/abs(beta.curr[j]),\n      dispersion = 1/lambdasq.curr)\n    if(method != \"fixed\"){\n    sumexpect <- sumexpect + mean(1/actuar::rinvgauss(\n      n = 1000, \n      mean = sqrt(lambdasq.curr*sigmasq.curr)/abs(beta.curr[j]),\n      dispersion = 1/lambdasq.curr))\n    }\n  }\n  chains[ind, tausq.ind] <- 1/tausqinv.curr\n  if(method != \"fixed\"){\n    lambdasq.curr <- chains[ind, lambdasq.ind] <- 2*p/sumexpect\n  }\n}\n  if(method != \"fixed\"){\n  chains[, lambdasq.ind] <- sqrt(chains[, lambdasq.ind])\n}\n# Cast Markov chains to mcmc class object.\nchains.mcmc <- coda::as.mcmc(chains)\n# Effective sample size\ness <- as.integer(round(coda::effectiveSize(chains.mcmc), 0))\n\n# Compute WAIC from log pointwise density\nlppd <- 0\npenalty <- 0\nfor(i in seq_len(n)){\n  lppd_i <- dnorm(\n    x = y[i], \n    mean = as.numeric(chains[,beta.ind] %*% c(x[i,])), \n    sd = sqrt(chains[,sigmasq.ind]), \n    log = TRUE)\n  lppd <- lppd + mean(lppd_i)\n  penalty <- penalty + var(lppd_i)\n}\nwaic <- (-lppd + penalty)/n\nl1norm <- mean(rowSums(abs(chains[,beta.ind])))\n\n# Parameter estimates and 95% equitailed credible intervals\nquant <- t(apply(chains, 2, \n                quantile, prob = c(0.025, 0.5, 0.975)))\nregpar <- as.data.frame(cbind(quant,\n                colMeans(chains),\n                coda::batchSE(chains.mcmc),\n                ess))\nregpar$pars <- rownames(quant)\nrownames(regpar) <- NULL\ncolnames(regpar) <- c(\"lower\", \"median\", \"upper\", \n                      \"mean\", \"se\", \"ess\", \"par\")\nregpar <- regpar[,c(7,4:5,1:3,6)]\nattr(regpar, \"waic\") <- waic\nattr(regpar, \"l1norm\") <- l1norm\n return(regpar)\n}\n\n# Call the MCMC sampler\nset.seed(2023)\nlasso_empbayes <- bayeslasso(lambda = NULL)\n# Extract the value of WAIC\nwaic <- attr(lasso_empbayes, \"waic\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Standardized median posterior estimates of the coefficients for the Bayesian LASSO with 95 percent equitailed credible intervals, with $\\lambda$ estimated using empirical Bayes. Ordinary least square estimates are denoted by crosses.](05-solution_files/figure-html/fig-parkcasellaf2-1.png){#fig-parkcasellaf2 width=672}\n:::\n:::\n\n\n\nThe plot corresponds to Figure 2 of @Park.Casella:2008 and the posterior summaries, reported in @tbl-coefs, are also in line with those of the paper.\n\n\n\n\n\n::: {#tbl-coefs .cell tbl-cap='Estimates posterior summaries from the Bayesian LASSO, based on 10K draws. Posterior means and adjusted standard errors, posterior median and equitailed 95 percent credible intervals, effective sample.'}\n::: {.cell-output-display}\n\n\n|par     |    mean|   se|   lower|  median|   upper|   ess|\n|:-------|-------:|----:|-------:|-------:|-------:|-----:|\n|beta1   |   -3.14| 0.58| -112.92|   -2.34|  105.68| 10000|\n|beta2   | -214.04| 0.58| -331.89| -213.49|  -97.58|  9012|\n|beta3   |  523.41| 0.76|  393.47|  523.87|  656.33|  9062|\n|beta4   |  307.64| 0.72|  177.38|  307.69|  439.33|  8957|\n|beta5   | -187.12| 2.60| -587.60| -170.71|  125.58|  5089|\n|beta6   |    7.79| 1.92| -272.14|   -1.62|  344.59|  6056|\n|beta7   | -154.43| 1.62| -385.12| -153.23|   72.40|  5259|\n|beta8   |   96.44| 1.60| -131.62|   87.10|  353.40|  6460|\n|beta9   |  524.09| 1.26|  331.93|  520.90|  727.05|  6630|\n|beta10  |   64.21| 0.69|  -49.05|   61.87|  188.78|  8897|\n|sigmasq | 2952.78| 2.00| 2585.93| 2945.02| 3370.20|  9511|\n|tausq1  |   20.55| 0.26|    0.24|   11.13|   93.70| 10000|\n|tausq2  |   34.75| 0.33|    3.42|   25.19|  122.06|  8868|\n|tausq3  |   58.44| 0.36|   13.42|   49.09|  155.41|  9375|\n|tausq4  |   41.99| 0.39|    6.09|   32.35|  133.19|  8972|\n|tausq5  |   34.17| 0.45|    1.03|   24.45|  122.56|  5159|\n|tausq6  |   26.82| 0.38|    0.57|   17.18|  105.39|  7441|\n|tausq7  |   31.06| 0.35|    1.04|   21.46|  118.21|  8954|\n|tausq8  |   27.47| 0.33|    0.56|   17.94|  107.05|  8551|\n|tausq9  |   59.02| 0.47|   12.95|   49.73|  160.42|  7805|\n|tausq10 |   23.35| 0.30|    0.40|   13.66|  102.21|  9703|\n|lambda  |    0.24| 0.00|    0.21|    0.24|    0.26|  1038|\n\n\n:::\n:::\n\n\n\n\nFor the last part, we can simply run the MCMC and find the value of $\\lambda$ that yields the lowest value of WAIC. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nblasso1 <- bayeslasso(lambda = 0.1)\nblasso2 <- bayeslasso(lambda = 0.2)\nblasso3 <- bayeslasso(lambda = 1)\n2*length(diabetes$y)*c(attr(blasso1, \"waic\"), attr(blasso2, \"waic\"), attr(blasso3, \"waic\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4802.449 4801.923 4804.496\n```\n\n\n:::\n:::\n\n\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}