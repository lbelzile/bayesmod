[
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Bayesics",
    "section": "",
    "text": "Probability\nKey concepts: prior, posterior and interpretation\nPredictive distributions\nMarginal likelihood and numerical integration\nCredible intervals, loss functions and posterior summaries\nThe beta binomial conjugate model"
  },
  {
    "objectID": "content/01-content.html#learning-objectives",
    "href": "content/01-content.html#learning-objectives",
    "title": "Bayesics",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the chapter, students should be able to\n\ndefine key notions (posterior distribution, posterior predictive, marginal likelihood, credible intervals) of Bayesian inference\nperform inference in conjugate models with a single parameter\ncalculate and compute numerically summaries of posterior distributions (point estimators and credible intervals)\nexplain some of the conceptual differences between frequentist and Bayesian inference"
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "Bayesics",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nWarning\n\n\n\nThese readings should be completed before class, to ensure timely understanding and let us discuss the concepts together through various examples and case studies — the strict minimum being the course notes.\n\n\n\nChapter 1 of the course notes"
  },
  {
    "objectID": "content/01-content.html#complementary-readings",
    "href": "content/01-content.html#complementary-readings",
    "title": "Bayesics",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n\n\n\n\n\nWarning\n\n\n\nComplementary readings are additional sources of information that are not required readings, but may be useful substitutes. Sometimes, they go beyond the scope of what we cover and provide more details.\n\n\n\nEdwards et al. (1992)\nJohnson et al. (2022), chapters 2–4 and sections 8.1, 8.3\nVillani (2023), chapters 1 and 2\nAlbert (2009), chapters 2 and 3"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Bayesics",
    "section": "Slides",
    "text": "Slides\n\n  View all slides in new window   Download PDF of all slides"
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Priors",
    "section": "",
    "text": "Conjugate, flat, vague, Jeffrey’s, informative and penalized complexity priors\nPriors for regression models\nParameter elicitation\nPrior sensitivity analysis"
  },
  {
    "objectID": "content/02-content.html#learning-objectives",
    "href": "content/02-content.html#learning-objectives",
    "title": "Priors",
    "section": "Learning objectives",
    "text": "Learning objectives\nAt the end of the chapter, students should be able to\n\nchoose a suitable prior for a Bayesian analysis\nderive parameters of posterior distributions in conjugate models\nperform parameter elicitation\nassess the impact of priors through sensitivity analysis"
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "Priors",
    "section": "Readings",
    "text": "Readings\n\n\n\n\n\n\nWarning\n\n\n\nThese readings should be completed before class, to ensure timely understanding and let us discuss the concepts together through various examples and case studies — the strict minimum being the course notes.\n\n\n\nChapter 2 of the course notes"
  },
  {
    "objectID": "content/02-content.html#complementary-readings",
    "href": "content/02-content.html#complementary-readings",
    "title": "Priors",
    "section": "Complementary readings",
    "text": "Complementary readings\n\n\n\n\n\n\nWarning\n\n\n\nComplementary readings are additional sources of information that are not required readings, but may be useful substitutes. Sometimes, they go beyond the scope of what we cover and provide more details.\n\n\n\nVillani (2023), chapter 4\nGelman et al. (2013), chapters 2, 3 and 5\nJohnson et al. (2022), chapter 5\ntruncated Student-t priors for hierarchical models (Gelman, 2006)\npenalized complexity priors (Simpson et al., 2017)"
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "Priors",
    "section": "Slides",
    "text": "Slides\n\n  View all slides in new window   Download PDF of all slides"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings, lectures, and videos",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before watching the lecture."
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Examples",
    "section": "",
    "text": "This section will contain annotated R code along with worked out examples. Useful resources for learning R, the tidyverse and Rmarkdown basics include\n\nThe Introduction to R and RStudio by Open Intro Stat\nTeacups, giraffes & statistics: basic statistical concepts and programming\nthe notebook RYouWithMe from R-Ladies Sydney\nthe book R for Data Science, which adheres to the tidyverse principles.\nthe R package DoSStoolkit, developped at the University of Toronto.\nthe introverse documentation.\nthe RStudio cheatsheets, also available from RStudio menu in Help > Cheat Sheets\n\nTo install all R packages used throughout the course, use the command\n\ninstall.packages(\"cmdstanr\",\n                 repos = c(\"https://mc-stan.org/r-packages/\",\n                           getOption(\"repos\")))\n\npackages <- c(\"coda\", \"mvtnorm\", \"remotes\", \"loo\", \"dagitty\", \"shape\", \"rust\")\nfor(i in seq_along(packages)){\n  if(!packages[i] %in% installed.packages()[,\"Package\"]){\n install.packages(packages[i])\n  }\n}\nremotes::install_github(\"rmcelreath/rethinking\")\n\ninstall.packages(c(\"BiocManager\",\"remotes\"))\nBiocManager::install(\"graph\")\nBiocManager::install(\"Rgraphviz\")\ninstall.packages(\n  \"INLA\",\n  repos = c(getOption(\"repos\"),\n            INLA = \"https://inla.r-inla-download.org/R/stable\"),\n  dep = TRUE)"
  },
  {
    "objectID": "example/installation.html",
    "href": "example/installation.html",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Install R\nTo install the latest version of R itself (the engine), currently 4.3.1 (Beagle Scouts).\n\nGo to the Comprehensive R Archive Network (CRAN) website: https://cran.r-project.org/\nClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\n\n\n\n\n\nIf you use macOS, scroll down to the first .pkg file in the list of files and download it.\n\n\n\n\n\nIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\n\n\n\n\n\nDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\nInstall OS specific programs\n\n\nmacOS only: download and install XQuartz.\nWindows only: download and install Rtools\n\n\n\nInstall RStudio\nRStudio is an integrated development environment (IDE) for R. To install the latter, follow the instructions.\n\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\nThe website should automatically detect your operating system (Linux, macOS or Windows) and show a big download button for it:\n\n\n\n\n\n\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n\n\n\n\n\n\nDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\nDouble click on RStudio to run it (check your applications folder or start menu).\n\n\nInstall tidyverse\nR packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\n\n\n\n\n\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\n\n\n\n\n\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n\n\nInstall tinytex\nWhen you knit to PDF, R uses a special scientific typesetting program named LaTeX, which is complicated and a large download. To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nTo install tinytex:\n\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console.\nRun tinytex::install_tinytex() in the console.\nWait for a bit while R downloads and installs everything you need.\nYou should now be able to knit to PDF."
  },
  {
    "objectID": "exercises/01-exercise.html",
    "href": "exercises/01-exercise.html",
    "title": "Exercice 1",
    "section": "",
    "text": "You consider the waiting times between buses coming to HEC. Your bus line has frequent buses, but you decide to check the frequency. From your prior experience, you know that measured waiting times range between 3 and 15 minutes. You collect data over the two first week of classes and get an average of 8 minutes based on 10 observations.\nFor modelling, we consider data to arise an independent and identically distributed sample from an exponential distribution with rate \\(\\lambda>0\\), with associated density \\(f(x) = \\lambda \\exp(-\\lambda x)\\)."
  },
  {
    "objectID": "exercises/01-exercise.html#exercise-1.1",
    "href": "exercises/01-exercise.html#exercise-1.1",
    "title": "Exercice 1",
    "section": "Exercise 1.1",
    "text": "Exercise 1.1\nCompute the marginal likelihood when the prior for \\(\\lambda\\) is\n\nan exponential distribution with rate \\(\\kappa>0\\), with prior density \\(p(\\lambda) = \\kappa \\exp(-\\lambda\\kappa)\\);\nsame, but truncated above at 1, so the density is \\(p(\\lambda) = \\kappa \\exp(-\\lambda\\kappa)\\mathrm{I}\\{\\lambda \\leq 1\\}\\); the indicator function \\(\\mathrm{I}(\\cdot)\\) equals one if the condition is true and zero otherwise.\n\\(\\lambda \\sim \\mathsf{Ga}(\\alpha, \\beta)\\), a gamma distribution with shape \\(\\alpha>0\\) and rate \\(\\beta>0\\), with prior density \\(p(\\lambda) = \\beta^\\alpha\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)/\\Gamma(\\alpha)\\).\n\nDeduce that, in all cases above and up to truncation, the posterior distribution is a gamma distribution."
  },
  {
    "objectID": "exercises/01-exercise.html#exercise-1.2",
    "href": "exercises/01-exercise.html#exercise-1.2",
    "title": "Exercice 1",
    "section": "Exercise 1.2",
    "text": "Exercise 1.2\nConsider the following priors for the reciprocal expected waiting time, \\(\\lambda\\).\n\na gamma with rate \\(\\beta=100\\) and shape \\(\\alpha = 10\\)\nan exponential with rate \\(5\\)\na uniform prior on the unit interval \\([5,10]\\) for \\(1/\\lambda\\)\n\nAssociate each plot of the posterior with each of the priors. Which one seems most suitable for modelling?\nExplain what would happen to the posterior curve if we increased the sample size.\n\n\n\n\n\nFigure 1: Posteriors (full) and scaled likelihood (dashed) for exponential data, with different priors. The dashed vertical line indicates the maximum likelihood estimator."
  },
  {
    "objectID": "exercises/01-exercise.html#exercice-1.3",
    "href": "exercises/01-exercise.html#exercice-1.3",
    "title": "Exercice 1",
    "section": "Exercice 1.3",
    "text": "Exercice 1.3\nThe purpose of this exercise is to show that even one-dimensional numerical integration requires some care. Consider a half Cauchy prior over the positive half line, with density \\(f(x) = 2\\pi^{-1}(1+x^2)^{-1}\\mathrm{I}(x \\geq 0)\\). Using a grid approximation in increments of 0.01, evaluate the unnormalized posterior density from 0 until 30 minutes\\(^{-1}\\). Compare this approximation with that of quadrature methods via integrate and return the corresponding estimates of the normalizing constant. Indication: check the values and make sure the output is sensical. Modify the integrand if necessary.\nPlot the posterior distribution of \\(1/\\lambda\\), the average waiting time."
  },
  {
    "objectID": "exercises/01-exercise.html#exercice-1.4",
    "href": "exercises/01-exercise.html#exercice-1.4",
    "title": "Exercice 1",
    "section": "Exercice 1.4",
    "text": "Exercice 1.4\nConsider \\(\\lambda \\sim \\mathsf{Ga}(\\alpha = 1.4, \\beta=0.2)\\).\n\nReturn the posterior mean, posterior median and maximum a posteriori (MAP).\nReturn the 89% equitailed percentile credible interval.\nReturn the 50% highest posterior density (HPD) interval (harder).\n\nNext, draw 1000 observations from the posterior predictive distribution and plot a histogram of the latter. Compare it to the exponential distribution of observations.\nWould the posterior predictive change if you gathered more data? Justify your answer and explain how it would change, if at all."
  },
  {
    "objectID": "exercises/01-solution.html",
    "href": "exercises/01-solution.html",
    "title": "Solution 1",
    "section": "",
    "text": "You consider the waiting times between buses coming to HEC. Your bus line has frequent buses, but you decide to check the frequency. From your prior experience, you know that measured waiting times range between 3 and 15 minutes. You collect data over the two first week of classes and get an average of 8 minutes based on 10 observations.\nFor modelling, we consider data to arise an independent and identically distributed sample from an exponential distribution with rate \\(\\lambda>0\\), with associated density \\(f(x) = \\lambda \\exp(-\\lambda x)\\)."
  },
  {
    "objectID": "exercises/01-solution.html#exercise-1.1",
    "href": "exercises/01-solution.html#exercise-1.1",
    "title": "Solution 1",
    "section": "Exercise 1.1",
    "text": "Exercise 1.1\nCompute the marginal likelihood when the prior for \\(\\lambda\\) is\n\nan exponential distribution with rate \\(\\kappa>0\\), with prior density \\(p(\\lambda) = \\kappa \\exp(-\\lambda\\kappa)\\);\nsame, but truncated above at 1, so the density is \\(p(\\lambda) = \\kappa \\exp(-\\lambda\\kappa)\\mathrm{I}\\{\\lambda \\leq 1\\}\\); the indicator function \\(\\mathrm{I}(\\cdot)\\) equals one if the condition is true and zero otherwise.\n\\(\\lambda \\sim \\mathsf{Ga}(\\alpha, \\beta)\\), a gamma distribution with shape \\(\\alpha>0\\) and rate \\(\\beta>0\\), with prior density \\(p(\\lambda) = \\beta^\\alpha\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)/\\Gamma(\\alpha)\\).\n\nDeduce that, in all cases above and up to truncation, the posterior distribution is a gamma distribution.\n\nSolution. The likelihood is \\[\\begin{align*}\nL(\\lambda) = \\prod_{i=1}^n \\lambda \\exp(-\\lambda y_i) = \\lambda^n \\exp(-\\lambda n\\overline{y}),\n\\end{align*}\\] where \\(\\overline{y}=8\\) is the sample mean and \\(n=10\\) the number of observations.\nThe exponential distribution is a special case of the gamma, where \\(\\lambda \\sim \\mathsf{Exp}(\\kappa) \\equiv \\mathsf{Ga}(1, \\kappa)\\).\nThe posterior densities are equal, up to proportionality, to \\[\\begin{align*}\np_1(\\lambda \\mid y) &\\propto\\lambda^{n}\\exp\\{-(n\\overline{y}+\\kappa)\\lambda\\} \\\\\np_2(\\lambda \\mid y) &\\propto\\lambda^{n}\\exp\\{-(n\\overline{y}+\\kappa)\\lambda\\}\\mathrm{I}\\{\\lambda \\geq 1\\} \\\\\np_3(\\lambda \\mid y) &\\propto\\lambda^{n+\\alpha-1}\\exp\\{-(n\\overline{y}+\\beta)\\lambda\\}\n\\end{align*}\\] so these are \\(\\mathsf{Ga}(n+1, n\\overline{y} + \\kappa)\\), \\(\\mathsf{Ga}(n+1, n\\overline{y} + \\kappa)\\) truncated over \\([0,1]\\) and \\(\\mathsf{Ga}(n+\\alpha, n\\overline{y} + \\beta)\\).\nLet \\(\\Gamma(\\cdot)\\) denote the gamma function and \\(\\gamma(a, x)\\) the lower incomplete gamma function, \\(\\gamma(a, x= \\int_0^x t^{a-1}\\exp(-t) \\mathrm{d} t.\\) The marginal likelihood can be obtained either by integrating the product of the likelihood and prior, or upon noting that\n\\[\\begin{align*}\np_i(\\boldsymbol{y}) = \\frac{L(\\lambda) p_i(\\lambda)}{p_i(\\lambda \\mid \\boldsymbol{y})}\n\\end{align*}\\] so we get \\[\\begin{align*}\np_1(\\boldsymbol{y}) &= \\kappa n!(n\\overline{y}+\\kappa)^{-(n+1)},\\\\\np_2(\\boldsymbol{y}) &= p_1(\\boldsymbol{y}) \\times \\gamma\\{n+1, (n\\overline{y}+\\kappa)\\}, \\\\\np_3(\\boldsymbol{y}) &= \\frac{\\Gamma(n + \\alpha)\\beta^\\alpha}{\\Gamma(\\alpha)(n\\overline{y} + \\beta)^{n+\\alpha}}.\n\\end{align*}\\]"
  },
  {
    "objectID": "exercises/01-solution.html#exercise-1.2",
    "href": "exercises/01-solution.html#exercise-1.2",
    "title": "Solution 1",
    "section": "Exercise 1.2",
    "text": "Exercise 1.2\n\n\n\n\n\n\n\n\n\n\n\nConsider the following priors for the reciprocal expected waiting time, \\(\\lambda\\).\n\na gamma with rate \\(\\beta=100\\) and shape \\(\\alpha = 10\\)\nan exponential with rate \\(5\\)\na uniform prior on the unit interval \\([5,10]\\) for \\(1/\\lambda\\)\n\nAssociate each plot of the posterior with each of the priors. Which one seems most suitable for modelling?\nExplain what would happen to the posterior curve if we increased the sample size.\n\n\n\n\n\nFigure 1: Posteriors (full) and scaled likelihood (dashed) for exponential data, with different priors. The dashed vertical line indicates the maximum likelihood estimator.\n\n\n\n\n\nSolution. As the sample size increase, the curve becomes more concentrated around the maximum a posteriori value, which approaches the maximum likelihood estimate, \\(\\widehat{\\lambda}=1/8\\), regardless of the prior function (if it is untruncated).\nThe mode of a gamma distribution with rate \\(\\beta\\) and shape \\(\\alpha\\) is \\((\\alpha-1)/\\beta\\) for \\(\\alpha>1\\).\nThe truncated distribution (bottom left) is clearly identified from the “stubborn” prior, which is \\(p(\\lambda) \\propto \\mathrm{I}(1/10 < \\lambda < 1/5)/\\lambda^2\\), so 3C. Prior 1 has an average of 1/10 (less than the MLE) and provides nearly as much information as the data, so it is the more peaked posterior, so 1B. The exponential, or \\(\\mathsf{Ga}(1,5)\\), is much less informative than the likelihood and posterior should be only slightly different for the other, so 2A."
  },
  {
    "objectID": "exercises/01-solution.html#exercice-1.3",
    "href": "exercises/01-solution.html#exercice-1.3",
    "title": "Solution 1",
    "section": "Exercice 1.3",
    "text": "Exercice 1.3\nThe purpose of this exercise is to show that even one-dimensional numerical integration requires some care. Consider a half Cauchy prior over the positive half line, with density \\(f(x) = 2\\pi^{-1}(1+x^2)^{-1}\\mathrm{I}(x \\geq 0)\\). Using a grid approximation in increments of 0.01, evaluate the unnormalized posterior density from 0 until 30 minutes\\(^{-1}\\). Compare this approximation with that of quadrature methods via integrate and return the corresponding estimates of the normalizing constant. Indication: check the values and make sure the output is sensical. Modify the integrand if necessary.\nPlot the posterior distribution of \\(\\omega = 1/\\lambda\\), the average waiting time.\n\nSolution. For an unnormalized density \\(g(x)\\) such that \\(\\int g(x) \\mathrm{d} x = c\\), the normalizing constant is \\(1/c\\).\nFirst, remember that the density function of the scale \\(\\omega = \\lambda^{-1}\\) is related to the one of the rate \\(\\lambda\\) through a change of variable, with Jacobian \\(\\omega^{-2}\\).\n\n# Likelihood function - NOTE FORMULATION TO AVOID NUMERICAL OVERFLOW\nlik <- function(x, n = 10, ybar = 8){exp(n*(log(x)-ybar*x))}\n# Density of Cauchy over positive real line\ndfoldedCauchy <- function(x){2/pi/(1+x^2)}\n# Check integral\nintegrate(dfoldedCauchy, lower = 0, upper = Inf)\n\n1 with absolute error < 1.6e-10\n\n# Unormalized posterior\nunnorm_post <- function(x){dfoldedCauchy(x)*lik(x)}\n(icst_lambda <- integrate(f = unnorm_post, 0, 100))\n\n7.074775e-15 with absolute error < 1.4e-14\n\n# Check that the normalized value gives integral of 1\ncst_test <- integrate(f = function(x){unnorm_post(x)/icst_lambda$value}, \n                      lower = 0,\n                      upper = 100)\n\nOuch! The integral is nearly zero, and there is significant risk of numerical underflow and the reciprocal of the integrals are one order of magnitude apart, even when the sample size is not large. The normalizing constant, the reciprocal of the marginal likelihood, is not well estimated. The absolute error is of higher order than the estimate of the marginal likelihood, and this will be more problematic in higher dimensions.\nTo palliate to this, we could slightly modify the integrand by scaling the likelihood by \\((n\\overline{y} )^{n+1}/n!\\), the normalizing constant of the gamma integral, which is around \\(2.4 \\times 10^{14}\\). This fixes the issue below.\n\n# Scale likelihood by using gamma distribution normalizing constant\nslik <- function(x, n = 10, ybar = 8){\n  exp(-lgamma(n+1) + (n+1)*log(n*ybar) + n*(log(x)-ybar*x))\n}\n# Unnormalized posterior, version 2\nunnorm_post2 <- function(x){dfoldedCauchy(x)*slik(x)}\n# Define grid and evaluate product\ngrid <- seq(from = 0, to = 30, by = 0.01)\nmarglik_grid <- sum(unnorm_post2(grid))*0.01\n# Is the grid approximation good enough?\n# check equality to numerical tolerance\nall.equal(integrate(f = unnorm_post2, 0, 100)$value, marglik_grid)\n\n[1] TRUE\n\nnorm_cst <- 1/marglik_grid\n\nThe prior normalizing by a known constant fixes the numerical issues. Generally, one would tend to build an approximation only at the mode and match the curvature of the posterior there (using, e.g., a quadratic approximation to the posterior density).\nFor the posterior density, we can simply use the approximation with \\(p(\\lambda \\mid \\boldsymbol{y}) \\approx \\widehat{c} L(\\lambda)p(\\lambda)\\). The posterior density for the reciprocal rate is \\(p(\\omega \\mid \\boldsymbol{y}) \\approx \\widehat{c} L(\\omega^{-1})p(\\omega^{-1})\\omega^{-2}\\) (don’t forget the Jacobian)!\n\n\nCode\nlibrary(ggplot2)\nggplot() + \n  stat_function(fun = function(x){unnorm_post2(1/x)*norm_cst/x^2},\n                xlim = c(0, 30),\n                n = 1001) +\n  labs(y = \"\", \n       x = expression(paste(\"average waiting time \", 1/lambda, \" (in minutes)\")),\n       subtitle = \"posterior density\") + \n  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, .1))) +\n  theme_classic()"
  },
  {
    "objectID": "exercises/01-solution.html#exercice-1.4",
    "href": "exercises/01-solution.html#exercice-1.4",
    "title": "Solution 1",
    "section": "Exercice 1.4",
    "text": "Exercice 1.4\nConsider \\(\\lambda \\sim \\mathsf{Ga}(\\alpha = 1.4, \\beta=0.2)\\).\n\nReturn the posterior mean, posterior median and maximum a posteriori (MAP).\nReturn the 89% equitailed percentile credible interval.\nReturn the 50% highest posterior density (HPD) interval (harder).\n\nNext, draw 1000 observations from the posterior predictive distribution and plot a histogram of the latter. Compare it to the exponential distribution of observations.\nWould the posterior predictive change if you gathered more data? Justify your answer and explain how it would change, if at all.\n\nSolution. We have already determined that the posterior is a Gamma distribution \\(\\mathsf{Ga}(a, b)\\), where \\(a= \\alpha+ n= 11.4\\) and \\(b=n\\overline{y} + \\beta = 80.2\\). We can query the moments directly: the mode is \\((a-1)/b\\) and the mean \\(a/b\\), while the median must be found numerically using the quantile function.\n\na <- 11.4 # shape parameter\nb <- 80.2 # rate parameter\nalpha <- 0.11 # level\npostmean <- a/b # posterior mean\npostmed <- qgamma(p = 0.5, shape = a, rate = b) # median\npostmode <- (a-1)/b # mode\ncredint <- qgamma(p = c(alpha/2, 1-alpha/2), shape = a, rate = b)\n# Documentation states you must return the quantile function\nHDInterval::hdi(\n    object = qgamma, \n    shape = a, \n    rate = b, \n    credMass = 0.50)\n\n    lower     upper \n0.1042228 0.1589696 \nattr(,\"credMass\")\n[1] 0.5\n\n# Alternatively, find this numerically\n# MAP via minimization of the negative log likelihood\nMAP <- optimize(\n  f = function(x){-dgamma(x, shape = a, rate = b, log = TRUE)},\n  interval = qgamma(p = c(0.2,0.8), shape = a, rate = b)\n  )$minimum\n# Other quantities approximated by Monte Carlo\n# Sample from posterior\npost_samp <- rgamma(n = 1e4, shape = a, rate = b)\n# Sample mean and sample quantiles\nmean(post_samp)\n\n[1] 0.1422132\n\nquantile(post_samp, c(alpha/2, 0.5, 1-alpha/2))\n\n      5.5%        50%      94.5% \n0.08286817 0.13834515 0.21530067 \n\n# Highest posterior density interval\ncredHDI <- HDInterval::hdi(\n  object = post_samp, \n  credMass = 0.50)\ncredHDI\n\n    lower     upper \n0.1061453 0.1597930 \nattr(,\"credMass\")\n[1] 0.5\n\n\nUp to three significant digits, the posterior mean is 0.142, the median is 0.138 and the mode 0.13. The bounds of the equitailed 89% credible interval are [0.082, 0.215], those of the 50% HDPI [0.1061453, 0.159793].\nFor the posterior predictive, we simply take the samples from the posterior, post_samp, and generate for each rate a new exponential\n\n\nCode\npostpred_samp <- rexp(n = 1000, rate = post_samp[seq_len(1000)])\nggplot(data = data.frame(x = postpred_samp))+\n  geom_histogram(aes(x = x, y = ..density..)) + \n  # the likelihood is proportional to a gamma\n  stat_function(fun = dexp,  \n                n = 1001, \n                args = list(rate = 1/8) # mle is average\n                ) +\n  labs(x = \"waiting time (in minutes)\", \n       y = \"\") +\n  scale_x_continuous(limits = c(0, NA), expand = expansion(mult = c(0, .1))) +\n  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, .1))) +\n  theme_classic()\n\n\n\n\n\nFigure 2: Histogram of posterior predictive draws and density function of observations."
  },
  {
    "objectID": "exercises/02-exercise.html",
    "href": "exercises/02-exercise.html",
    "title": "Exercice 2",
    "section": "",
    "text": "Consider a simple random sample of size \\(n\\) from the Wald distribution, with density \\[\\begin{align*}\nf(y; \\nu, \\lambda) = \\left(\\frac{\\lambda}{2\\pi y^{3}}\\right)^{1/2} \\exp\\left\\{ - \\frac{\\lambda (y-\\nu)^2}{2\\nu^2y}\\right\\}\\mathrm{I}(y > 0)\n\\end{align*}\\] for location \\(\\nu >0\\) and shape \\(\\tau>0\\).\n\nWrite down the likelihood and show that it can be written in terms of the sufficient statistics \\(\\sum_{i=1}^n y_i\\) and \\(\\sum_{i=1} y_i^{-1}\\).\nShow that the joint prior \\[ p(\\lambda) \\sim \\mathsf{Ga}(\\alpha, \\beta), \\quad p(1/\\nu \\mid \\lambda) \\sim  \\mathsf{No}(\\mu, \\tau^{-1}\\lambda^{-1}),\\] the product of a gamma and a reciprocal Gaussian, is conjugate for the Wald distribution parameters.\nDerive the parameters of the posterior distribution and provide an interpretation of the prior parameters. Hint: write down the posterior parameters as a weighted average of data-dependent quantities and prior parameters.\nDerive the marginal posterior \\(p(\\lambda)\\)."
  },
  {
    "objectID": "exercises/02-exercise.html#exercise-2.2",
    "href": "exercises/02-exercise.html#exercise-2.2",
    "title": "Exercice 2",
    "section": "Exercise 2.2",
    "text": "Exercise 2.2\nConsider the Rayleigh distribution with scale \\(\\sigma>0\\). It’s density is \\[f(y; \\sigma) = \\frac{y}{\\sigma^2} \\exp\\left(-\\frac{y^2}{2\\sigma^2}\\right)\\mathrm{I}(x \\geq 0).\\]\nDerive the Fisher information matrix and use it to obtain Jeffrey’s prior for \\(\\sigma\\). Determine whether the prior is proper."
  },
  {
    "objectID": "exercises/02-exercise.html#exercise-2.3",
    "href": "exercises/02-exercise.html#exercise-2.3",
    "title": "Exercice 2",
    "section": "Exercise 2.3",
    "text": "Exercise 2.3\nConsider a binomial model with an unknown probability of successes \\(\\theta \\in [0,1]\\) model. Suppose your prior guess for \\(\\theta\\) has mean \\(0.1\\) and standard deviation \\(0.2\\)\nUsing moment matching, return values for the parameters of the conjugate beta prior corresponding to your opinion.\nPlot the resulting beta prior and compare it with a truncated Gaussian distribution on the unit interval with location \\(\\mu=0.1\\) and scale \\(\\sigma=0.2\\).^{Note that the parameters of the truncated Gaussian distribution do not correspond to moments!}"
  },
  {
    "objectID": "exercises/02-exercise.html#exercise-2.4",
    "href": "exercises/02-exercise.html#exercise-2.4",
    "title": "Exercice 2",
    "section": "Exercise 2.4",
    "text": "Exercise 2.4\nReplicate the analysis of Example 2.6 (Should you phrase your headline as a question?) of the course notes using the upworthy_question data from the hecbayes package."
  },
  {
    "objectID": "exercises/02-solution.html",
    "href": "exercises/02-solution.html",
    "title": "Exercice 2",
    "section": "",
    "text": "Consider a simple random sample of size \\(n\\) from the Wald distribution, with density \\[\\begin{align*}\nf(y; \\nu, \\lambda) = \\left(\\frac{\\lambda}{2\\pi y^{3}}\\right)^{1/2} \\exp\\left\\{ - \\frac{\\lambda (y-\\nu)^2}{2\\nu^2y}\\right\\}\\mathrm{I}(y > 0)\n\\end{align*}\\] for location \\(\\nu >0\\) and shape \\(\\tau>0\\).\n\nWrite down the likelihood and show the posterior only depends on the data through the sufficient statistics \\(\\sum_{i=1}^n y_i\\) and \\(\\sum_{i=1} y_i^{-1}\\).\nShow that the joint prior \\[ p(\\lambda) \\sim \\mathsf{Ga}(\\alpha, \\beta), \\quad p(1/\\nu \\mid \\lambda) \\sim  \\mathsf{No}(\\mu, \\tau^{-1}\\lambda^{-1}),\\] the product of a gamma and a reciprocal Gaussian, is conjugate for the Wald distribution parameters.\nDerive the parameters of the posterior distribution and provide an interpretation of the prior parameters. Hint: write down the posterior parameters as a weighted average of data-dependent quantities and prior parameters.\nDerive the marginal posterior \\(p(\\lambda)\\).\n\n\nSolution. The log likelihood for an independent and identically distributed sample is \\[\\begin{align*}\n\\ell(\\nu, \\lambda) = \\frac{n}{2} \\ln(\\lambda) - \\frac{3}{2} \\sum_{i=1}^n \\ln(y_i) - \\frac{\\lambda}{2\\nu^2} \\sum_{i=1}^n (y_i - 2\\nu + \\nu^2/y_i)\n\\end{align*}\\] and we readily see that the model is an exponential family with sufficient statistics \\(t_1(\\boldsymbol{y}) = \\sum_{i=1}^n y_i\\) and \\(t_2(\\boldsymbol{y}) = \\sum_{i=1}^n y_i^{-1}\\).\nThe joint prior is of the form \\[\\begin{align*}\np(\\lambda, \\nu) &\\propto \\lambda^{\\alpha-1}\\exp(-\\lambda \\beta) \\\\\np(\\nu \\mid \\lambda) & \\propto \\frac{(\\lambda \\tau)^{1/2}}{\\nu^2}\\exp\\left\\{-\\frac{\\lambda\\tau }{2}(\\nu^{-1}-\\mu)^2\\right\\},\n\\end{align*}\\] where the last step follows from a change of variable. To show conjugacy, we must prove that the posterior is of the same family. Multiplying the likelihood with the joint prior and expanding the squares in the exponential terms, we get \\[\\begin{align*}\np(\\lambda, \\nu \\mid \\boldsymbol{y}) &\\propto \\frac{\\lambda^{(n+1)/2+\\alpha-1}}{\\nu^2}\\exp(-\\lambda \\beta) \\exp\\left[ -\\frac{\\lambda}{2}\\left\\{\\frac{\\tau}{\\nu^{2}}-2\\frac{\\tau\\mu}{\\nu} + \\tau\\mu^2 +\\frac{t_1(\\boldsymbol{y})}{\\nu^2} -  \\frac{2n}{\\nu} + t_2(\\boldsymbol{y})\\right\\}\\right] \\\\& \\propto \\frac{\\lambda^{(n+1)/2+\\alpha-1}}{\\nu^2}\\exp(-\\lambda \\beta) \\exp\\left[ -\\frac{\\lambda}{2}\\left\\{\\frac{\\tau + t_1(\\boldsymbol{y})}{\\nu^2} - 2\\frac{\\tau\\mu+n}{\\nu} + \\tau\\mu^2 + t_2(\\boldsymbol{y})\\right\\} \\right]\n\\\\& \\propto \\frac{\\lambda^{(n+1)/2+\\alpha-1}}{\\nu^2}\\exp\\left[-\\lambda \\left\\{\\beta + \\frac{c_3-c_1c_2^2}{2}\\right\\}\\right] \\exp\\left\\{ -\\frac{\\lambda c_1}{2}\\left(\\frac{1}{\\nu^2} - \\frac{2c_2}{\\nu} + c_2^2\\right)\\right\\}\n\\end{align*}\\] where \\(c_1=\\{\\tau + t_1(\\boldsymbol{y})\\}\\), \\(c_2 =(\\tau\\mu+n)/c_1\\) and \\(c_3 =\\tau\\mu^2 + t_2(\\boldsymbol{y})\\)."
  },
  {
    "objectID": "exercises/02-solution.html#exercise-2.2",
    "href": "exercises/02-solution.html#exercise-2.2",
    "title": "Exercice 2",
    "section": "Exercise 2.2",
    "text": "Exercise 2.2\nConsider the Rayleigh distribution with scale \\(\\sigma>0\\). It’s density is \\[f(y; \\sigma) = \\frac{y}{\\sigma^2} \\exp\\left(-\\frac{y^2}{2\\sigma^2}\\right)\\mathrm{I}(x \\geq 0).\\]\nDerive the Fisher information matrix and use it to obtain Jeffrey’s prior for \\(\\sigma\\). Determine whether the prior is proper.\n\nSolution. The log likelihood for a sample of size one is \\[\\begin{align*}\n\\ell(\\sigma) = \\log(y) - 2\\log(\\sigma) - \\frac{y^2}{2\\sigma^2}\n\\end{align*}\\] and the negative of the Hessian is \\[\\begin{align*}\n\\jmath(\\sigma) = -\\frac{\\partial \\ell(\\sigma)}{\\partial \\sigma} = -\\frac{2}{\\sigma^2} + \\frac{3y^2}{\\sigma^4}\n\\end{align*}\\] To compute the Fisher information, we need the second moment of the Rayleigh distribution, \\[\\begin{align*}\n\\mathsf{E}(Y^2) &= \\int_0^\\infty \\frac{y^3}{\\sigma^2}\\exp\\left(-\\frac{y^2}{2\\sigma^2}\\right)\\mathrm{d} y\\\\&= 2\\sigma^2 \\int_0^\\infty u \\exp(-u) \\mathrm{d} u\n\\\\&= 2\\sigma^2\n\\end{align*}\\] where we made the change of variable \\(u= 0.5y^2\\sigma^{-2}\\), \\(\\mathrm{d} u = y\\sigma^{-2}\\mathrm{d} y\\) and recovered the expected value of a unit exponential distribution.\nThe Fisher information is \\(\\imath(\\sigma) = \\mathsf{E}\\{\\jmath(\\sigma)\\}=-2/\\sigma^2 + 6/\\sigma^2 = 3/\\sigma^2\\). Jeffrey’s prior for the scale, \\(p(\\sigma) =|\\imath(\\sigma)|^{1/2}\\), is proportional to \\(\\sigma^{-1}\\) and thus improper."
  },
  {
    "objectID": "exercises/02-solution.html#exercise-2.3",
    "href": "exercises/02-solution.html#exercise-2.3",
    "title": "Exercice 2",
    "section": "Exercise 2.3",
    "text": "Exercise 2.3\nConsider a binomial model with an unknown probability of successes \\(\\theta \\in [0,1]\\) model. Suppose your prior guess for \\(\\theta\\) has mean \\(0.1\\) and standard deviation \\(0.2\\)\nUsing moment matching, return values for the parameters of the conjugate beta prior corresponding to your opinion.\nPlot the resulting beta prior and compare it with a truncated Gaussian distribution on the unit interval with location \\(\\mu=0.1\\) and scale \\(\\sigma=0.2\\).^{Note that the parameters of the truncated Gaussian distribution do not correspond to moments!}\n\nSolution. The beta distribution has expected value \\(\\alpha/(\\alpha+\\beta)\\) and variance \\(\\alpha\\beta(\\alpha+\\beta)^{-2}(\\alpha+\\beta+1)^{-1}\\).\nSince the system of equations is nonlinear, we need to solve numerically to find the two unknown value of the parameters or else simply substitute \\(\\beta = \\alpha (1-\\mu)/\\mu\\) in the equation for the variance.\n\nbeta_moments <- function(par, mean, variance){\n  alpha <- par[1]\n  beta <- par[2]\n  c(alpha/(alpha+beta) - mean,\n    alpha*beta/(alpha+beta)^2/(alpha+beta+1) - variance)\n}\n# Numerical root finding algorithm\n# We need to give good starting values\nrootSolve::multiroot(f = beta_moments,\n                     start = c(0.1, 0.8),\n                     positive = TRUE,\n                     mean = 0.1, \n                     variance = 0.04)\n\n$root\n[1] 0.125 1.125\n\n$f.root\n[1] 5.504125e-12 4.298423e-12\n\n$iter\n[1] 5\n\n$estim.precis\n[1] 4.901274e-12\n\nalpha <- uniroot(f = function(alpha, mean = 0.1, variance = 0.04){\n  beta <- alpha*(1-mean)/mean\n  alpha*beta/(alpha+beta)^2/(alpha+beta+1) - variance}, \n  interval = c(1e-4, 1))$root\nbeta <- alpha*(1-0.1)/0.1\nc(alpha = alpha, beta = beta)\n\nalpha  beta \n0.125 1.125 \n\n\nand we find \\(\\alpha = 0.125\\) and \\(\\beta=1.125\\).\nLet \\(a = -\\mu/\\sigma\\) and \\(b = (1-\\mu)/\\sigma\\) denote the standardized lower and upper bounds, respectively. The density of the truncated Gaussian with location \\(\\mu\\) and scale \\(\\sigma\\) on the unit interval is \\[\\begin{align*}\nf(x; \\mu, \\sigma) = \\frac{1}{\\sigma}\\frac{\\phi\\left(\\frac{x - \\mu}{\\sigma}\\right)}{\\Phi(b) - \\Phi(a)},\n\\end{align*}\\] where \\(\\phi\\) is the density of a standard Gaussian \\(\\mathsf{No}(0,1)\\) and \\(\\Phi\\) the corresponding distribution function.\n\n\n\n\n\n\nFigure 1: Beta (full line) and truncated Gaussian (dashed line) prior densities.\n\n\n\n\nThe beta distribution, while matching the moments, is implying very low chance of success with a mode at zero. By contrast, the truncated Gaussian (which does not have mean \\(\\mu\\) and variance \\(\\sigma^2\\)) has the mode at \\(\\mu=0.1\\). Which one is preferable depends on the context; we could also match the parameters of the truncated Gaussian."
  },
  {
    "objectID": "exercises/02-solution.html#exercise-2.4",
    "href": "exercises/02-solution.html#exercise-2.4",
    "title": "Exercice 2",
    "section": "Exercise 2.4",
    "text": "Exercise 2.4\nReplicate the analysis of Example 2.6 (Should you phrase your headline as a question?) of the course notes using the upworthy_question data from the hecbayes package.\n\n\nSolution. \ndata(upworthy_question, package = \"hecbayes\")\n# Prior parameters\nalpha <- 2.5 # shape\nbeta <- 0.04 # rate\n\n# Pool data from all questions (only total counts matter)\nsummary_stats <- upworthy_question |>\n  dplyr::group_by(question) |>\n  dplyr::summarize(\n    total_impressions = sum(impressions),\n    total_clicks = sum(clicks)) |>\n  dplyr::ungroup() |>\n  as.vector()\n# Extract total number of successes out of trials\n# Impressions here serve as count\nn_yes <- summary_stats$total_impressions[1]\ny_yes <- summary_stats$total_clicks[1]\nn_no <-  summary_stats$total_impressions[2]\ny_no <- summary_stats$total_clicks[2]\n# Generate posterior sample draws\n# Since likelihood and priors are independent, \n# so are the posteriors for the rates. \n# We can draw independently from each.\nset.seed(1234)\npost_data_upworthy_question <- \n  data.frame( \n  yes = rgamma(n = 1e4, shape = alpha + y_yes, rate = beta + n_yes),\n  no = rgamma(n = 1e4,  shape = alpha + y_no, rate = beta + n_no))\n# Plot the histogram of the ratio of rates (question / no question)\nggplot(data = post_data_upworthy_question,\n  mapping = aes(x = yes/no)) +\n  geom_histogram() +\n  labs(x = \"rate ratio\", y = \"\",\n       caption = expression(lambda[\"yes\"]/lambda[\"no\"])) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_classic()\n\n\n\n# Compute the posterior mean for the quantity\npost_mean <- with(post_data_upworthy_question, mean(yes/no))\npost_mean\n\n[1] 0.9317745"
  },
  {
    "objectID": "exercises/03-exercise.html",
    "href": "exercises/03-exercise.html",
    "title": "Exercice 3",
    "section": "",
    "text": "We revisit Exercise 1.3, which used a half-Cauchy prior for the exponential waiting time of buses.\nThe ratio-of-uniform method, implemented in the rust R package, can be used to simulate independent draws from the posterior of the rate \\(\\lambda\\). The following code produces\n\nnobs <- 10L # number of observations\nybar <- 8   # average waiting time\nB <- 1000L  # number of draws\n# Un-normalized log posterior: scaled log likelihood + log prior\nupost <- function(x){ \n  dgamma(x = x, shape = nobs + 1L, rate = nobs*ybar, log = TRUE) +\n    log(2) + dt(x = x, df = 1, log = TRUE)}\npost_samp <- rust::ru(logf = upost, \n                      n = B, \n                      d = 1,  # dimension of parameter (scalar)\n                      init = nobs/ybar) # initial value of mode\n\nEstimate using the Monte Carlo sample:\n\nthe probability that the waiting time is between 3 and 15 minutes\nthe average waiting time\nthe standard deviation of the waiting time\n\nNext, implement a random walk Metropolis–Hastings algorithm to sample draws from the posterior. Estimate the same quantities and assess the variability of the Monte Carlo estimators for both correlated draws and the independent draws."
  },
  {
    "objectID": "exercises/index.html",
    "href": "exercises/index.html",
    "title": "Exercises",
    "section": "",
    "text": "This section will contain exercises that are directly connected to the course notes, in addition to other exercises from reference manuals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Special topics in data science --- Bayesian modelling",
    "section": "",
    "text": "Special topics in data science — Bayesian modelling\n        \n        \n            Hands on introduction to Bayesian data analysis. The course will cover the formulation, evaluation and comparison of Bayesian models through examples.\n        \n        \n            MATH 80636A, Fall 2023HEC Montréal\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nInstructor\n\n   Dr. Léo Belzile\n   4.850, Côte-Sainte-Catherine\n   leo.belzile@hec.ca\n\n\n\nCourse details\n\n   Fall 2023\n   Tuesday\n   8:30-11:30\n   Decelles, Rivière-du-Loup\n\n\n\nContacting me\nI am best reached by email."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#probability-vs-frequency",
    "href": "slides/bayesmod-slides1.html#probability-vs-frequency",
    "title": "Bayesian modelling",
    "section": "Probability vs frequency",
    "text": "Probability vs frequency\nIn frequentist statistic, “probability” is synonym for\n\n\n\nlong-term frequency under repeated sampling"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#what-is-probability",
    "href": "slides/bayesmod-slides1.html#what-is-probability",
    "title": "Bayesian modelling",
    "section": "What is probability?",
    "text": "What is probability?\nProbability reflects incomplete information.\nQuoting Finetti (1974)\n\nProbabilistic reasoning — always to be understood as subjective — merely stems from our being uncertain about something."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#why-opt-for-the-bayesian-paradigm",
    "href": "slides/bayesmod-slides1.html#why-opt-for-the-bayesian-paradigm",
    "title": "Bayesian modelling",
    "section": "Why opt for the Bayesian paradigm?",
    "text": "Why opt for the Bayesian paradigm?\n\nSatisfies the likelihood principle\nGenerative approach naturally extends to complex settings (hierarchical models)\nUncertainty quantification and natural framework for prediction\nCapability to incorporate subject-matter expertise"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#bayesian-versus-frequentist",
    "href": "slides/bayesmod-slides1.html#bayesian-versus-frequentist",
    "title": "Bayesian modelling",
    "section": "Bayesian versus frequentist",
    "text": "Bayesian versus frequentist\n\n\nFrequentist\n\nParameters treated as fixed, data as random\n\ntrue value of parameter \\(\\boldsymbol{\\theta}\\) is unknown.\n\nTarget is point estimator\n\n\nBayesian\n\nBoth parameters and data are random\n\ninference is conditional on observed data\n\nTarget is a distribution"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#joint-and-marginal-distribution",
    "href": "slides/bayesmod-slides1.html#joint-and-marginal-distribution",
    "title": "Bayesian modelling",
    "section": "Joint and marginal distribution",
    "text": "Joint and marginal distribution\nThe joint density of data \\(\\boldsymbol{Y}\\) and parameters \\(\\boldsymbol{\\theta}\\) is\n\\[\\begin{align*}\np(\\boldsymbol{Y}, \\boldsymbol{\\theta}) = p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) =  p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y}) p(\\boldsymbol{Y})\n\\end{align*}\\] where the marginal \\(p(\\boldsymbol{Y}) = \\int_{\\boldsymbol{\\Theta}} p(\\boldsymbol{Y}, \\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\\)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#posterior",
    "href": "slides/bayesmod-slides1.html#posterior",
    "title": "Bayesian modelling",
    "section": "Posterior",
    "text": "Posterior\nUsing Bayes’ theorem, the posterior density is\n\\[\\begin{align*}\n\\color{#D55E00}{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})} = \\frac{\\color{#0072B2}{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})} \\times  \\color{#56B4E9}{p(\\boldsymbol{\\theta})}}{\\color{#E69F00}{\\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\\mathrm{d} \\boldsymbol{\\theta}}},\n\\end{align*}\\]\nmeaning that \\[\\color{#D55E00}{\\text{posterior}} \\propto \\color{#0072B2}{\\text{likelihood}} \\times \\color{#56B4E9}{\\text{prior}}\\]\n\n\n\nEvaluating the marginal likelihood \\(\\color{#E69F00}{p(\\boldsymbol{Y})}\\), is challenging when \\(\\boldsymbol{\\theta}\\) is high-dimensional."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#updating-beliefs-and-sequentiality",
    "href": "slides/bayesmod-slides1.html#updating-beliefs-and-sequentiality",
    "title": "Bayesian modelling",
    "section": "Updating beliefs and sequentiality",
    "text": "Updating beliefs and sequentiality\nBy Bayes’ rule, we can consider updating the posterior by adding terms to the likelihood, noting that for independent \\(\\boldsymbol{y}_1\\) and \\(\\boldsymbol{y}_2\\), \\[\\begin{align*}\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\boldsymbol{y}_2) \\propto p(\\boldsymbol{y}_2 \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1)\n\\end{align*}\\] The posterior is be updated in light of new information."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#binomial-distribution",
    "href": "slides/bayesmod-slides1.html#binomial-distribution",
    "title": "Bayesian modelling",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nA binomial variable with probability of success \\(\\theta \\in [0,1]\\) has mass function \\[\\begin{align*}\nf(y; \\theta) = \\binom{n}{y} \\theta^y (1-\\theta)^{n-y}, \\qquad y = 0, \\ldots, n.\n\\end{align*}\\] Moments of the number of successes out of \\(n\\) trials are \\[\\mathsf{E}(Y \\mid \\theta) = n \\theta, \\quad \\mathsf{Va}(Y \\mid \\theta) = n \\theta(1-\\theta).\\]\n\n\nThe binomial coefficient \\(\\binom{n}{y}=n!/\\{(n-y)!y!\\}\\), where \\(n!=\\Gamma(n+1)\\)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#beta-distribution",
    "href": "slides/bayesmod-slides1.html#beta-distribution",
    "title": "Bayesian modelling",
    "section": "Beta distribution",
    "text": "Beta distribution\nThe beta distribution with shapes \\(\\alpha>0\\) and \\(\\beta>0\\), denoted \\(\\mathsf{Be}(\\alpha,\\beta)\\), has density \\[f(y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}y^{\\alpha - 1}(1-y)^{\\beta - 1}, \\qquad y \\in [0,1]\\]\n\nexpectation: \\(\\alpha/(\\alpha+\\beta)\\);\nmode \\((\\alpha-1)/(\\alpha+\\beta-2)\\) if \\(\\alpha, \\beta>1\\), else, \\(0\\), \\(1\\) or none;\nvariance: \\(\\alpha\\beta/\\{(\\alpha+\\beta)^2(\\alpha+\\beta+1)\\}\\).\n\n\nIt is a continuous distribution over the unit interval.\nThe uniform is a special case when both shapes are unity."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#beta-binomial-example",
    "href": "slides/bayesmod-slides1.html#beta-binomial-example",
    "title": "Bayesian modelling",
    "section": "Beta-binomial example",
    "text": "Beta-binomial example\nWe write \\(Y \\sim \\mathsf{Bin}(n, \\theta)\\) for \\(\\theta \\in [0,1]\\); the likelihood is \\[L(\\theta; y) = \\binom{n}{y} \\theta^y(1-\\theta)^{n-y}.\\]\nConsider a beta prior, \\(\\theta \\sim \\mathsf{Be}(\\alpha, \\beta)\\), with density \\[\np(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta) }\\theta^{\\alpha-1}(1-\\theta)^{\\beta - 1}.\n\\]"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#density-versus-likelihood",
    "href": "slides/bayesmod-slides1.html#density-versus-likelihood",
    "title": "Bayesian modelling",
    "section": "Density versus likelihood",
    "text": "Density versus likelihood\nThe binomial distribution is discrete with support \\(0, \\ldots, n\\), whereas the likelihood is continuous over \\(\\theta \\in [0,1]\\).\n\n\n\n\n\nFigure 1: Binomial density function (left) and scaled likelihood function (right).\n\n\n\n\n\n\nIf the density or mass function integrates to 1 over the range of \\(Y\\), the integral of the likelihood over \\(\\theta\\) does not."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#proportionality",
    "href": "slides/bayesmod-slides1.html#proportionality",
    "title": "Bayesian modelling",
    "section": "Proportionality",
    "text": "Proportionality\nAny term not a function of \\(\\theta\\) can be dropped, since it will absorbed by the normalizing constant. The posterior density is proportional to\n\\[\\begin{align*}\nL(\\theta; y)p(\\theta) & \\stackrel{\\theta}{\\propto} \\theta^{y}(1-\\theta)^{n-y} \\times \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}\n\\\\& =\\theta^{y + \\alpha - 1}(1-\\theta)^{n-y + \\beta - 1}\n\\end{align*}\\] the kernel of a beta density with shape parameters \\(y + \\alpha\\) and \\(n-y + \\beta\\).\n\n\nThe symbol \\(\\propto\\), for proportionality, means dropping all terms not an argument of the left hand side."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#experiments-and-likelihoods",
    "href": "slides/bayesmod-slides1.html#experiments-and-likelihoods",
    "title": "Bayesian modelling",
    "section": "Experiments and likelihoods",
    "text": "Experiments and likelihoods\nConsider the following sampling mechanism, which lead to \\(k\\) successes out of \\(n\\) independent trials, with the same probability of success \\(\\theta\\).\n\nBernoulli: sample fixed number of observations with \\(L(\\theta; y) =\\theta^k(1-\\theta)^{n-k}\\)\nbinomial: same, but record only total number of successes so \\(L(\\theta; y) =\\binom{n}{k}\\theta^k(1-\\theta)^{n-k}\\)\nnegative binomial: sample data until you obtain a predetermined number of successes, whence \\(L(\\theta; y) =\\binom{n-1}{k-1}\\theta^k(1-\\theta)^{n-k}\\)"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#likelihood-principle",
    "href": "slides/bayesmod-slides1.html#likelihood-principle",
    "title": "Bayesian modelling",
    "section": "Likelihood principle",
    "text": "Likelihood principle\nTwo likelihoods that are proportional, up to a constant not depending on unknown parameters, yield the same evidence.\nIn all cases, \\(L(\\theta; y) \\stackrel{\\theta}{\\propto} \\theta^k(1-\\theta)^{n-k}\\), so these yield the same inference for Bayesian.\n\n\nFor a more in-depth discussion, see Section 6.3.2 of Casella & Berger (2002)"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#integration",
    "href": "slides/bayesmod-slides1.html#integration",
    "title": "Bayesian modelling",
    "section": "Integration",
    "text": "Integration\nWe could approximate the \\(\\color{#E69F00}{\\text{marginal likelihood}}\\) through either\n\nnumerical integration (cubature)\nMonte Carlo simulations\n\nIn more complicated models, we will try to sample observations by bypassing completely this calculation.\n\n\nThe likelihood terms can be small (always less than one and decreasing for discrete data), so watch out for numerical overflow when evaluating normalizing constants."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#numerical-example-of-monte-carlo-integration",
    "href": "slides/bayesmod-slides1.html#numerical-example-of-monte-carlo-integration",
    "title": "Bayesian modelling",
    "section": "Numerical example of (Monte Carlo) integration",
    "text": "Numerical example of (Monte Carlo) integration\n\ny <- 6L # number of successes \nn <- 14L # number of trials\nalpha <- beta <- 1.5 # prior parameters\nunnormalized_posterior <- function(theta){\n  theta^(y+alpha-1) * (1-theta)^(n-y + beta - 1)\n}\nintegrate(f = unnormalized_posterior,\n          lower = 0,\n          upper = 1)\n\n1.066906e-05 with absolute error < 1e-12\n\n# Compare with known constant\nbeta(y + alpha, n - y + beta)\n\n[1] 1.066906e-05\n\n# Monte Carlo integration\nmean(unnormalized_posterior(runif(1e5)))\n\n[1] 1.064055e-05"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#prior-likelihood-and-posterior",
    "href": "slides/bayesmod-slides1.html#prior-likelihood-and-posterior",
    "title": "Bayesian modelling",
    "section": "Prior, likelihood and posterior",
    "text": "Prior, likelihood and posterior\n\nFigure 2: Scaled Binomial likelihood for six successes out of 14 trials, \\(\\mathsf{Beta}(3/2, 3/2)\\) prior and corresponding posterior distribution from a beta-binomial model."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#proper-prior",
    "href": "slides/bayesmod-slides1.html#proper-prior",
    "title": "Bayesian modelling",
    "section": "Proper prior",
    "text": "Proper prior\nWe could define the posterior simply as the normalized product of the likelihood and some prior function.\nThe prior function need not even be proportional to a density function (i.e., integrable as a function of \\(\\boldsymbol{\\theta}\\)).\nFor example,\n\n\\(p(\\theta) \\propto \\theta^{-1}(1-\\theta)^{-1}\\) is improper because it is not integrable.\n\\(p(\\theta) \\propto 1\\) is a proper prior over \\([0,1]\\) (uniform)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#validity-of-the-posterior",
    "href": "slides/bayesmod-slides1.html#validity-of-the-posterior",
    "title": "Bayesian modelling",
    "section": "Validity of the posterior",
    "text": "Validity of the posterior\n\nThe marginal likelihood does not depend on \\(\\boldsymbol{\\theta}\\)\n\n(a normalizing constant)\n\nFor the posterior density to be proper,\n\nthe marginal likelihood must be a finite!\nin continuous models, the posterior is proper whenever the prior function is proper."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#different-priors-give-different-posteriors",
    "href": "slides/bayesmod-slides1.html#different-priors-give-different-posteriors",
    "title": "Bayesian modelling",
    "section": "Different priors give different posteriors",
    "text": "Different priors give different posteriors\n\nFigure 3: Scaled binomial likelihood for six successes out of 14 trials, with \\(\\mathsf{Beta}(3/2, 3/2)\\) prior (left), \\(\\mathsf{Beta}(1/4, 1/4)\\) (middle) and truncated uniform on \\([0,1/2]\\) (right), with the corresponding posterior distributions."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#role-of-the-prior",
    "href": "slides/bayesmod-slides1.html#role-of-the-prior",
    "title": "Bayesian modelling",
    "section": "Role of the prior",
    "text": "Role of the prior\nThe posterior is beta, with expected value \\[\\begin{align*}\n\\mathsf{E}(\\theta \\mid y) &= w\\frac{y}{n} + (1-w) \\frac{\\alpha}{\\alpha + \\beta}, \\\\ w&=\\frac{n}{n+\\alpha+\\beta}\n\\end{align*}\\] a weighted average of\n\nthe maximum likelihood estimator and\nthe prior mean.\n\n\nWe can think of the parameter \\(\\alpha\\) (respectively \\(\\beta\\)) as representing the fixed prior number of success (resp. failures)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#posterior-concentration",
    "href": "slides/bayesmod-slides1.html#posterior-concentration",
    "title": "Bayesian modelling",
    "section": "Posterior concentration",
    "text": "Posterior concentration\nExcept for stubborn priors, the likelihood contribution dominates in large samples. The impact of the prior is then often negligible.\n\nFigure 4: Beta posterior and binomial likelihood with a uniform prior for increasing number of observations (from left to right)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#summarizing-posterior-distributions",
    "href": "slides/bayesmod-slides1.html#summarizing-posterior-distributions",
    "title": "Bayesian modelling",
    "section": "Summarizing posterior distributions",
    "text": "Summarizing posterior distributions\n\nThe output of the Bayesian learning will be either of:\n\na fully characterized distribution (in toy examples).\na numerical approximation to the posterior distribution.\nan exact or approximate sample drawn from the posterior distribution.\n\n\nThe first case, which we have already encountered, allows us to query moments (mean, median, mode) directly provided there are analytical expressions for the latter, or else we could simulate from the model."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#bayesian-inference-in-practice",
    "href": "slides/bayesmod-slides1.html#bayesian-inference-in-practice",
    "title": "Bayesian modelling",
    "section": "Bayesian inference in practice",
    "text": "Bayesian inference in practice\nMost of the field revolves around the creation of algorithms that either\n\ncircumvent the calculation of the normalizing constant\n\n(Monte Carlo and Markov chain Monte Carlo methods)\n\nprovide accurate numerical approximation, including for marginalizing out all but one parameter.\n\n(integrated nested Laplace approximations, variational inference, etc.)"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#predictive-distributions",
    "href": "slides/bayesmod-slides1.html#predictive-distributions",
    "title": "Bayesian modelling",
    "section": "Predictive distributions",
    "text": "Predictive distributions\nDefine the \\(\\color{#D55E00}{\\text{posterior predictive}}\\), \\[\\begin{align*}\np(y_{\\text{new}}\\mid \\boldsymbol{y}) = \\int_{\\boldsymbol{\\Theta}} p(y_{\\text{new}} \\mid \\boldsymbol{\\theta}) \\color{#D55E00}{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})} \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\] and the \\(\\color{#56B4E9}{\\text{prior predictive}}\\) \\[\\begin{align*}\np(y_{\\text{new}}) = \\int_{\\boldsymbol{\\Theta}} p(y_{\\text{new}} \\mid \\boldsymbol{\\theta}) \\color{#56B4E9}{p(\\boldsymbol{\\theta})} \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\] is useful for determining whether the prior is sensical."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#analytical-derivation-of-predictive-distribution",
    "href": "slides/bayesmod-slides1.html#analytical-derivation-of-predictive-distribution",
    "title": "Bayesian modelling",
    "section": "Analytical derivation of predictive distribution",
    "text": "Analytical derivation of predictive distribution\nGiven the \\(\\mathsf{Be}(a, b)\\) prior or posterior, the predictive for \\(n_{\\text{new}}\\) trials is beta-binomial with density \\[\\begin{align*}\np(y_{\\text{new}}\\mid y) &= \\int_0^1 \\binom{n_{\\text{new}}}{y_{\\text{new}}} \\frac{\\theta^{a + y_{\\text{new}}-1}(1-\\theta)^{b + k - y_{\\text{new}}-1}}{\n\\mathrm{Be}(a, b)}\\mathrm{d} \\theta\n\\\\&= \\binom{n_{\\text{new}}}{y_{\\text{new}}} \\frac{\\mathrm{Be}(a + y_{\\text{new}}, b + n_{\\text{new}} - y_{\\text{new}})}{\\mathrm{Be}(a, b)}\n\\end{align*}\\]\nReplace \\(a=y + \\alpha\\) and \\(b=n-y + \\beta\\) to get the posterior predictive distribution."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#posterior-predictive-distribution",
    "href": "slides/bayesmod-slides1.html#posterior-predictive-distribution",
    "title": "Bayesian modelling",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\n\nFigure 5: Beta-binomial posterior predictive distribution with corresponding binomial mass function evaluated at the maximum likelihood estimator."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#posterior-predictive-distribution-via-simulation",
    "href": "slides/bayesmod-slides1.html#posterior-predictive-distribution-via-simulation",
    "title": "Bayesian modelling",
    "section": "Posterior predictive distribution via simulation",
    "text": "Posterior predictive distribution via simulation\nThe posterior predictive carries over the parameter uncertainty so will typically be wider and overdispersed relative to the corresponding distribution.\nGiven a draw \\(\\theta^*\\) from the posterior, simulate a new observation from the distribution \\(f(y_{\\text{new}}; \\theta^*)\\).\n\nnpost <- 1e4L\n# Sample draws from the posterior distribution\npost_samp <- rbeta(n = npost, y + alpha, n - y + beta)\n# For each draw, sample new observation\npost_pred <- rbinom(n = npost, size = n, prob = post_samp)\n\n\n\nThe beta-binomial is used to model overdispersion in binary regression models."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#summarizing-posterior-distributions-1",
    "href": "slides/bayesmod-slides1.html#summarizing-posterior-distributions-1",
    "title": "Bayesian modelling",
    "section": "Summarizing posterior distributions",
    "text": "Summarizing posterior distributions\nThe output of a Bayesian procedure is a distribution for the parameters given the data.\nWe may wish to return different numerical summaries (expected value, variance, mode, quantiles, …)\nThe question: which point estimator to return?"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#decision-theory-and-loss-functions",
    "href": "slides/bayesmod-slides1.html#decision-theory-and-loss-functions",
    "title": "Bayesian modelling",
    "section": "Decision theory and loss functions",
    "text": "Decision theory and loss functions\nA loss function \\(c(\\boldsymbol{\\theta}, \\boldsymbol{\\upsilon}): \\boldsymbol{\\Theta} \\mapsto \\mathbb{R}^k\\) assigns a weight to each value \\(\\boldsymbol{\\theta}\\), corresponding to the regret or loss.\nThe point estimator \\(\\widehat{\\boldsymbol{\\upsilon}}\\) is the minimizer of the expected loss \\[\\begin{align*}\n\\widehat{\\boldsymbol{\\upsilon}} &= \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\upsilon}}\\mathsf{E}_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y}}\\{c(\\boldsymbol{\\theta}, \\boldsymbol{v})\\} \\\\&=\\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\upsilon}} \\int_{\\boldsymbol{\\Theta}} c(\\boldsymbol{\\theta}, \\boldsymbol{\\upsilon})p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#point-estimators-and-loss-functions",
    "href": "slides/bayesmod-slides1.html#point-estimators-and-loss-functions",
    "title": "Bayesian modelling",
    "section": "Point estimators and loss functions",
    "text": "Point estimators and loss functions\nIn a univariate setting, the most widely used point estimators are\n\nmean: quadratic loss \\(c(\\theta, \\upsilon) = (\\theta-\\upsilon)^2\\)\nmedian: absolute loss \\(c(\\theta, \\upsilon)=|\\theta - \\upsilon|\\)\nmode: 0-1 loss \\(c(\\theta, \\upsilon) = 1-\\mathrm{I}(\\upsilon = \\theta)\\)\n\nThe posterior mode \\(\\boldsymbol{\\theta}_{\\mathrm{map}} = \\mathrm{argmax}_{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\) is the maximum a posteriori or MAP estimator."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#measures-of-central-tendency",
    "href": "slides/bayesmod-slides1.html#measures-of-central-tendency",
    "title": "Bayesian modelling",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\n\nFigure 6: Point estimators from a right-skewed distribution (left) and from a multimodal distribution (right)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#example-of-loss-functions",
    "href": "slides/bayesmod-slides1.html#example-of-loss-functions",
    "title": "Bayesian modelling",
    "section": "Example of loss functions",
    "text": "Example of loss functions\n\nFigure 7: Posterior density with mean, mode and median point estimators (left) and corresponding loss functions, scaled to have minimum value of zero (right)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#credible-regions",
    "href": "slides/bayesmod-slides1.html#credible-regions",
    "title": "Bayesian modelling",
    "section": "Credible regions",
    "text": "Credible regions\nThe freshman dream comes true! A \\(1-\\alpha\\) credible region give a set of parameter values which contains the “true value” of the parameter \\(\\boldsymbol{\\theta}\\) with probability \\(1-\\alpha\\).\nCaveat: McElreath (2020) suggests the term ‘compatibility’, as it\n\nreturns the range of parameter values compatible with the model and data."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#which-credible-intervals",
    "href": "slides/bayesmod-slides1.html#which-credible-intervals",
    "title": "Bayesian modelling",
    "section": "Which credible intervals?",
    "text": "Which credible intervals?\nMultiple \\(1-\\alpha\\) intervals, most common are\n\nequitailed: region \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles and\nhighest posterior density interval (HPDI), which gives the smallest interval \\((1-\\alpha)\\) probability\n\n\n\nIf we accept to have more than a single interval, the highest posterior density region can be a set of disjoint intervals. The HDPI is more sensitive to the number of draws and more computationally intensive (see R package HDinterval)"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#illustration-of-credible-regions",
    "href": "slides/bayesmod-slides1.html#illustration-of-credible-regions",
    "title": "Bayesian modelling",
    "section": "Illustration of credible regions",
    "text": "Illustration of credible regions\n\nFigure 8: Density plots with 89% (top) and 50% (bottom) equitailed or central credible (left) and highest posterior density (right) regions for two data sets, highlighted in grey."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#references",
    "href": "slides/bayesmod-slides1.html#references",
    "title": "Bayesian modelling",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nCasella, G., & Berger, R. L. (2002). Statistical inference (2nd ed.). Duxbury.\n\n\nFinetti, B. de. (1974). Theory of probability: A critical introductory treatment (Vol. 1). Wiley.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and STAN (2nd ed.). Chapman; Hall/CRC."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#priors",
    "href": "slides/bayesmod-slides2.html#priors",
    "title": "Bayesian modelling",
    "section": "Priors",
    "text": "Priors\nThe posterior density is\n\\[\\begin{align*}\n\\color{#D55E00}{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})} = \\frac{\\color{#0072B2}{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})} \\times  \\color{#56B4E9}{p(\\boldsymbol{\\theta})}}{\\color{#E69F00}{\\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\\mathrm{d} \\boldsymbol{\\theta}}},\n\\end{align*}\\]\nwhere \\[\\color{#D55E00}{\\text{posterior}} \\propto \\color{#0072B2}{\\text{likelihood}} \\times \\color{#56B4E9}{\\text{prior}}\\]\nWe need to determine a suitable prior."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#impact-of-the-prior",
    "href": "slides/bayesmod-slides2.html#impact-of-the-prior",
    "title": "Bayesian modelling",
    "section": "Impact of the prior",
    "text": "Impact of the prior\nThe posterior is a compromise prior and likelihood:\n\nthe more informative the prior, the more the posterior resembles it.\nin large samples, the effect of the prior is often negligible1\n\ndepends on the parameter!"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#controversial",
    "href": "slides/bayesmod-slides2.html#controversial",
    "title": "Bayesian modelling",
    "section": "Controversial?",
    "text": "Controversial?\n\nNo unique choice for the prior: different analysts get different inferences\nWhat is the robustness to the prior specification? Check through sensitivity analysis.\nBy tuning the prior, we can get any answer we get (if informative enough)\nEven with prior knowledge, hard to elicit parameter (many different models could yield similar summary statistics)"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#choosing-priors",
    "href": "slides/bayesmod-slides2.html#choosing-priors",
    "title": "Bayesian modelling",
    "section": "Choosing priors",
    "text": "Choosing priors\nInfinite number of choice, but many default choices…\n\nconditionally conjugate priors (ease of interpretation, computational advantages)\nflat priors and vague priors (mostly uninformative)\ninformative priors (expert opinion)\nJeffrey’s priors (improper, invariant to reparametrization)\npenalized complexity (regularization)\nshrinkage priors (variable selection, reduce overfitting)"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#determining-hyperparameters",
    "href": "slides/bayesmod-slides2.html#determining-hyperparameters",
    "title": "Bayesian modelling",
    "section": "Determining hyperparameters",
    "text": "Determining hyperparameters\nWe term hyperparameters the parameters of the (hyper)priors.\nHow to elicit reasonable values for them?\n\nuse moment matching to get sensible values\ntrial-and-error using the prior predictive"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#example-of-simple-linear-regression",
    "href": "slides/bayesmod-slides2.html#example-of-simple-linear-regression",
    "title": "Bayesian modelling",
    "section": "Example of simple linear regression",
    "text": "Example of simple linear regression\nWorking with standardized response and inputs \\[x_i \\mapsto (x_i - \\overline{x})/\\mathrm{sd}(\\boldsymbol{x}),\\]\n\nthe slope is the correlation between explanatory \\(\\mathrm{X}\\) and response \\(Y\\)\nthe intercept should be mean zero\nare there sensible bounds for the range of the response?"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#bixi-counts",
    "href": "slides/bayesmod-slides2.html#bixi-counts",
    "title": "Bayesian modelling",
    "section": "Bixi counts",
    "text": "Bixi counts\n\nFigure 1: Prior draws of the linear regression coefficients with observed data superimposed (left), and scatterplot of prior predictive draws (light gray) against observed data (right). There are 20 docks on the platform."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#example-2---simple-linear-regression",
    "href": "slides/bayesmod-slides2.html#example-2---simple-linear-regression",
    "title": "Bayesian modelling",
    "section": "Example 2 - simple linear regression",
    "text": "Example 2 - simple linear regression\nConsider the relationship between height (\\(Y\\), in cm) and weight (\\(X\\), in kg) among humans adults.1\nModel using a simple linear regression\n\\[\\begin{align*}\nh_i &\\sim \\mathsf{No}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1(\\mathrm{x}_i - \\overline{x}) \\\\\n\\beta_0 &\\sim \\mathsf{No}(178, 20^2) \\\\\n\\sigma &\\sim \\mathsf{U}(0, 50)\n\\end{align*}\\]\nSection 4.4.1 of McElreath (2020)"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#priors-for-the-slope",
    "href": "slides/bayesmod-slides2.html#priors-for-the-slope",
    "title": "Bayesian modelling",
    "section": "Priors for the slope",
    "text": "Priors for the slope\n\nFigure 2: Prior draws of linear regressions with different priors: vague \\(\\beta_1 \\sim \\mathsf{No}(0, 100)\\) (left) and lognormal \\(\\ln(\\beta_1) \\sim \\mathsf{No}(0,1)\\) (right). Figure 4.5 of McElreath (2020). The Guiness record for the world’s tallest person is 272cm."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#conjugate-priors",
    "href": "slides/bayesmod-slides2.html#conjugate-priors",
    "title": "Bayesian modelling",
    "section": "Conjugate priors",
    "text": "Conjugate priors\nA prior density \\(p(\\boldsymbol{\\theta})\\) is conjugate for likelihood \\(L(\\boldsymbol{\\theta}; \\boldsymbol{y})\\) if the product \\(L(\\boldsymbol{\\theta}; \\boldsymbol{y})p(\\boldsymbol{\\theta})\\), after renormalization, is of the same parametric family as the prior.\nDistributions that are exponential family admit conjugate priors.1\nA distribution is an exponential family if it’s density can be written \\[\\begin{align*}\nf(y; \\boldsymbol{\\theta}) = \\exp\\left\\{ \\sum_{k=1}^K Q_k(\\boldsymbol{\\theta}) t_k(y) + D(\\boldsymbol{\\theta})\\right\\}.\n\\end{align*}\\] The support of \\(f\\) mustn’t depend on \\(\\boldsymbol{\\theta}\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#conjugate-priors-for-common-exponential-families",
    "href": "slides/bayesmod-slides2.html#conjugate-priors-for-common-exponential-families",
    "title": "Bayesian modelling",
    "section": "Conjugate priors for common exponential families",
    "text": "Conjugate priors for common exponential families\n\n\n\n\n\n\n\n\ndistribution\nunknown parameter\nconjugate prior\n\n\n\n\n\\(Y \\sim \\mathsf{Exp}(\\lambda)\\)\n\\(\\lambda\\)\n\\(\\lambda \\sim \\mathsf{Ga}(\\alpha, \\beta)\\)\n\n\n\\(Y \\sim \\mathsf{Po}(\\mu)\\)\n\\(\\mu\\)\n\\(\\mu \\sim \\mathsf{Ga}(\\alpha, \\beta)\\)\n\n\n\\(Y \\sim \\mathsf{Bin}(n, \\theta)\\)\n\\(\\theta\\)\n\\(\\theta \\sim \\mathsf{Be}(\\alpha, \\beta)\\)\n\n\n\\(Y \\sim \\mathsf{No}(\\mu, \\sigma^2)\\)\n\\(\\mu\\)\n\\(\\mu \\sim \\mathsf{No}(\\nu, \\omega^2)\\)\n\n\n\\(Y \\sim \\mathsf{No}(\\mu, \\sigma^2)\\)\n\\(\\sigma\\)\n\\(\\sigma^{-2} \\sim \\mathsf{Ga}(\\alpha, \\beta)\\)\n\n\n\\(Y \\sim \\mathsf{No}(\\mu, \\sigma^2)\\)\n\\(\\mu, \\sigma\\)\n\\(\\mu \\mid \\sigma^2 \\sim \\mathsf{No}(\\nu, \\omega \\sigma^2)\\), \\(\\sigma^{-2} \\sim \\mathsf{Ga}(\\alpha, \\beta)\\)"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#conjugate-prior-for-the-poisson",
    "href": "slides/bayesmod-slides2.html#conjugate-prior-for-the-poisson",
    "title": "Bayesian modelling",
    "section": "Conjugate prior for the Poisson",
    "text": "Conjugate prior for the Poisson\nIf \\(Y \\sim \\mathsf{Po}(\\mu)\\) with density \\(f(y) = \\mu^x\\exp(-\\mu x)/x!\\), then for \\(\\mu \\sim \\mathsf{Ga}(\\alpha, \\beta)\\) with \\(\\alpha, \\beta\\) fixed.\n\\[\np(\\mu \\mid y) \\stackrel{\\mu}{\\propto} \\mu^x \\exp(-\\mu x) \\mu^{\\alpha-1} \\exp(-\\beta \\mu)\n\\] so the posterior is gamma \\(\\mathsf{Ga}(x + \\alpha, x + \\beta)\\).\nParameter interpretation: \\(\\alpha\\) events in \\(\\beta\\) time intervals."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#conjugate-prior-for-gaussian-known-variance",
    "href": "slides/bayesmod-slides2.html#conjugate-prior-for-gaussian-known-variance",
    "title": "Bayesian modelling",
    "section": "Conjugate prior for Gaussian (known variance)",
    "text": "Conjugate prior for Gaussian (known variance)\nConsider an iid sample, \\(Y_i \\sim \\mathsf{No}(\\mu, \\sigma^2)\\) and let \\(\\mu \\mid \\sigma \\sim \\mathsf{No}(\\nu, \\sigma^2\\tau^2)\\). Then, \\[\\begin{align*}\np(\\mu, \\sigma) &\\propto \\frac{p(\\sigma)}{\\sigma^{n+1}} \\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_{i}-\\mu)^2\\right\\} \\exp\\left\\{-\\frac{1}{2\\sigma^2\\tau^2}(\\mu - \\nu)^2\\right\\}\n\\\\&\\propto \\frac{p(\\sigma)}{\\sigma^{n+1}} \\exp\\left\\{\\left(\\sum_{i=1}^n y_{i} + \\frac{\\nu}{\\tau^2}\\right)\\frac{\\mu}{\\sigma^2} - \\left( \\frac{n}{2} +\\frac{1}{2\\tau^2}\\right)\\frac{\\mu^2}{\\sigma^2}\\right\\}.\n\\end{align*}\\]\nThe conditional posterior \\(p(\\mu \\mid \\sigma)\\) is Gaussian with\n\nmean \\((n\\overline{y}\\tau^2 + \\nu)/(n\\tau^2 + 1)\\) and\nprecision (reciprocal variance) \\((n + 1/\\tau^2)/\\sigma^2\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#upworthy-examples",
    "href": "slides/bayesmod-slides2.html#upworthy-examples",
    "title": "Bayesian modelling",
    "section": "Upworthy examples",
    "text": "Upworthy examples\n\nThe Upworthy Research Archive (Matias et al., 2021) contains results for 22743 experiments, with a click through rate of 1.58% on average and a standard deviation of 1.23%.\nWe consider an A/B test that compared four different headlines for a story.\nWe model the conversion rate for each using \\(\\texttt{click}_i \\sim \\mathsf{Po}(\\lambda_i\\texttt{impression}_i)\\)"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#ab-test-sesame-street-example",
    "href": "slides/bayesmod-slides2.html#ab-test-sesame-street-example",
    "title": "Bayesian modelling",
    "section": "A/B test: Sesame street example",
    "text": "A/B test: Sesame street example\n\n\n\n\n\nheadline\nimpressions\nclicks\n\n\n\n\nH1\n3060\n49\n\n\nH2\n2982\n20\n\n\nH3\n3112\n31\n\n\nH4\n3083\n9\n\n\n\n\n\nConjugate prior: moment matching for \\(\\lambda \\sim \\mathsf{Ga}(\\alpha, \\beta )\\) gives \\(\\alpha = 1.64\\) and \\(\\beta = 0.01\\), as \\(\\beta = \\mathsf{Va}_0(\\lambda)/\\mathsf{E}_0(\\lambda)\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#posterior-distributions-for-sesame-street",
    "href": "slides/bayesmod-slides2.html#posterior-distributions-for-sesame-street",
    "title": "Bayesian modelling",
    "section": "Posterior distributions for Sesame Street",
    "text": "Posterior distributions for Sesame Street\n\nFigure 3: Gamma posterior of the conversion rate for the Upworthy Sesame street headline."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#proper-priors",
    "href": "slides/bayesmod-slides2.html#proper-priors",
    "title": "Bayesian modelling",
    "section": "Proper priors",
    "text": "Proper priors\n\nTheorem 1 A sufficient condition for a prior to yield a proper (i.e., integrable) posterior density function is that it is (proportional) to a density function.\n\n\nIf we pick an improper prior, we need to check that the posterior is well-defined.\nThe answer to this question may depend on the sample size."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#proper-posterior-in-a-random-effect-model",
    "href": "slides/bayesmod-slides2.html#proper-posterior-in-a-random-effect-model",
    "title": "Bayesian modelling",
    "section": "Proper posterior in a random effect model",
    "text": "Proper posterior in a random effect model\nConsider a Gaussian random effect model with \\(n\\) independent observations in \\(J\\) groups\nThe \\(i\\)th observation in group \\(j\\) is \\[\\begin{align*}\nY_{ij} &\\sim \\mathsf{No}(\\mu_{ij}, \\sigma^2) \\\\\n\\mu_{ij}&= \\mathbf{X}_i \\boldsymbol{\\beta} + \\alpha_j,  \\\\\n\\alpha_j &\\sim \\mathsf{No}(0, \\tau^2)\\\\\n...\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#conditions-for-a-proper-posterior",
    "href": "slides/bayesmod-slides2.html#conditions-for-a-proper-posterior",
    "title": "Bayesian modelling",
    "section": "Conditions for a proper posterior",
    "text": "Conditions for a proper posterior\n\nfor \\(\\tau \\sim \\mathsf{U}(0, \\infty)\\), we need at least \\(J \\geq 3\\) ‘groups’ for the posterior to be proper.\nif we take \\(p(\\tau) \\propto \\tau^{-1}\\), the posterior is never proper.\n\nAs Gelman (2006) states:\n\nin a hierarchical model the data can never rule out a group-level variance of zero, and so [a] prior distribution cannot put an infinite mass in this area"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#improper-priors-as-limiting-cases",
    "href": "slides/bayesmod-slides2.html#improper-priors-as-limiting-cases",
    "title": "Bayesian modelling",
    "section": "Improper priors as limiting cases",
    "text": "Improper priors as limiting cases\nWe can view the improper prior as a limiting case \\[\\sigma \\sim \\mathsf{U}(0, t), \\qquad t \\to \\infty.\\]\nThe Haldane prior for \\(\\theta\\) in a binomial model is \\(\\theta^{-1}(1-\\theta)^{-1}\\), a limiting \\(\\mathsf{Be}(0,0)\\) distribution.\nThe improper prior \\(p(\\sigma) \\propto \\sigma^{-1}\\) is equivalent to an inverse gamma \\(\\mathsf{IGa}(\\epsilon, \\epsilon)\\) when \\(\\epsilon \\to 0\\).\nThe limiting posterior is thus improper for random effects scales, so the value of \\(\\epsilon\\) matters."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#mdi-prior-for-generalized-pareto",
    "href": "slides/bayesmod-slides2.html#mdi-prior-for-generalized-pareto",
    "title": "Bayesian modelling",
    "section": "MDI prior for generalized Pareto",
    "text": "MDI prior for generalized Pareto\nLet \\(Y_i \\sim \\mathsf{GP}(\\sigma, \\xi)\\) be generalized Pareto with density \\[f(x) = \\sigma^{-1}(1+\\xi x/\\sigma)_{+}^{-1/\\xi-1}\\] for \\(\\sigma>0\\) and \\(\\xi \\in \\mathbb{R}\\), and \\(x_{+} =\\max\\{0, x\\}\\).\nConsider the maximum data information (MDI) \\[p(\\xi) \\propto \\exp(-\\xi).\\]\nSince \\(\\lim_{\\xi \\to -\\infty} \\exp(-\\xi) = \\infty\\), the prior density increases without bound as \\(\\xi\\) becomes smaller."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#truncated-mdi-for-generalized-pareto-distribution",
    "href": "slides/bayesmod-slides2.html#truncated-mdi-for-generalized-pareto-distribution",
    "title": "Bayesian modelling",
    "section": "Truncated MDI for generalized Pareto distribution",
    "text": "Truncated MDI for generalized Pareto distribution\nThe MDI prior leads to an improper posterior without modification.\n\nFigure 4: Unscaled maximum data information (MDI) prior density.If we restrict the range of the MDI prior \\(p(\\xi)\\) to \\(\\xi \\geq -1\\), then \\(p(\\xi + 1) \\sim \\mathsf{Exp}(1)\\) and posterior is proper."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#flat-priors",
    "href": "slides/bayesmod-slides2.html#flat-priors",
    "title": "Bayesian modelling",
    "section": "Flat priors",
    "text": "Flat priors\nUniform prior over the support of \\(\\theta\\), \\[p(\\theta) \\propto 1.\\]\nImproper prior unless \\(\\theta \\in [a,b]\\) for finite \\(a, b\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#flat-priors-for-scale-parameters",
    "href": "slides/bayesmod-slides2.html#flat-priors-for-scale-parameters",
    "title": "Bayesian modelling",
    "section": "Flat priors for scale parameters",
    "text": "Flat priors for scale parameters\nConsider a scale parameter \\(\\sigma > 0\\).\n\nWe could truncate the range, e.g., \\(\\sigma \\sim \\mathsf{U}(0, 50)\\), but this is not ‘uninformative’, as extreme values of \\(\\sigma\\) are as likely as small ones.\nThese priors are not invariant: if \\(p\\{\\log(\\sigma)\\} \\propto 1\\) implies \\(p(\\sigma) \\propto \\sigma^{-1}\\) so can be informative on another scale."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#vague-priors",
    "href": "slides/bayesmod-slides2.html#vague-priors",
    "title": "Bayesian modelling",
    "section": "Vague priors",
    "text": "Vague priors\nVague priors are very diffuse proper prior.\nFor example, a vague Gaussian prior for regression coefficients on standardized data, \\[\\boldsymbol{\\beta} \\sim \\mathsf{No}_p(\\mathbf{0}_p, 100\\mathbf{I}_p).\\]\n\nif we consider a logistic regression with a binary variable \\(\\mathrm{X}_j \\in \\{0,1\\}\\), then \\(\\beta_j =5\\) gives odds ratios of 150, and \\(\\beta_j=10\\) of around 22K…"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#invariance-and-jeffreys-prior",
    "href": "slides/bayesmod-slides2.html#invariance-and-jeffreys-prior",
    "title": "Bayesian modelling",
    "section": "Invariance and Jeffrey’s prior",
    "text": "Invariance and Jeffrey’s prior\nIn single-parameter models, the Jeffrey’s prior \\[p(\\theta) \\propto |\\imath(\\theta)|^{1/2},\\] proportional to the square root of the determinant of the Fisher information matrix, is invariant to any (differentiable) reparametrization."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#jeffreys-prior-for-the-binomial-distribution",
    "href": "slides/bayesmod-slides2.html#jeffreys-prior-for-the-binomial-distribution",
    "title": "Bayesian modelling",
    "section": "Jeffrey’s prior for the binomial distribution",
    "text": "Jeffrey’s prior for the binomial distribution\nConsider \\(Y \\sim \\mathsf{Bin}(1, \\theta)\\). The negative of the second derivative of the log likelihood with respect to \\(p\\) is \\[\n\\jmath(\\theta) = - \\partial^2 \\ell(\\theta; y) / \\partial \\theta^2 = y/\\theta^2 + (1-y)/(1-\\theta)^2.\n\\]\nSince \\(\\mathsf{E}(Y)=\\theta\\), the Fisher information is \\[\\imath(\\vartheta) = \\mathsf{E}\\{\\jmath(\\theta)\\}=1/\\theta + 1/(1-\\theta) = n/\\{\\theta(1-\\theta)\\}.\\] Jeffrey’s prior is therefore \\(p(\\theta) \\propto \\theta^{-1/2}(1-\\theta)^{-1/2}\\), a conjugate Beta prior \\(\\mathsf{Be}(0.5,0.5)\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#invariant-priors-for-location-scale-families",
    "href": "slides/bayesmod-slides2.html#invariant-priors-for-location-scale-families",
    "title": "Bayesian modelling",
    "section": "Invariant priors for location-scale families",
    "text": "Invariant priors for location-scale families\nFor a location-scale family with location \\(\\mu\\) and scale \\(\\sigma\\), the independent priors \\[\\begin{align*}\np(\\mu) &\\propto 1\\\\\np(\\sigma) &\\propto \\sigma^{-1}\n\\end{align*}\\] are location-scale invariant.\nThe results are invariant to affine transformations of the units, \\(\\vartheta = a + b \\theta\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#penalized-complexity-priors",
    "href": "slides/bayesmod-slides2.html#penalized-complexity-priors",
    "title": "Bayesian modelling",
    "section": "Penalized complexity priors",
    "text": "Penalized complexity priors\nSimpson et al. (2017) consider a principled way of constructing priors that penalized model complexity for stable inference and limit over-specification.\nSuppose that the restriction of the parameter creates a simpler base version.\n\ne.g., if we have a random effect \\(\\alpha \\sim \\mathsf{No}(0, \\zeta^2)\\), the value \\(\\zeta=0\\) corresponds to no group variability."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#ingredients-of-penalized-complexity-priors",
    "href": "slides/bayesmod-slides2.html#ingredients-of-penalized-complexity-priors",
    "title": "Bayesian modelling",
    "section": "Ingredients of penalized complexity priors",
    "text": "Ingredients of penalized complexity priors\nConsider a penalized complexity prior for parameter \\(\\zeta\\).\nOccam’s razor states that the simpler base model should be preferred if there is not enough evidence in favor of the full model.\nWe measure the complexity of the full model with density \\(f\\) using the Kullback–Leibler divergence between \\(f\\) and base model \\(f_0\\) densities. This is transformed into a distance \\(d=\\sqrt{2\\mathsf{KL}(f || f_0)}\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#penalized-complexity-prior-construction",
    "href": "slides/bayesmod-slides2.html#penalized-complexity-prior-construction",
    "title": "Bayesian modelling",
    "section": "Penalized complexity prior construction",
    "text": "Penalized complexity prior construction\nUsing a constant rate penalization from base model gives an exponential prior \\(p(d) = \\lambda \\exp(-\\lambda d)\\) on the distance scale, with a mode at \\(d=0\\), corresponding to the base model.\nBacktransform to parameter space to get \\(p(\\zeta)\\), truncate above if \\(d\\) is upper bounded, \\[p(\\zeta) = \\lambda \\exp\\{-\\lambda \\cdot d(\\zeta)\\} \\left| \\frac{\\partial d(\\zeta)}{\\partial \\zeta}\\right|.\\]"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#fixing-penalized-complexity-hyperparameter",
    "href": "slides/bayesmod-slides2.html#fixing-penalized-complexity-hyperparameter",
    "title": "Bayesian modelling",
    "section": "Fixing penalized complexity hyperparameter",
    "text": "Fixing penalized complexity hyperparameter\nPick rate \\(\\lambda\\) to control prior density in the tail, by specifying a value for (a transformation of) the parameter, say \\(g(\\zeta)\\), which is interpretable.\nElicit values of \\(Q\\) and small probability \\(\\alpha\\) such that the tail probability \\[\\Pr\\{g(\\zeta) > Q\\} = \\alpha.\\]"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#penalized-complexity-prior-for-random-effect-scale",
    "href": "slides/bayesmod-slides2.html#penalized-complexity-prior-for-random-effect-scale",
    "title": "Bayesian modelling",
    "section": "Penalized complexity prior for random effect scale",
    "text": "Penalized complexity prior for random effect scale\nIf \\(\\alpha_j \\sim \\mathsf{No}(0, \\zeta^2)\\), the penalized complexity prior is exponential with rate \\(\\lambda\\).\nGiven \\(Q\\) a high quantile of the standard deviation \\(\\zeta\\), set \\(\\lambda = -\\ln(\\alpha/Q)\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#priors-for-scale-of-random-effects",
    "href": "slides/bayesmod-slides2.html#priors-for-scale-of-random-effects",
    "title": "Bayesian modelling",
    "section": "Priors for scale of random effects",
    "text": "Priors for scale of random effects\nThe conjugate inverse gamma prior \\(p(1/\\zeta) \\sim \\mathsf{Ga}(\\alpha, \\beta)\\) is such that the mode for \\(\\zeta\\) is \\(\\beta/(1+\\alpha)\\).\nOften, we take \\(\\beta=\\alpha = 0.01\\) or \\(0.001\\), but this leads to improper prior. So small values are not optimal for ‘random effects’, and this prior cannot provide shrinkage or allow for no variability between groups."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#priors-for-scale-of-random-effects-1",
    "href": "slides/bayesmod-slides2.html#priors-for-scale-of-random-effects-1",
    "title": "Bayesian modelling",
    "section": "Priors for scale of random effects",
    "text": "Priors for scale of random effects\nA popular suggestion, due to Gelman (2006), is to take a centered Student-\\(t\\) distribution with \\(\\nu\\) degrees of freedoms, truncated over \\([0, \\infty)\\) with scale \\(s\\).\n\nsince the mode is at zero, provides support for the base model\nwe want small degrees of freedom \\(\\nu\\), preferable to take \\(\\nu=3\\)? Cauchy model (\\(\\nu=1\\)) still popular."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#prior-sensitivity",
    "href": "slides/bayesmod-slides2.html#prior-sensitivity",
    "title": "Bayesian modelling",
    "section": "Prior sensitivity",
    "text": "Prior sensitivity\nDoes the priors matter? As robustness check, one can fit the model with\n\ndifferent priors function\ndifferent hyperparameter values\n\nCostly, but may be needed to convince reviewers ;)"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#distraction-from-smartwach",
    "href": "slides/bayesmod-slides2.html#distraction-from-smartwach",
    "title": "Bayesian modelling",
    "section": "Distraction from smartwach",
    "text": "Distraction from smartwach\nWe consider an experimental study conducted at Tech3Lab on road safety.\n\nIn Brodeur et al. (2021), 31 participants were asked to drive in a virtual environment.\nThe number of road violation was measured for 4 different type of distractions (phone notification, phone on speaker, texting and smartwatch).\nBalanced data, random order of tasks"
  },
  {
    "objectID": "slides/bayesmod-slides2.html#poisson-mixed-model",
    "href": "slides/bayesmod-slides2.html#poisson-mixed-model",
    "title": "Bayesian modelling",
    "section": "Poisson mixed model",
    "text": "Poisson mixed model\nWe model the number of violations, nviolation as a function of distraction type (task) and participant id.1 \\[\\begin{align*}\n\\texttt{nviolation}_{ij} &\\sim \\mathsf{Po}(\\mu_{ij})\\\\\n\\mu_{ij} &= \\exp(\\beta_{j} + \\alpha_i),\\\\\n\\beta_j &\\sim \\mathsf{No}(0, 100), \\\\\n\\alpha_i &\\sim \\mathsf{No}(0, \\kappa^2).\n\\end{align*}\\]\nSpecifically, \\(\\beta_j\\) is the coefficient for task \\(j\\) (distraction type) and \\(\\alpha_i\\) is the random effect of participant \\(i\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#priors-for-random-effect-scale",
    "href": "slides/bayesmod-slides2.html#priors-for-random-effect-scale",
    "title": "Bayesian modelling",
    "section": "Priors for random effect scale",
    "text": "Priors for random effect scale\nConsider different priors for \\(\\kappa\\)\n\nflat uniform prior \\(\\mathsf{U}(0,10)\\)\nconjugate inverse gamma \\(\\mathsf{IG}(0.01, 0.01)\\) prior\na Student-\\(t\\) with \\(\\nu=3\\) degrees of freedom\na penalized complexity prior such that the 0.95 percentile of the scale is 5, corresponding to \\(\\mathsf{Exp}(0.6)\\)."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#sensitivity-analysis-for-smartwatch-data",
    "href": "slides/bayesmod-slides2.html#sensitivity-analysis-for-smartwatch-data",
    "title": "Bayesian modelling",
    "section": "Sensitivity analysis for smartwatch data",
    "text": "Sensitivity analysis for smartwatch data\n\nFigure 5: Posterior density of \\(\\zeta\\) for four different priors. The circle denotes the median and the bars the 50% and 95% percentile credible intervals.Basically indistinguishable results for the random scale.."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#eight-schools-example",
    "href": "slides/bayesmod-slides2.html#eight-schools-example",
    "title": "Bayesian modelling",
    "section": "Eight schools example",
    "text": "Eight schools example\nAverage results on SAT program, for eight schools (Rubin, 1981).\nThe hierarchical model is\n\\[\\begin{align*}\nY_i &\\sim \\mathsf{No}(\\mu + \\eta_i, \\sigma_i^2)\\\\\n\\mu &\\sim \\mathsf{No}(0, 100)\\\\\n\\eta_i & \\sim \\mathsf{No}(0, \\tau^2)\n\\end{align*}\\] Given the large sample in each school, we treat \\(\\sigma_i\\) as fixed data."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#sensibility-analysis-for-eight-schools-example",
    "href": "slides/bayesmod-slides2.html#sensibility-analysis-for-eight-schools-example",
    "title": "Bayesian modelling",
    "section": "Sensibility analysis for eight schools example",
    "text": "Sensibility analysis for eight schools example\n\nFigure 6: Posterior density of the school-specific random effects standard deviation \\(\\tau\\) under different priors."
  },
  {
    "objectID": "slides/bayesmod-slides2.html#references",
    "href": "slides/bayesmod-slides2.html#references",
    "title": "Bayesian modelling",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nBrodeur, M., Ruer, P., Léger, P.-M., & Sénécal, S. (2021). Smartwatches are more distracting than mobile phones while driving: Results from an experimental study. Accident Analysis & Prevention, 149, 105846. https://doi.org/10.1016/j.aap.2020.105846\n\n\nGelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Bayesian Analysis, 1(3), 515–534. https://doi.org/10.1214/06-BA117A\n\n\nMatias, J. N., Munger, K., Le Quere, M. A., & Ebersole, C. (2021). The Upworthy Research Archive, a time series of 32,487 experiments in U.S. media. Scientific Data, 8(195). https://doi.org/10.1038/s41597-021-00934-7\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and STAN (2nd ed.). Chapman; Hall/CRC.\n\n\nRubin, D. B. (1981). Estimation in parallel randomized experiments. Journal of Educational Statistics, 6(4), 377–401. https://doi.org/10.3102/10769986006004377\n\n\nSimpson, D., Rue, H., Riebler, A., Martins, T. G., & Sørbye, S. H. (2017). Penalising model component complexity: A principled, practical approach to constructing priors. Statistical Science, 32(1), 1–28. https://doi.org/10.1214/16-STS576"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Outline",
    "section": "",
    "text": "Dr. Léo Belzile\n   4.850, Côte-Sainte-Catherine\n   leo.belzile@hec.ca\n\n\n\n\n\n   Fall 2023\n   Tuesday\n   8:30-11:30\n   Decelles, Rivière-du-Loup"
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Outline",
    "section": "Textbooks",
    "text": "Textbooks\nI will assign readings from McElreath (2020) and Johnson et al. (2022), which is freely available online.\n\n\n\nStatistical Rethinking:A Bayesian Course with Examples in R and Stan\nR. McElreath\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Rules!An Introduction to Applied Bayesian Modeling\nA.A. Johnson, M.Q. Ott, and M. Dogucu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourse notes for the class can be found online"
  },
  {
    "objectID": "syllabus.html#other-references",
    "href": "syllabus.html#other-references",
    "title": "Outline",
    "section": "Other references",
    "text": "Other references\nThere will occasionally be additional articles to read; links to these other resources will be included on the content page for that session."
  },
  {
    "objectID": "syllabus.html#student-hours",
    "href": "syllabus.html#student-hours",
    "title": "Outline",
    "section": "Student hours",
    "text": "Student hours\nWednesday afternoon from 13:00 until 16:00 or by appointment. My office, 4.850, is located next to the southern elevators in Côte-Sainte-Catherine building.\nPlease watch this video:\n\n\nStudent hours are set times dedicated to all of you (most professors call these “office hours”; I don’t1). This means that I will be in my office waiting for you to come by if you want to talk to me in person (or remotely) with whatever questions you have. This is the best and easiest way to find me and the best chance for discussing class material and concerns."
  },
  {
    "objectID": "syllabus.html#intellectual-integrity",
    "href": "syllabus.html#intellectual-integrity",
    "title": "Outline",
    "section": "Intellectual integrity",
    "text": "Intellectual integrity\nPlease don’t cheat! The official policy lists the school rules regarding plagiarism and academic integrity."
  },
  {
    "objectID": "syllabus.html#student-services",
    "href": "syllabus.html#student-services",
    "title": "Outline",
    "section": "Student services",
    "text": "Student services\nStudents with special needs should feel free to approach me so we can best discuss accommodations. Do check out HEC Montréal’s disabled students and [psychological] (https://www.hec.ca/en/students/support-resources/psychological-support/index.html) support services."
  },
  {
    "objectID": "syllabus.html#harassment-and-sexual-violence",
    "href": "syllabus.html#harassment-and-sexual-violence",
    "title": "Outline",
    "section": "Harassment and sexual violence",
    "text": "Harassment and sexual violence\nThe Center for Harassment Intervention (BIMH) is the unique access point for all members of the community subject to harassment or sexual violence. You can reach them at 514 343-7020 or by email at harcelement@hec.ca from Monday until Friday, from 8:30 until 4:30pm.\nIf you are in an emergency situation or fear for your safety, call emergency services at 911, followed by HEC Montréal security services at 514 340-6611.\nCheck the school official policy on these matters for more details."
  },
  {
    "objectID": "syllabus.html#family-policy",
    "href": "syllabus.html#family-policy",
    "title": "Outline",
    "section": "Family policy",
    "text": "Family policy\nHEC does not have an official family policy, so the following guidelines reflect my own beliefs and commitments towards parent students2\n\nBabies are welcome in class as often as necessary for support feeding relationship.\nYou are welcome to bring your child to class in order to cover unforeseeable gaps in childcare.\nIf you come with babies or toddler, I ask that you sit close to the door so that, in case your little one needs special attention and is disrupting the learning of other students, you may step outside of class until their needs are met. Seats close to the door are reserved for parents attending class with their child."
  }
]