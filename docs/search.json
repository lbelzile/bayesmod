[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Outline",
    "section": "",
    "text": "Dr. Léo Belzile\n   4.850, Côte-Sainte-Catherine\n   leo.belzile@hec.ca\n\n\n\n\n\n   Winter 2025\n   Monday\n   15:30-18:30\n   TBD"
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Outline",
    "section": "Textbooks",
    "text": "Textbooks\nI will assign readings from McElreath (2020) and Johnson et al. (2022), which is freely available online.\n\n\n\nStatistical Rethinking:A Bayesian Course with Examples in R and Stan\nR. McElreath\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Rules!An Introduction to Applied Bayesian Modeling\nA.A. Johnson, M.Q. Ott, and M. Dogucu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourse notes for the class can be found online"
  },
  {
    "objectID": "syllabus.html#other-references",
    "href": "syllabus.html#other-references",
    "title": "Outline",
    "section": "Other references",
    "text": "Other references\nThere will occasionally be additional articles to read; links to these other resources will be included on the content page for that session."
  },
  {
    "objectID": "syllabus.html#student-hours",
    "href": "syllabus.html#student-hours",
    "title": "Outline",
    "section": "Student hours",
    "text": "Student hours\nWednesday afternoon from 13:00 until 16:00 or by appointment. My office, 4.850, is located next to the southern elevators in Côte-Sainte-Catherine building.\nPlease watch this video:\n\n\nStudent hours are set times dedicated to all of you (most professors call these “office hours”; I don’t1). This means that I will be in my office waiting for you to come by if you want to talk to me in person (or remotely) with whatever questions you have. This is the best and easiest way to find me and the best chance for discussing class material and concerns."
  },
  {
    "objectID": "syllabus.html#intellectual-integrity",
    "href": "syllabus.html#intellectual-integrity",
    "title": "Outline",
    "section": "Intellectual integrity",
    "text": "Intellectual integrity\nPlease don’t cheat! The official policy lists the school rules regarding plagiarism and academic integrity."
  },
  {
    "objectID": "syllabus.html#student-services",
    "href": "syllabus.html#student-services",
    "title": "Outline",
    "section": "Student services",
    "text": "Student services\nStudents with special needs should feel free to approach me so we can best discuss accommodations. Do check out HEC Montréal’s disabled students and [psychological] (https://www.hec.ca/en/students/support-resources/psychological-support/index.html) support services."
  },
  {
    "objectID": "syllabus.html#harassment-and-sexual-violence",
    "href": "syllabus.html#harassment-and-sexual-violence",
    "title": "Outline",
    "section": "Harassment and sexual violence",
    "text": "Harassment and sexual violence\nThe Center for Harassment Intervention (BIMH) is the unique access point for all members of the community subject to harassment or sexual violence. You can reach them at 514 343-7020 or by email at harcelement@hec.ca from Monday until Friday, from 8:30 until 4:30pm.\nIf you are in an emergency situation or fear for your safety, call emergency services at 911, followed by HEC Montréal security services at 514 340-6611.\nCheck the school official policy on these matters for more details."
  },
  {
    "objectID": "syllabus.html#family-policy",
    "href": "syllabus.html#family-policy",
    "title": "Outline",
    "section": "Family policy",
    "text": "Family policy\nHEC now has an official family policy, but the following guidelines reflect my own beliefs and commitments towards parent students2\n\nBabies are welcome in class as often as necessary for support feeding relationship.\nYou are welcome to bring your child to class in order to cover unforeseeable gaps in childcare.\nIf you come with babies or toddler, I ask that you sit close to the door so that, in case your little one needs special attention and is disrupting the learning of other students, you may step outside of class until their needs are met. Seats close to the door are reserved for parents attending class with their child."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Outline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere’s fairly widespread misunderstanding about what office hours actually are! Many students often think that they are the times I shouldn’t be disturbed, which is the exact opposite of what they’re for!↩︎\nShamelessly stolen/adapted from similar policy by Drs. Melissa Cheney, Guy Grossman and Rohan Alexander↩︎"
  },
  {
    "objectID": "exercises/04-exercise.html",
    "href": "exercises/04-exercise.html",
    "title": "Exercise 4",
    "section": "",
    "text": "The Pareto distribution with shape \\(\\alpha&gt;0\\) and scale \\(\\tau&gt;0\\) has density \\[\nf(x; \\alpha, \\tau) = \\alpha x^{-\\alpha-1}\\tau^\\alpha \\mathsf{I}(x &gt; \\tau).\n\\] It can be used to model power laws in insurance and finance, or in demography. The uscitypopn data set in the hecbayes package contains the population size of cities above 200K inhabitants in the United States, from the 2020 census.\n\nUsing improper priors, write the joint posterior for a simple random sample of size \\(n\\) and derive the conditional distributions \\(p(\\alpha \\mid \\boldsymbol{y}, \\tau)\\) and \\(p(\\tau \\mid \\alpha, \\boldsymbol{y})\\). Hint: the conditional density \\(p(\\alpha \\mid \\boldsymbol{y}, \\tau)\\) is that of a gamma; use the fact that \\(m^\\alpha=\\exp\\{\\alpha\\log(m)\\}\\) for \\(m&gt;0\\).\nThe mononomial distribution \\(\\mathsf{Mono}(a,b)\\) has density \\(p(x) \\propto x^{a-1}\\mathsf{I}(0 \\leq x \\leq b)\\) for \\(a, b &gt; 0\\). Find the normalizing constant for the distribution and obtain the quantile function to derive a random number generator.\nImplement Gibbs sampling for this problem for the uscitypopn data. Draw enough observations to obtain an effective sample size of at least 1000 observations. Calculate the accuracy of your estimates.",
    "crumbs": [
      "Exercises",
      "Markov chain Monte Carlo",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/04-exercise.html#exercise-4.1",
    "href": "exercises/04-exercise.html#exercise-4.1",
    "title": "Exercise 4",
    "section": "",
    "text": "The Pareto distribution with shape \\(\\alpha&gt;0\\) and scale \\(\\tau&gt;0\\) has density \\[\nf(x; \\alpha, \\tau) = \\alpha x^{-\\alpha-1}\\tau^\\alpha \\mathsf{I}(x &gt; \\tau).\n\\] It can be used to model power laws in insurance and finance, or in demography. The uscitypopn data set in the hecbayes package contains the population size of cities above 200K inhabitants in the United States, from the 2020 census.\n\nUsing improper priors, write the joint posterior for a simple random sample of size \\(n\\) and derive the conditional distributions \\(p(\\alpha \\mid \\boldsymbol{y}, \\tau)\\) and \\(p(\\tau \\mid \\alpha, \\boldsymbol{y})\\). Hint: the conditional density \\(p(\\alpha \\mid \\boldsymbol{y}, \\tau)\\) is that of a gamma; use the fact that \\(m^\\alpha=\\exp\\{\\alpha\\log(m)\\}\\) for \\(m&gt;0\\).\nThe mononomial distribution \\(\\mathsf{Mono}(a,b)\\) has density \\(p(x) \\propto x^{a-1}\\mathsf{I}(0 \\leq x \\leq b)\\) for \\(a, b &gt; 0\\). Find the normalizing constant for the distribution and obtain the quantile function to derive a random number generator.\nImplement Gibbs sampling for this problem for the uscitypopn data. Draw enough observations to obtain an effective sample size of at least 1000 observations. Calculate the accuracy of your estimates.",
    "crumbs": [
      "Exercises",
      "Markov chain Monte Carlo",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/04-exercise.html#exercise-4.2",
    "href": "exercises/04-exercise.html#exercise-4.2",
    "title": "Exercise 4",
    "section": "Exercise 4.2",
    "text": "Exercise 4.2\nImplement the Bayesian LASSO for the diabetes cancer surgery from package lars. Check Park & Casella (2008) for the details of the Gibbs sampling.\n\nFit the model for a range of values of \\(\\lambda\\) and produce parameter estimate paths to replicate Figure 2 of the paper.\nCheck the effective sample size and comment on the mixing. Is it impacted by the tuning parameter?\nImplement the method of section 3.1 from Park & Casella (2008) by adding \\(\\lambda\\) as a parameter.\nFor three models with different values of \\(\\lambda\\), compute the widely applicable information criterion (WAIC) and use it to assess predictive performance.",
    "crumbs": [
      "Exercises",
      "Markov chain Monte Carlo",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/04-solution.html",
    "href": "exercises/04-solution.html",
    "title": "Solution 4",
    "section": "",
    "text": "The Pareto distribution with shape \\(\\alpha&gt;0\\) and scale \\(\\tau&gt;0\\) has density \\[\nf(x; \\alpha, \\tau) = \\alpha x^{-\\alpha-1}\\tau^\\alpha \\mathsf{I}(x &gt; \\tau).\n\\] It can be used to model power laws in insurance and finance, or in demography. The uscitypopn data set in the hecbayes package contains the population size of cities above 200K inhabitants in the United States, from the 2020 census.\n\nUsing improper priors, write the joint posterior for a simple random sample of size \\(n\\) and derive the conditional distributions \\(p(\\alpha \\mid \\boldsymbol{y}, \\tau)\\) and \\(p(\\tau \\mid \\alpha, \\boldsymbol{y})\\).\nThe mononomial distribution \\(\\mathsf{Mono}(a,b)\\) has density \\(p(x) \\propto x^{a-1}\\mathsf{I}(0 \\leq x \\leq b)\\). Find the normalizing constant for the distribution and obtain the quantile function to derive a sampler.\nImplement Gibbs sampling for this problem for the uscitypopn data. Draw enough observations to obtain an effective sample size of at least 1000 observations. Calculate the accuracy of your estimates?\n\n\nSolution. With improper prior, the joint posterior is the product of the likelihood contributions so \\[\np(\\alpha, \\tau \\mid \\boldsymbol{y}) \\propto \\alpha^n \\left(\\prod_{i=1}^n y_i\\right)^{-\\alpha-1} \\tau^{-n\\alpha} \\mathsf{I}(\\min_i y_i &gt; \\tau).\n\\] Using the hint, write the conditional density for \\(\\alpha\\) given the rest as \\[\\begin{align*}\np(\\alpha \\mid \\boldsymbol{y}, \\tau) \\propto \\alpha^n \\left( \\frac{\\prod_{i=1}^n y_i}{\\tau^n}\\right)^{-\\alpha} = \\alpha^{(n+1)-1} \\exp\\left\\{-\\alpha \\left(\\sum_{i=1}^n\\log y_i - n\\log \\tau\\right) \\right\\}\n\\end{align*}\\] which is \\(\\mathsf{Gamma}\\big(n+1, \\sum_{i=1}^n \\log y_i - n \\log \\tau \\big)\\). For the second, we have \\[\\begin{align*}\np(\\tau \\mid \\alpha, \\boldsymbol{y}) \\propto \\tau^{n\\alpha} \\mathsf{I}(\\min_{i} y_i &gt; \\tau),\n\\end{align*}\\] a mononomial distribution with parameters \\(a=n\\alpha+1\\) and \\(b = \\min_{i} y_i\\).\nTo find the normalizing constant of the mononomial distribution, we simply integrate the unnormalized density to obtain the reciprocal constant: if \\(c = \\int g(x) \\mathrm{d} x\\) for \\(c &lt; \\infty\\) and \\(g(x) \\geq 0\\) for all \\(x\\), then \\(g(x)/c\\) integrates to one and is a valid density. Thus, we find \\[c= \\int_0^b x^{a-1}\\mathrm{d} x = \\left[\\frac{x^{a}}{a}\\right]_{0}^b= \\frac{b^{a}}{a}.\\] The distribution function is \\(G(x) = (x/b)^{a}\\) for \\(x \\in [0,b]\\) and the quantile function \\(G^{-1}(u) = u^{1/a}b\\).\n\nqmono &lt;- function(u, a, b, log = FALSE){\n  stopifnot(isTRUE(all(a &gt; 0, b &gt; 0, u &gt;= 0, u &lt;= 1)))\n logq &lt;-   log(u)/(a+1) + log(b)\n if(log){ return(logq)} else { return(exp(logq)) }\n}\n\n\n# Load data\ndata(\"uscitypopn\", package = \"hecbayes\")\ny &lt;- uscitypopn$population\nn &lt;- length(y)\n# Summary statistics appearing in the posterior distribution\nsumlogy &lt;- sum(log(y))\nminy &lt;- min(y)\n# MCMC via Gibbs sampling\nB &lt;- 1e4L\nchains &lt;- matrix(0, nrow = B, ncol = 2)\ncolnames(chains) &lt;- c(\"alpha\", \"tau\")\ncurr &lt;- c(2, 2e5)\nfor(b in seq_len(B)){\n  chains[b,1] &lt;- curr[1] &lt;- rgamma(n = 1, shape = n+1, rate = sumlogy - n*log(curr[2]))\n  chains[b,2] &lt;- curr[2] &lt;- qmono(runif(1), a = n*curr[1]+1, b = miny)\n}\nchains &lt;- coda::as.mcmc(chains)\n# Compute effective sample size\ncoda::effectiveSize(chains)\n\n    alpha       tau \n 9590.174 10000.000 \n\nsummary(chains)\n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean        SD  Naive SE Time-series SE\nalpha 1.386e+00    0.1297  0.001297       0.001324\ntau   1.991e+05 1257.8213 12.578213      12.578213\n\n2. Quantiles for each variable:\n\n           2.5%       25%       50%      75%     97.5%\nalpha 1.142e+00 1.298e+00 1.382e+00 1.47e+00 1.651e+00\ntau   1.957e+05 1.986e+05 1.995e+05 2.00e+05 2.004e+05\n\n\nWe can see that the autocorrelation is minimal, so the sampler is quite efficient.",
    "crumbs": [
      "Exercises",
      "Markov chain Monte Carlo",
      "Solution 4"
    ]
  },
  {
    "objectID": "exercises/04-solution.html#exercise-4.1",
    "href": "exercises/04-solution.html#exercise-4.1",
    "title": "Solution 4",
    "section": "",
    "text": "The Pareto distribution with shape \\(\\alpha&gt;0\\) and scale \\(\\tau&gt;0\\) has density \\[\nf(x; \\alpha, \\tau) = \\alpha x^{-\\alpha-1}\\tau^\\alpha \\mathsf{I}(x &gt; \\tau).\n\\] It can be used to model power laws in insurance and finance, or in demography. The uscitypopn data set in the hecbayes package contains the population size of cities above 200K inhabitants in the United States, from the 2020 census.\n\nUsing improper priors, write the joint posterior for a simple random sample of size \\(n\\) and derive the conditional distributions \\(p(\\alpha \\mid \\boldsymbol{y}, \\tau)\\) and \\(p(\\tau \\mid \\alpha, \\boldsymbol{y})\\).\nThe mononomial distribution \\(\\mathsf{Mono}(a,b)\\) has density \\(p(x) \\propto x^{a-1}\\mathsf{I}(0 \\leq x \\leq b)\\). Find the normalizing constant for the distribution and obtain the quantile function to derive a sampler.\nImplement Gibbs sampling for this problem for the uscitypopn data. Draw enough observations to obtain an effective sample size of at least 1000 observations. Calculate the accuracy of your estimates?\n\n\nSolution. With improper prior, the joint posterior is the product of the likelihood contributions so \\[\np(\\alpha, \\tau \\mid \\boldsymbol{y}) \\propto \\alpha^n \\left(\\prod_{i=1}^n y_i\\right)^{-\\alpha-1} \\tau^{-n\\alpha} \\mathsf{I}(\\min_i y_i &gt; \\tau).\n\\] Using the hint, write the conditional density for \\(\\alpha\\) given the rest as \\[\\begin{align*}\np(\\alpha \\mid \\boldsymbol{y}, \\tau) \\propto \\alpha^n \\left( \\frac{\\prod_{i=1}^n y_i}{\\tau^n}\\right)^{-\\alpha} = \\alpha^{(n+1)-1} \\exp\\left\\{-\\alpha \\left(\\sum_{i=1}^n\\log y_i - n\\log \\tau\\right) \\right\\}\n\\end{align*}\\] which is \\(\\mathsf{Gamma}\\big(n+1, \\sum_{i=1}^n \\log y_i - n \\log \\tau \\big)\\). For the second, we have \\[\\begin{align*}\np(\\tau \\mid \\alpha, \\boldsymbol{y}) \\propto \\tau^{n\\alpha} \\mathsf{I}(\\min_{i} y_i &gt; \\tau),\n\\end{align*}\\] a mononomial distribution with parameters \\(a=n\\alpha+1\\) and \\(b = \\min_{i} y_i\\).\nTo find the normalizing constant of the mononomial distribution, we simply integrate the unnormalized density to obtain the reciprocal constant: if \\(c = \\int g(x) \\mathrm{d} x\\) for \\(c &lt; \\infty\\) and \\(g(x) \\geq 0\\) for all \\(x\\), then \\(g(x)/c\\) integrates to one and is a valid density. Thus, we find \\[c= \\int_0^b x^{a-1}\\mathrm{d} x = \\left[\\frac{x^{a}}{a}\\right]_{0}^b= \\frac{b^{a}}{a}.\\] The distribution function is \\(G(x) = (x/b)^{a}\\) for \\(x \\in [0,b]\\) and the quantile function \\(G^{-1}(u) = u^{1/a}b\\).\n\nqmono &lt;- function(u, a, b, log = FALSE){\n  stopifnot(isTRUE(all(a &gt; 0, b &gt; 0, u &gt;= 0, u &lt;= 1)))\n logq &lt;-   log(u)/(a+1) + log(b)\n if(log){ return(logq)} else { return(exp(logq)) }\n}\n\n\n# Load data\ndata(\"uscitypopn\", package = \"hecbayes\")\ny &lt;- uscitypopn$population\nn &lt;- length(y)\n# Summary statistics appearing in the posterior distribution\nsumlogy &lt;- sum(log(y))\nminy &lt;- min(y)\n# MCMC via Gibbs sampling\nB &lt;- 1e4L\nchains &lt;- matrix(0, nrow = B, ncol = 2)\ncolnames(chains) &lt;- c(\"alpha\", \"tau\")\ncurr &lt;- c(2, 2e5)\nfor(b in seq_len(B)){\n  chains[b,1] &lt;- curr[1] &lt;- rgamma(n = 1, shape = n+1, rate = sumlogy - n*log(curr[2]))\n  chains[b,2] &lt;- curr[2] &lt;- qmono(runif(1), a = n*curr[1]+1, b = miny)\n}\nchains &lt;- coda::as.mcmc(chains)\n# Compute effective sample size\ncoda::effectiveSize(chains)\n\n    alpha       tau \n 9590.174 10000.000 \n\nsummary(chains)\n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean        SD  Naive SE Time-series SE\nalpha 1.386e+00    0.1297  0.001297       0.001324\ntau   1.991e+05 1257.8213 12.578213      12.578213\n\n2. Quantiles for each variable:\n\n           2.5%       25%       50%      75%     97.5%\nalpha 1.142e+00 1.298e+00 1.382e+00 1.47e+00 1.651e+00\ntau   1.957e+05 1.986e+05 1.995e+05 2.00e+05 2.004e+05\n\n\nWe can see that the autocorrelation is minimal, so the sampler is quite efficient.",
    "crumbs": [
      "Exercises",
      "Markov chain Monte Carlo",
      "Solution 4"
    ]
  },
  {
    "objectID": "exercises/04-solution.html#exercise-4.2",
    "href": "exercises/04-solution.html#exercise-4.2",
    "title": "Solution 4",
    "section": "Exercise 4.2",
    "text": "Exercise 4.2\nImplement the Bayesian LASSO for the diabetes cancer surgery from package lars. Check Park & Casella (2008) for the details of the Gibbs sampling.\n\nFit the model for a range of values of \\(\\lambda\\) and produce parameter estimate paths to replicate Figure 2 of the paper.\nCheck the effective sample size and comment on the mixing. Is it impacted by the tuning parameter?\nImplement the method of section 3.1 from Park & Casella (2008) by adding \\(\\lambda\\) as a parameter.\nFor three models with different values of \\(\\lambda\\), compute the widely applicable information criterion (WAIC) and use it to assess predictive performance.\n\n\nSolution. We first setup a Gibbs sampler for a given value of \\(\\lambda\\), or using the empirical Bayes estimator provided in section 3.1. The effective sampling size for fixed \\(\\lambda\\) is good. If we let the parameter varies, the performance degrades and we obtain an effective size shy of 1000 for 10K iterations for \\(\\lambda\\), and comfortably above 5000 for others.\n\ndata(diabetes, package = \"lars\")\nbayeslasso &lt;- function(lambda = NULL, \n                       B = 1e4L,\n                       x = diabetes$x, \n                       y = diabetes$y){\n  stopifnot(is.matrix(x), is.vector(y))\n  # Scale inputs in case\n  x &lt;- scale(x, center = TRUE, scale = FALSE)\n  y &lt;- y - mean(y)\n  # Check method\n  if(is.null(lambda)){\n    method &lt;- \"empbayes\"\n  } else{\n    method &lt;- \"fixed\" \n  }\n  burnin &lt;- 250L\n  # Precompute quantities and dimensions\n  xtx &lt;- crossprod(x)\n  p &lt;- ncol(x)\n  n &lt;- nrow(x)\n  # Obtain initial estimates\n  linmod &lt;- lm(y ~ x - 1)\n  betaols &lt;- coef(linmod)\n  beta.curr &lt;- betaols\n  sigmasq.curr &lt;- mean(residuals(linmod)^2)\n  tausqinv.curr &lt;- rep(1, p)\n  # Value reported in the text for the optimal parameter: lambda = 0.237\n  beta.ind &lt;- 1:p\n  sigmasq.ind &lt;- p + 1L\n  tausq.ind &lt;- seq(from = p + 2L, length.out = p, by = 1L)\n  chains &lt;- matrix(0, nrow = B, ncol = p + 1 + p + \n                     ifelse(method == \"fixed\", 0,1))\n  if(method == \"fixed\"){\n    colnames(chains) &lt;- c(paste0(\"beta\", 1:p), \"sigmasq\",\n                          paste0(\"tausq\", 1:p))\n    lambdasq.curr &lt;- lambda[1]^2\n  } else{\n    colnames(chains) &lt;- c(paste0(\"beta\", 1:p), \"sigmasq\", \n                          paste0(\"tausq\", 1:p), \"lambda\")\n    lambdasq.curr &lt;- p*sqrt(sigmasq.curr)/sum(abs(betaols))\n    lambdasq.ind &lt;- ncol(chains)\n  }\n# MCMC loop\nfor(b in seq_len(B + burnin)){\n  ind &lt;- pmax(1, b-burnin)\n  Ainv &lt;- solve(xtx + diag(tausqinv.curr))\n  beta.curr &lt;- chains[ind,beta.ind] &lt;- as.numeric(\n    mvtnorm::rmvnorm(\n      n = 1, \n      mean = as.numeric(Ainv %*% t(x) %*% y), \n      sigma = sigmasq.curr*Ainv))\n  sigmasq.curr &lt;- chains[ind, sigmasq.ind] &lt;- 1/rgamma(\n    n = 1, \n    shape = (n-1+p)/2,\n    rate = sum((y-x %*% beta.curr)^2)/2 + \n      sum(beta.curr^2*tausqinv.curr)/2)\n  # Compute marginal posterior mean for lambda, using section 3.1\n  sumexpect &lt;- 0\n  for(j in 1:p){\n    tausqinv.curr[j] &lt;- actuar::rinvgauss(\n      n = 1, \n      mean = sqrt(lambdasq.curr*sigmasq.curr)/abs(beta.curr[j]),\n      dispersion = 1/lambdasq.curr)\n    if(method != \"fixed\"){\n    sumexpect &lt;- sumexpect + mean(1/actuar::rinvgauss(\n      n = 1000, \n      mean = sqrt(lambdasq.curr*sigmasq.curr)/abs(beta.curr[j]),\n      dispersion = 1/lambdasq.curr))\n    }\n  }\n  chains[ind, tausq.ind] &lt;- 1/tausqinv.curr\n  if(method != \"fixed\"){\n    lambdasq.curr &lt;- chains[ind, lambdasq.ind] &lt;- 2*p/sumexpect\n  }\n}\n  if(method != \"fixed\"){\n  chains[, lambdasq.ind] &lt;- sqrt(chains[, lambdasq.ind])\n}\n# Cast Markov chains to mcmc class object.\nchains.mcmc &lt;- coda::as.mcmc(chains)\n# Effective sample size\ness &lt;- as.integer(round(coda::effectiveSize(chains.mcmc), 0))\n\n# Compute WAIC from log pointwise density\nlppd &lt;- 0\npenalty &lt;- 0\nfor(i in seq_len(n)){\n  lppd_i &lt;- dnorm(\n    x = y[i], \n    mean = as.numeric(chains[,beta.ind] %*% c(x[i,])), \n    sd = sqrt(chains[,sigmasq.ind]), \n    log = TRUE)\n  lppd &lt;- lppd + mean(lppd_i)\n  penalty &lt;- penalty + var(lppd_i)\n}\nwaic &lt;- (-lppd + penalty)/n\nl1norm &lt;- mean(rowSums(abs(chains[,beta.ind])))\n\n# Parameter estimates and 95% equitailed credible intervals\nquant &lt;- t(apply(chains, 2, \n                quantile, prob = c(0.025, 0.5, 0.975)))\nregpar &lt;- as.data.frame(cbind(quant,\n                colMeans(chains),\n                coda::batchSE(chains.mcmc),\n                ess))\nregpar$pars &lt;- rownames(quant)\nrownames(regpar) &lt;- NULL\ncolnames(regpar) &lt;- c(\"lower\", \"median\", \"upper\", \n                      \"mean\", \"se\", \"ess\", \"par\")\nregpar &lt;- regpar[,c(7,4:5,1:3,6)]\nattr(regpar, \"waic\") &lt;- waic\nattr(regpar, \"l1norm\") &lt;- l1norm\n return(regpar)\n}\n\n# Call the MCMC sampler\nset.seed(2023)\nlasso_empbayes &lt;- bayeslasso(lambda = NULL)\n# Extract the value of WAIC\nwaic &lt;- attr(lasso_empbayes, \"waic\")\n\n\n\n\n\n\n\n\n\nFigure 1: Standardized median posterior estimates of the coefficients for the Bayesian LASSO with 95 percent equitailed credible intervals, with \\(\\lambda\\) estimated using empirical Bayes. Ordinary least square estimates are denoted by crosses.\n\n\n\n\n\nThe plot corresponds to Figure 2 of Park & Casella (2008) and the posterior summaries, reported in Table 1, are also in line with those of the paper.\n\n\n\n\nTable 1: Estimates posterior summaries from the Bayesian LASSO, based on 10K draws. Posterior means and adjusted standard errors, posterior median and equitailed 95 percent credible intervals, effective sample.\n\n\n\n\n\n\npar\nmean\nse\nlower\nmedian\nupper\ness\n\n\n\n\nbeta1\n-3.14\n0.58\n-112.92\n-2.34\n105.68\n10000\n\n\nbeta2\n-214.04\n0.58\n-331.89\n-213.49\n-97.58\n9012\n\n\nbeta3\n523.41\n0.76\n393.47\n523.87\n656.33\n9062\n\n\nbeta4\n307.64\n0.72\n177.38\n307.69\n439.33\n8957\n\n\nbeta5\n-187.12\n2.60\n-587.60\n-170.71\n125.58\n5089\n\n\nbeta6\n7.79\n1.92\n-272.14\n-1.62\n344.59\n6056\n\n\nbeta7\n-154.43\n1.62\n-385.12\n-153.23\n72.40\n5259\n\n\nbeta8\n96.44\n1.60\n-131.62\n87.10\n353.40\n6460\n\n\nbeta9\n524.09\n1.26\n331.93\n520.90\n727.05\n6630\n\n\nbeta10\n64.21\n0.69\n-49.05\n61.87\n188.78\n8897\n\n\nsigmasq\n2952.78\n2.00\n2585.93\n2945.02\n3370.20\n9511\n\n\ntausq1\n20.55\n0.26\n0.24\n11.13\n93.70\n10000\n\n\ntausq2\n34.75\n0.33\n3.42\n25.19\n122.06\n8868\n\n\ntausq3\n58.44\n0.36\n13.42\n49.09\n155.41\n9375\n\n\ntausq4\n41.99\n0.39\n6.09\n32.35\n133.19\n8972\n\n\ntausq5\n34.17\n0.45\n1.03\n24.45\n122.56\n5159\n\n\ntausq6\n26.82\n0.38\n0.57\n17.18\n105.39\n7441\n\n\ntausq7\n31.06\n0.35\n1.04\n21.46\n118.21\n8954\n\n\ntausq8\n27.47\n0.33\n0.56\n17.94\n107.05\n8551\n\n\ntausq9\n59.02\n0.47\n12.95\n49.73\n160.42\n7805\n\n\ntausq10\n23.35\n0.30\n0.40\n13.66\n102.21\n9703\n\n\nlambda\n0.24\n0.00\n0.21\n0.24\n0.26\n1038\n\n\n\n\n\n\n\n\nFor the last part, we can simply run the MCMC and find the value of \\(\\lambda\\) that yields the lowest value of WAIC.\n\nset.seed(2023)\nblasso1 &lt;- bayeslasso(lambda = 0.1)\nblasso2 &lt;- bayeslasso(lambda = 0.2)\nblasso3 &lt;- bayeslasso(lambda = 1)\n2*length(diabetes$y)*c(attr(blasso1, \"waic\"), attr(blasso2, \"waic\"), attr(blasso3, \"waic\"))\n\n[1] 4802.449 4801.923 4804.496",
    "crumbs": [
      "Exercises",
      "Markov chain Monte Carlo",
      "Solution 4"
    ]
  },
  {
    "objectID": "slides/bayesmod-slides1.html#probability-vs-frequency",
    "href": "slides/bayesmod-slides1.html#probability-vs-frequency",
    "title": "Bayesian modelling",
    "section": "Probability vs frequency",
    "text": "Probability vs frequency\nIn frequentist statistic, “probability” is synonym for\n\n\n\nlong-term frequency under repeated sampling"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#what-is-probability",
    "href": "slides/bayesmod-slides1.html#what-is-probability",
    "title": "Bayesian modelling",
    "section": "What is probability?",
    "text": "What is probability?\nProbability reflects incomplete information.\nQuoting Finetti (1974)\n\nProbabilistic reasoning — always to be understood as subjective — merely stems from our being uncertain about something."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#why-opt-for-the-bayesian-paradigm",
    "href": "slides/bayesmod-slides1.html#why-opt-for-the-bayesian-paradigm",
    "title": "Bayesian modelling",
    "section": "Why opt for the Bayesian paradigm?",
    "text": "Why opt for the Bayesian paradigm?\n\nSatisfies the likelihood principle\nGenerative approach naturally extends to complex settings (hierarchical models)\nUncertainty quantification and natural framework for prediction\nCapability to incorporate subject-matter expertise"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#bayesian-versus-frequentist",
    "href": "slides/bayesmod-slides1.html#bayesian-versus-frequentist",
    "title": "Bayesian modelling",
    "section": "Bayesian versus frequentist",
    "text": "Bayesian versus frequentist\n\n\nFrequentist\n\nParameters treated as fixed, data as random\n\ntrue value of parameter \\(\\boldsymbol{\\theta}\\) is unknown.\n\nTarget is point estimator\n\n\nBayesian\n\nBoth parameters and data are random\n\ninference is conditional on observed data\n\nTarget is a distribution"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#joint-and-marginal-distribution",
    "href": "slides/bayesmod-slides1.html#joint-and-marginal-distribution",
    "title": "Bayesian modelling",
    "section": "Joint and marginal distribution",
    "text": "Joint and marginal distribution\nThe joint density of data \\(\\boldsymbol{Y}\\) and parameters \\(\\boldsymbol{\\theta}\\) is\n\\[\\begin{align*}\np(\\boldsymbol{Y}, \\boldsymbol{\\theta}) = p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) =  p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y}) p(\\boldsymbol{Y})\n\\end{align*}\\] where the marginal \\(p(\\boldsymbol{Y}) = \\int_{\\boldsymbol{\\Theta}} p(\\boldsymbol{Y}, \\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\\)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#posterior",
    "href": "slides/bayesmod-slides1.html#posterior",
    "title": "Bayesian modelling",
    "section": "Posterior",
    "text": "Posterior\nUsing Bayes’ theorem, the posterior density is\n\\[\\begin{align*}\n\\color{#D55E00}{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})} = \\frac{\\color{#0072B2}{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})} \\times  \\color{#56B4E9}{p(\\boldsymbol{\\theta})}}{\\color{#E69F00}{\\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\\mathrm{d} \\boldsymbol{\\theta}}},\n\\end{align*}\\]\nmeaning that \\[\\color{#D55E00}{\\text{posterior}} \\propto \\color{#0072B2}{\\text{likelihood}} \\times \\color{#56B4E9}{\\text{prior}}\\]\n\n\n\nEvaluating the marginal likelihood \\(\\color{#E69F00}{p(\\boldsymbol{Y})}\\), is challenging when \\(\\boldsymbol{\\theta}\\) is high-dimensional."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#updating-beliefs-and-sequentiality",
    "href": "slides/bayesmod-slides1.html#updating-beliefs-and-sequentiality",
    "title": "Bayesian modelling",
    "section": "Updating beliefs and sequentiality",
    "text": "Updating beliefs and sequentiality\nBy Bayes’ rule, we can consider updating the posterior by adding terms to the likelihood, noting that for independent \\(\\boldsymbol{y}_1\\) and \\(\\boldsymbol{y}_2\\), \\[\\begin{align*}\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\boldsymbol{y}_2) \\propto p(\\boldsymbol{y}_2 \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1)\n\\end{align*}\\] The posterior is be updated in light of new information."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#binomial-distribution",
    "href": "slides/bayesmod-slides1.html#binomial-distribution",
    "title": "Bayesian modelling",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nA binomial variable with probability of success \\(\\theta \\in [0,1]\\) has mass function \\[\\begin{align*}\nf(y; \\theta) = \\binom{n}{y} \\theta^y (1-\\theta)^{n-y}, \\qquad y = 0, \\ldots, n.\n\\end{align*}\\] Moments of the number of successes out of \\(n\\) trials are \\[\\mathsf{E}(Y \\mid \\theta) = n \\theta, \\quad \\mathsf{Va}(Y \\mid \\theta) = n \\theta(1-\\theta).\\]\n\n\nThe binomial coefficient \\(\\binom{n}{y}=n!/\\{(n-y)!y!\\}\\), where \\(n!=\\Gamma(n+1)\\)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#beta-distribution",
    "href": "slides/bayesmod-slides1.html#beta-distribution",
    "title": "Bayesian modelling",
    "section": "Beta distribution",
    "text": "Beta distribution\nThe beta distribution with shapes \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\), denoted \\(\\mathsf{Be}(\\alpha,\\beta)\\), has density \\[f(y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}y^{\\alpha - 1}(1-y)^{\\beta - 1}, \\qquad y \\in [0,1]\\]\n\nexpectation: \\(\\alpha/(\\alpha+\\beta)\\);\nmode \\((\\alpha-1)/(\\alpha+\\beta-2)\\) if \\(\\alpha, \\beta&gt;1\\), else, \\(0\\), \\(1\\) or none;\nvariance: \\(\\alpha\\beta/\\{(\\alpha+\\beta)^2(\\alpha+\\beta+1)\\}\\).\n\n\nIt is a continuous distribution over the unit interval.\nThe uniform is a special case when both shapes are unity."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#beta-binomial-example",
    "href": "slides/bayesmod-slides1.html#beta-binomial-example",
    "title": "Bayesian modelling",
    "section": "Beta-binomial example",
    "text": "Beta-binomial example\nWe write \\(Y \\sim \\mathsf{Bin}(n, \\theta)\\) for \\(\\theta \\in [0,1]\\); the likelihood is \\[L(\\theta; y) = \\binom{n}{y} \\theta^y(1-\\theta)^{n-y}.\\]\nConsider a beta prior, \\(\\theta \\sim \\mathsf{Be}(\\alpha, \\beta)\\), with density \\[\np(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta) }\\theta^{\\alpha-1}(1-\\theta)^{\\beta - 1}.\n\\]"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#density-versus-likelihood",
    "href": "slides/bayesmod-slides1.html#density-versus-likelihood",
    "title": "Bayesian modelling",
    "section": "Density versus likelihood",
    "text": "Density versus likelihood\nThe binomial distribution is discrete with support \\(0, \\ldots, n\\), whereas the likelihood is continuous over \\(\\theta \\in [0,1]\\).\n\n\n\n\n\n\n\n\nFigure 1: Binomial density function (left) and scaled likelihood function (right).\n\n\n\n\n\n\n\nIf the density or mass function integrates to 1 over the range of \\(Y\\), the integral of the likelihood over \\(\\theta\\) does not."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#proportionality",
    "href": "slides/bayesmod-slides1.html#proportionality",
    "title": "Bayesian modelling",
    "section": "Proportionality",
    "text": "Proportionality\nAny term not a function of \\(\\theta\\) can be dropped, since it will absorbed by the normalizing constant. The posterior density is proportional to\n\\[\\begin{align*}\nL(\\theta; y)p(\\theta) & \\stackrel{\\theta}{\\propto} \\theta^{y}(1-\\theta)^{n-y} \\times \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}\n\\\\& =\\theta^{y + \\alpha - 1}(1-\\theta)^{n-y + \\beta - 1}\n\\end{align*}\\] the kernel of a beta density with shape parameters \\(y + \\alpha\\) and \\(n-y + \\beta\\).\n\n\nThe symbol \\(\\propto\\), for proportionality, means dropping all terms not an argument of the left hand side."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#experiments-and-likelihoods",
    "href": "slides/bayesmod-slides1.html#experiments-and-likelihoods",
    "title": "Bayesian modelling",
    "section": "Experiments and likelihoods",
    "text": "Experiments and likelihoods\nConsider the following sampling mechanism, which lead to \\(k\\) successes out of \\(n\\) independent trials, with the same probability of success \\(\\theta\\).\n\nBernoulli: sample fixed number of observations with \\(L(\\theta; y) =\\theta^k(1-\\theta)^{n-k}\\)\nbinomial: same, but record only total number of successes so \\(L(\\theta; y) =\\binom{n}{k}\\theta^k(1-\\theta)^{n-k}\\)\nnegative binomial: sample data until you obtain a predetermined number of successes, whence \\(L(\\theta; y) =\\binom{n-1}{k-1}\\theta^k(1-\\theta)^{n-k}\\)"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#likelihood-principle",
    "href": "slides/bayesmod-slides1.html#likelihood-principle",
    "title": "Bayesian modelling",
    "section": "Likelihood principle",
    "text": "Likelihood principle\nTwo likelihoods that are proportional, up to a constant not depending on unknown parameters, yield the same evidence.\nIn all cases, \\(L(\\theta; y) \\stackrel{\\theta}{\\propto} \\theta^k(1-\\theta)^{n-k}\\), so these yield the same inference for Bayesian.\n\n\nFor a more in-depth discussion, see Section 6.3.2 of Casella & Berger (2002)"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#integration",
    "href": "slides/bayesmod-slides1.html#integration",
    "title": "Bayesian modelling",
    "section": "Integration",
    "text": "Integration\nWe could approximate the \\(\\color{#E69F00}{\\text{marginal likelihood}}\\) through either\n\nnumerical integration (cubature)\nMonte Carlo simulations\n\nIn more complicated models, we will try to sample observations by bypassing completely this calculation.\n\n\nThe likelihood terms can be small (always less than one and decreasing for discrete data), so watch out for numerical overflow when evaluating normalizing constants."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#numerical-example-of-monte-carlo-integration",
    "href": "slides/bayesmod-slides1.html#numerical-example-of-monte-carlo-integration",
    "title": "Bayesian modelling",
    "section": "Numerical example of (Monte Carlo) integration",
    "text": "Numerical example of (Monte Carlo) integration\n\ny &lt;- 6L # number of successes \nn &lt;- 14L # number of trials\nalpha &lt;- beta &lt;- 1.5 # prior parameters\nunnormalized_posterior &lt;- function(theta){\n  theta^(y+alpha-1) * (1-theta)^(n-y + beta - 1)\n}\nintegrate(f = unnormalized_posterior,\n          lower = 0,\n          upper = 1)\n## 1.07e-05 with absolute error &lt; 1e-12\n# Compare with known constant\nbeta(y + alpha, n - y + beta)\n## [1] 1.07e-05\n# Monte Carlo integration\nmean(unnormalized_posterior(runif(1e5)))\n## [1] 1.06e-05"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#prior-likelihood-and-posterior",
    "href": "slides/bayesmod-slides1.html#prior-likelihood-and-posterior",
    "title": "Bayesian modelling",
    "section": "Prior, likelihood and posterior",
    "text": "Prior, likelihood and posterior\n\n\nFigure 2: Scaled Binomial likelihood for six successes out of 14 trials, \\(\\mathsf{Beta}(3/2, 3/2)\\) prior and corresponding posterior distribution from a beta-binomial model."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#proper-prior",
    "href": "slides/bayesmod-slides1.html#proper-prior",
    "title": "Bayesian modelling",
    "section": "Proper prior",
    "text": "Proper prior\nWe could define the posterior simply as the normalized product of the likelihood and some prior function.\nThe prior function need not even be proportional to a density function (i.e., integrable as a function of \\(\\boldsymbol{\\theta}\\)).\nFor example,\n\n\\(p(\\theta) \\propto \\theta^{-1}(1-\\theta)^{-1}\\) is improper because it is not integrable.\n\\(p(\\theta) \\propto 1\\) is a proper prior over \\([0,1]\\) (uniform)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#validity-of-the-posterior",
    "href": "slides/bayesmod-slides1.html#validity-of-the-posterior",
    "title": "Bayesian modelling",
    "section": "Validity of the posterior",
    "text": "Validity of the posterior\n\nThe marginal likelihood does not depend on \\(\\boldsymbol{\\theta}\\)\n\n(a normalizing constant)\n\nFor the posterior density to be proper,\n\nthe marginal likelihood must be a finite!\nin continuous models, the posterior is proper whenever the prior function is proper."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#different-priors-give-different-posteriors",
    "href": "slides/bayesmod-slides1.html#different-priors-give-different-posteriors",
    "title": "Bayesian modelling",
    "section": "Different priors give different posteriors",
    "text": "Different priors give different posteriors\n\n\nFigure 3: Scaled binomial likelihood for six successes out of 14 trials, with \\(\\mathsf{Beta}(3/2, 3/2)\\) prior (left), \\(\\mathsf{Beta}(1/4, 1/4)\\) (middle) and truncated uniform on \\([0,1/2]\\) (right), with the corresponding posterior distributions."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#role-of-the-prior",
    "href": "slides/bayesmod-slides1.html#role-of-the-prior",
    "title": "Bayesian modelling",
    "section": "Role of the prior",
    "text": "Role of the prior\nThe posterior is beta, with expected value \\[\\begin{align*}\n\\mathsf{E}(\\theta \\mid y) &= w\\frac{y}{n} + (1-w) \\frac{\\alpha}{\\alpha + \\beta}, \\\\ w&=\\frac{n}{n+\\alpha+\\beta}\n\\end{align*}\\] a weighted average of\n\nthe maximum likelihood estimator and\nthe prior mean.\n\n\nWe can think of the parameter \\(\\alpha\\) (respectively \\(\\beta\\)) as representing the fixed prior number of success (resp. failures)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#posterior-concentration",
    "href": "slides/bayesmod-slides1.html#posterior-concentration",
    "title": "Bayesian modelling",
    "section": "Posterior concentration",
    "text": "Posterior concentration\nExcept for stubborn priors, the likelihood contribution dominates in large samples. The impact of the prior is then often negligible.\n\n\nFigure 4: Beta posterior and binomial likelihood with a uniform prior for increasing number of observations (from left to right)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#summarizing-posterior-distributions",
    "href": "slides/bayesmod-slides1.html#summarizing-posterior-distributions",
    "title": "Bayesian modelling",
    "section": "Summarizing posterior distributions",
    "text": "Summarizing posterior distributions\n\nThe output of the Bayesian learning will be either of:\n\na fully characterized distribution (in toy examples).\na numerical approximation to the posterior distribution.\nan exact or approximate sample drawn from the posterior distribution.\n\n\nThe first case, which we have already encountered, allows us to query moments (mean, median, mode) directly provided there are analytical expressions for the latter, or else we could simulate from the model."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#bayesian-inference-in-practice",
    "href": "slides/bayesmod-slides1.html#bayesian-inference-in-practice",
    "title": "Bayesian modelling",
    "section": "Bayesian inference in practice",
    "text": "Bayesian inference in practice\nMost of the field revolves around the creation of algorithms that either\n\ncircumvent the calculation of the normalizing constant\n\n(Monte Carlo and Markov chain Monte Carlo methods)\n\nprovide accurate numerical approximation, including for marginalizing out all but one parameter.\n\n(integrated nested Laplace approximations, variational inference, etc.)"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#predictive-distributions",
    "href": "slides/bayesmod-slides1.html#predictive-distributions",
    "title": "Bayesian modelling",
    "section": "Predictive distributions",
    "text": "Predictive distributions\nDefine the \\(\\color{#D55E00}{\\text{posterior predictive}}\\), \\[\\begin{align*}\np(y_{\\text{new}}\\mid \\boldsymbol{y}) = \\int_{\\boldsymbol{\\Theta}} p(y_{\\text{new}} \\mid \\boldsymbol{\\theta}) \\color{#D55E00}{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})} \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\] and the \\(\\color{#56B4E9}{\\text{prior predictive}}\\) \\[\\begin{align*}\np(y_{\\text{new}}) = \\int_{\\boldsymbol{\\Theta}} p(y_{\\text{new}} \\mid \\boldsymbol{\\theta}) \\color{#56B4E9}{p(\\boldsymbol{\\theta})} \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\] is useful for determining whether the prior is sensical."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#analytical-derivation-of-predictive-distribution",
    "href": "slides/bayesmod-slides1.html#analytical-derivation-of-predictive-distribution",
    "title": "Bayesian modelling",
    "section": "Analytical derivation of predictive distribution",
    "text": "Analytical derivation of predictive distribution\nGiven the \\(\\mathsf{Be}(a, b)\\) prior or posterior, the predictive for \\(n_{\\text{new}}\\) trials is beta-binomial with density \\[\\begin{align*}\np(y_{\\text{new}}\\mid y) &= \\int_0^1 \\binom{n_{\\text{new}}}{y_{\\text{new}}} \\frac{\\theta^{a + y_{\\text{new}}-1}(1-\\theta)^{b + k - y_{\\text{new}}-1}}{\n\\mathrm{Be}(a, b)}\\mathrm{d} \\theta\n\\\\&= \\binom{n_{\\text{new}}}{y_{\\text{new}}} \\frac{\\mathrm{Be}(a + y_{\\text{new}}, b + n_{\\text{new}} - y_{\\text{new}})}{\\mathrm{Be}(a, b)}\n\\end{align*}\\]\nReplace \\(a=y + \\alpha\\) and \\(b=n-y + \\beta\\) to get the posterior predictive distribution."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#posterior-predictive-distribution",
    "href": "slides/bayesmod-slides1.html#posterior-predictive-distribution",
    "title": "Bayesian modelling",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\n\n\nFigure 5: Beta-binomial posterior predictive distribution with corresponding binomial mass function evaluated at the maximum likelihood estimator."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#posterior-predictive-distribution-via-simulation",
    "href": "slides/bayesmod-slides1.html#posterior-predictive-distribution-via-simulation",
    "title": "Bayesian modelling",
    "section": "Posterior predictive distribution via simulation",
    "text": "Posterior predictive distribution via simulation\nThe posterior predictive carries over the parameter uncertainty so will typically be wider and overdispersed relative to the corresponding distribution.\nGiven a draw \\(\\theta^*\\) from the posterior, simulate a new observation from the distribution \\(f(y_{\\text{new}}; \\theta^*)\\).\n\nnpost &lt;- 1e4L\n# Sample draws from the posterior distribution\npost_samp &lt;- rbeta(n = npost, y + alpha, n - y + beta)\n# For each draw, sample new observation\npost_pred &lt;- rbinom(n = npost, size = n, prob = post_samp)\n\n\n\nThe beta-binomial is used to model overdispersion in binary regression models."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#summarizing-posterior-distributions-1",
    "href": "slides/bayesmod-slides1.html#summarizing-posterior-distributions-1",
    "title": "Bayesian modelling",
    "section": "Summarizing posterior distributions",
    "text": "Summarizing posterior distributions\nThe output of a Bayesian procedure is a distribution for the parameters given the data.\nWe may wish to return different numerical summaries (expected value, variance, mode, quantiles, …)\nThe question: which point estimator to return?"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#decision-theory-and-loss-functions",
    "href": "slides/bayesmod-slides1.html#decision-theory-and-loss-functions",
    "title": "Bayesian modelling",
    "section": "Decision theory and loss functions",
    "text": "Decision theory and loss functions\nA loss function \\(c(\\boldsymbol{\\theta}, \\boldsymbol{\\upsilon}): \\boldsymbol{\\Theta} \\mapsto \\mathbb{R}^k\\) assigns a weight to each value \\(\\boldsymbol{\\theta}\\), corresponding to the regret or loss.\nThe point estimator \\(\\widehat{\\boldsymbol{\\upsilon}}\\) is the minimizer of the expected loss \\[\\begin{align*}\n\\widehat{\\boldsymbol{\\upsilon}} &= \\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\upsilon}}\\mathsf{E}_{\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y}}\\{c(\\boldsymbol{\\theta}, \\boldsymbol{v})\\} \\\\&=\\mathop{\\mathrm{argmin}}_{\\boldsymbol{\\upsilon}} \\int_{\\boldsymbol{\\Theta}} c(\\boldsymbol{\\theta}, \\boldsymbol{\\upsilon})p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#point-estimators-and-loss-functions",
    "href": "slides/bayesmod-slides1.html#point-estimators-and-loss-functions",
    "title": "Bayesian modelling",
    "section": "Point estimators and loss functions",
    "text": "Point estimators and loss functions\nIn a univariate setting, the most widely used point estimators are\n\nmean: quadratic loss \\(c(\\theta, \\upsilon) = (\\theta-\\upsilon)^2\\)\nmedian: absolute loss \\(c(\\theta, \\upsilon)=|\\theta - \\upsilon|\\)\nmode: 0-1 loss \\(c(\\theta, \\upsilon) = 1-\\mathrm{I}(\\upsilon = \\theta)\\)\n\nThe posterior mode \\(\\boldsymbol{\\theta}_{\\mathrm{map}} = \\mathrm{argmax}_{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\) is the maximum a posteriori or MAP estimator."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#measures-of-central-tendency",
    "href": "slides/bayesmod-slides1.html#measures-of-central-tendency",
    "title": "Bayesian modelling",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\n\n\nFigure 6: Point estimators from a right-skewed distribution (left) and from a multimodal distribution (right)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#example-of-loss-functions",
    "href": "slides/bayesmod-slides1.html#example-of-loss-functions",
    "title": "Bayesian modelling",
    "section": "Example of loss functions",
    "text": "Example of loss functions\n\n\nFigure 7: Posterior density with mean, mode and median point estimators (left) and corresponding loss functions, scaled to have minimum value of zero (right)."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#credible-regions",
    "href": "slides/bayesmod-slides1.html#credible-regions",
    "title": "Bayesian modelling",
    "section": "Credible regions",
    "text": "Credible regions\nThe freshman dream comes true! A \\(1-\\alpha\\) credible region give a set of parameter values which contains the “true value” of the parameter \\(\\boldsymbol{\\theta}\\) with probability \\(1-\\alpha\\).\nCaveat: McElreath (2020) suggests the term ‘compatibility’, as it\n\nreturns the range of parameter values compatible with the model and data."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#which-credible-intervals",
    "href": "slides/bayesmod-slides1.html#which-credible-intervals",
    "title": "Bayesian modelling",
    "section": "Which credible intervals?",
    "text": "Which credible intervals?\nMultiple \\(1-\\alpha\\) intervals, most common are\n\nequitailed: region \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles and\nhighest posterior density interval (HPDI), which gives the smallest interval \\((1-\\alpha)\\) probability\n\n\n\nIf we accept to have more than a single interval, the highest posterior density region can be a set of disjoint intervals. The HDPI is more sensitive to the number of draws and more computationally intensive (see R package HDinterval)"
  },
  {
    "objectID": "slides/bayesmod-slides1.html#illustration-of-credible-regions",
    "href": "slides/bayesmod-slides1.html#illustration-of-credible-regions",
    "title": "Bayesian modelling",
    "section": "Illustration of credible regions",
    "text": "Illustration of credible regions\n\n\nFigure 8: Density plots with 89% (top) and 50% (bottom) equitailed or central credible (left) and highest posterior density (right) regions for two data sets, highlighted in grey."
  },
  {
    "objectID": "slides/bayesmod-slides1.html#references",
    "href": "slides/bayesmod-slides1.html#references",
    "title": "Bayesian modelling",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\nCasella, G., & Berger, R. L. (2002). Statistical inference (2nd ed.). Duxbury.\n\n\nFinetti, B. de. (1974). Theory of probability: A critical introductory treatment (Vol. 1). Wiley.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and STAN (2nd ed.). Chapman; Hall/CRC."
  }
]