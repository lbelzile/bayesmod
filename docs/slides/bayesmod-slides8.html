<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2bb0ec5e928ee8c40b12725cb7836c35.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.1">

  <meta name="author" content="Léo Belzile">
  <title>Bayesian modelling</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-da069d641d4916e8549bf8dc2b95f825.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-color="#ff585d" class="quarto-title-block center">
  <h1 class="title">Bayesian modelling</h1>
  <p class="subtitle">Bayesian regression</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Léo Belzile 
</div>
</div>
</div>

  <p class="date">Last compiled Friday Mar 14, 2025</p>
</section>
<section id="bayesian-regression" class="slide level2">
<h2>Bayesian regression</h2>
<p>Same, same, but different…</p>
<ul>
<li>Generalized linear models, with distributional assumptions and link functions.</li>
<li>We assign priors to <span class="math inline">\(\boldsymbol{\beta}\)</span> which
<ul>
<li>can provide shrinkage (regularization towards zero)</li>
<li>can enable variable selection (spike and slab)</li>
</ul></li>
</ul>
</section>
<section id="model-setup" class="slide level2">
<h2>Model setup</h2>
<p>Consider regression models with</p>
<ul>
<li>model (or design) matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span></li>
<li>regression coefficients <span class="math inline">\(\boldsymbol{\beta} = (\beta_1, \ldots, \beta_p)^\top \in \mathbb{R}^p\)</span></li>
</ul>
</section>
<section id="ordinary-linear-regression-model." class="slide level2">
<h2>Ordinary linear regression model.</h2>
<p>In the ordinary linear regression model, observations are independent and homoscedastic and <span class="math display">\[\begin{align*}
\boldsymbol{Y} \mid \mathbf{X}, \boldsymbol{\beta}, \omega \sim \mathsf{Gauss}_n(\beta_0\mathbf{1}_n + \mathbf{X}\boldsymbol{\beta}, \omega^{-1}\mathbf{I}_n).
\end{align*}\]</span></p>
<p>The intercept <span class="math inline">\(\beta_0\)</span> receives special treatment, is always included. It is typically assigned an improper prior <span class="math inline">\(p(\beta_0) \propto 1.\)</span></p>
</section>
<section id="decomposition-of-quadratic-forms" class="slide level2">
<h2>Decomposition of quadratic forms</h2>
<p>For quadratic forms (in <span class="math inline">\(\boldsymbol{x}\)</span>) with <span class="math display">\[\begin{align*}
&amp; (\boldsymbol{x} - \boldsymbol{a})^\top \mathbf{A}(\boldsymbol{x} - \boldsymbol{a}) + (\boldsymbol{x} - \boldsymbol{b})^\top \mathbf{B}(\boldsymbol{x} - \boldsymbol{b}) \\
&amp;\stackrel{\boldsymbol{x}}{\propto} (\boldsymbol{x} - \boldsymbol{c})^\top \mathbf{C}(\boldsymbol{x} - \boldsymbol{c})
\end{align*}\]</span> where <span class="math inline">\(\mathbf{C} = \mathbf{A} + \mathbf{B}\)</span> and <span class="math inline">\(\boldsymbol{c}= \mathbf{C}^{-1}(\mathbf{A}\boldsymbol{a} + \mathbf{B}\boldsymbol{b})\)</span>.</p>
<p>This is useful to complete the square in Gaussian-Gaussian models.</p>
</section>
<section id="bayesian-gaussian-linear-model" class="slide level2">
<h2>Bayesian Gaussian linear model</h2>
<p>Consider Gaussian-gamma <strong>conjugate</strong> priors for the mean and precision parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\omega\)</span>, <span class="math display">\[\begin{align*}
\boldsymbol{\beta} \mid \omega &amp;\sim \mathsf{Gauss}\left\{\boldsymbol{\mu}_0, (
\omega\boldsymbol{\Omega}_0)^{-1}\right\} \\
\omega &amp;\sim \mathsf{gamma}(\nu_0/2,\tau_0/2).
\end{align*}\]</span> Recall the sampling distribution of the ordinary least squares estimator is <span class="math display">\[\widehat{\boldsymbol{\beta}} \sim \mathsf{Gauss}_p\{\boldsymbol{\beta}, (\omega\mathbf{X}^\top\mathbf{X})^{-1}\}.\]</span></p>
</section>
<section id="conditional-distributions" class="slide level2">
<h2>Conditional distributions</h2>
<p>The conditional and marginal posterior distributions for the mean coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> and for the precision <span class="math inline">\(\omega\)</span> are <span class="math display">\[\begin{align*}
\boldsymbol{\beta} \mid \omega, \boldsymbol{y} &amp;\sim \mathsf{Gauss}_p\left\{\boldsymbol{\mu}_n, (\omega\boldsymbol{\Omega}_n)^{-1}\right\}  \\
\omega \mid  \boldsymbol{y} &amp;\sim \mathsf{gamma}\left\{(\nu_0 + n)/2,  \tau^2_n/2\right\}.
\end{align*}\]</span></p>
<p>If we integrate over the precision, we get instead <span class="math display">\[\begin{align*}
\boldsymbol{\beta} \mid  \boldsymbol{y} &amp;\sim \mathsf{Student}_p(\boldsymbol{\mu}_n,  \tau_n/(\nu_0+n) \times \mathbf{\Omega}_n^{-1}, \nu_0 + n)
\end{align*}\]</span></p>
</section>
<section id="posterior-parameters" class="slide level2">
<h2>Posterior parameters</h2>
<p>The precision is the sum of the precision of OLS estimator and prior precision.</p>
<p>The posterior mean is a weighted combination of the prior and OLS means, weighted by the scaled precision. <span class="math display">\[\begin{align*}
\boldsymbol{\Omega}_n &amp;= \mathbf{X}^\top\mathbf{X} + \boldsymbol{\Omega}_0\\
\boldsymbol{\mu}_n &amp;= \boldsymbol{\Omega}_n^{-1}(\mathbf{X}^\top\mathbf{X}\widehat{\boldsymbol{\beta}} + \boldsymbol{\Omega}_0\boldsymbol{\mu}_0) = \boldsymbol{\Omega}_n^{-1}(\mathbf{X}^\top\boldsymbol{y} + \boldsymbol{\Omega}_0\boldsymbol{\mu}_0)\\
\tau_n &amp;= \tau_0 + (\boldsymbol{y} - \mathbf{X}\widehat{\boldsymbol{\beta}})^\top(\boldsymbol{y} - \mathbf{X}\widehat{\boldsymbol{\beta}}) + (\boldsymbol{\mu}_n - \widehat{\boldsymbol{\beta}})^\top \mathbf{X}^\top\mathbf{X}(\boldsymbol{\mu}_n - \widehat{\boldsymbol{\beta}}) \\&amp; \quad + (\boldsymbol{\mu}_n-\boldsymbol{\mu}_0)^\top\boldsymbol{\Omega}_0(\boldsymbol{\mu}_n-\boldsymbol{\mu}_0)
\end{align*}\]</span></p>
</section>
<section id="scale-mixture-of-gaussians" class="slide level2">
<h2>Scale mixture of Gaussians</h2>
<p>If <span class="math inline">\(X \mid \sigma^2 \sim \mathsf{Gauss}(0, \sigma^2)\)</span> and we assign a prior <span class="math inline">\(p(\sigma^2)\)</span></p>
<ul>
<li>if <span class="math inline">\(\sigma^2 \sim \mathsf{inv. gamma}(\nu/2, \nu/2)\)</span>, then <span class="math inline">\(X \sim \mathsf{Student}(0,1, \nu)\)</span></li>
<li>if <span class="math inline">\(\sigma^2 \sim \mathsf{exp}(1/\lambda^2),\)</span> then <span class="math inline">\(X \sim \mathsf{Laplace}(0, \lambda).\)</span></li>
</ul>
</section>
<section id="sketch-of-proof" class="slide level2">
<h2>Sketch of proof</h2>
<ol type="1">
<li>write down the joint posterior as <span class="math display">\[\begin{align*}
p(\boldsymbol{\beta}, \omega \mid \boldsymbol{y}) &amp;\propto p(\boldsymbol{y} \mid \boldsymbol{\beta}, \omega) p(\omega)
\end{align*}\]</span></li>
<li>rewrite the first quadratic form in <span class="math inline">\(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}\)</span> using the orthogonal decomposition <span class="math display">\[\begin{align*}
(\boldsymbol{y}-\mathbf{X}\widehat{\boldsymbol{\beta}}) + (\mathbf{X}\widehat{\boldsymbol{\beta}} - \mathbf{X}\boldsymbol{\beta})
\end{align*}\]</span></li>
<li>pull terms together and separate the conditional posterior <span class="math inline">\(p(\boldsymbol{\beta} \mid \boldsymbol{y}, \omega)\)</span> and <span class="math inline">\(p(\omega \mid \boldsymbol{y})\)</span></li>
</ol>
</section>
<section id="sketch-of-proof-continued" class="slide level2">
<h2>Sketch of proof (continued)</h2>
<ol start="4" type="1">
<li>use decomposition of quadratic forms with <span class="math inline">\(\boldsymbol{a} = \widehat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\mathbf{A}=\mathbf{X}^\top\mathbf{X}\)</span>, <span class="math inline">\(\boldsymbol{b} = \boldsymbol{\mu}_0\)</span> and <span class="math inline">\(\mathbf{B}=\boldsymbol{\Omega}_0\)</span></li>
<li>the marginal of <span class="math inline">\(\boldsymbol{\beta}\)</span> is obtained by regrouping all terms that depend on <span class="math inline">\(\omega\)</span> and integrating over the latter, recognizing the integral as an unnormalized gamma density</li>
</ol>
</section>
<section id="cultural-appropriation" class="slide level2">
<h2>Cultural appropriation</h2>
<p>Study 4 of <span class="citation" data-cites="Lin.Kim.Uduehi.Keinan:2024">Lin et al. (<a href="#/references" role="doc-biblioref" onclick="">2024</a>)</span> is a 3 by 2 by 2 three-way between-subject ANOVA focusing on cultural appropriation using a fictional scenario on publication of a soul food recipe cookbook from Chef Dax.</p>
</section>
<section id="experimental-variables" class="slide level2">
<h2>Experimental variables</h2>
<ul>
<li>ethnicity: chef is African-American or not</li>
<li>action: the way he obtained the recipes (by peeking without permission in kitchens, by asking permission or without mention (control)</li>
<li>political ideology of respondant (liberal or conservative).</li>
</ul>
</section>
<section id="posterior-densities-for-marginal-effects" class="slide level2">
<h2>Posterior densities for marginal effects</h2>

<img data-src="bayesmod-slides8_files/figure-revealjs/fig-contrasts-1.png" width="960" class="r-stretch quarto-figure-center" id="fig-contrasts"><p class="caption">
Figure&nbsp;1: Difference in appropriation rating for black vs non-black Chef Dax, average accross different levels of brand action.
</p></section>
<section id="chef-dax-and-cultural-appropriation" class="slide level2">
<h2>Chef Dax and cultural appropriation</h2>
<p>The coefficients and standard errors from the linear regression are very nearly similar to the posterior mean and standard deviations for <span class="math inline">\(\boldsymbol{\beta}\)</span> from the marginal Student-<span class="math inline">\(t,\)</span> owing to the large sample size and uninformative priors.</p>
<p>On average, liberals perceive cultural appropriation more strongly (with nearly 2 points more), than conservatives (0.7 points on average).</p>
</section>
<section id="modelling-random-effects" class="slide level2">
<h2>Modelling random effects</h2>
<p>Gaussian mixed models in frequentist statistics are of the form <span class="math display">\[\begin{align*}
\boldsymbol{Y} \mid \mathcal{B}=\boldsymbol{b} &amp;\sim \mathsf{Gauss}_n\left(\mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{b}, \sigma^2 \mathbf{I}_n\right)\\
\mathcal{B} &amp;\sim \mathsf{Gauss}_q(\boldsymbol{0}_q, \boldsymbol{\Psi}).
\end{align*}\]</span></p>
<p>Bayesians also assign priors to <span class="math inline">\(\boldsymbol{\beta}\)</span> as well! but typically apriori independent with <span class="math inline">\(\mathsf{Va}_{\boldsymbol{\beta}}(\boldsymbol{\beta}) \propto \mathbf{I}_p.\)</span></p>
</section>
<section id="prior-for-covariance-matrices" class="slide level2">
<h2>Prior for covariance matrices</h2>
<p>We need a prior for <span class="math inline">\(p \times p\)</span> symmetric positive definite matrix random matrices!</p>
<p>We consider two cases:</p>
<ul>
<li>Wishart (precision) / inverse Wishart (covariance), the conjugate prior for Gaussian</li>
<li>onion peel prior on the correlation matrix</li>
</ul>
<p>Wishart allows for conjugacy, but has unintuitive properties.</p>
</section>
<section id="wishart-distribution" class="slide level2">
<h2>Wishart distribution</h2>
<p>We say <span class="math inline">\(\mathbf{Q} \sim \mathsf{Wishart}_p(\nu, \mathbf{S})\)</span> for <span class="math inline">\(\nu&gt;0\)</span> degrees of freedom and scale <span class="math inline">\(\mathbf{S}\)</span> if it’s density is proportional to <span class="math display">\[\begin{align*}
f(\boldsymbol{Q}) \stackrel{\boldsymbol{Q}}{\propto} |\boldsymbol{Q}|^{(\nu-p-1)/2}\exp\left\{-\frac{\mathrm{tr}(\mathbf{S}^{-1}\boldsymbol{Q})}{2}\right\}, \quad \nu &gt; p-1.
\end{align*}\]</span> where <span class="math inline">\(|\cdot|\)</span> denotes the determinant of the matrix and <span class="math inline">\(\mathrm{tr}(\cdot)\)</span> the trace operator.</p>
</section>
<section id="wishart-distribution-1" class="slide level2">
<h2>Wishart distribution</h2>
<p>The Wishart arises from considering <span class="math inline">\(n \geq p\)</span> independent and identically distributed mean zero Gaussian vectors <span class="math inline">\(\boldsymbol{Y}_i \sim \mathsf{Gauss}_p(\boldsymbol{0}_p, \mathbf{S})\)</span>, where <span class="math display">\[\begin{align*}
\sum_{i=1}^{\nu} \boldsymbol{Y}_i\boldsymbol{Y}_i^\top \sim \mathsf{Wishart}_p(\nu, \mathbf{S}).
\end{align*}\]</span></p>
</section>
<section id="prior-elicitation-for-wishart" class="slide level2">
<h2>Prior elicitation for Wishart</h2>
<p>For prior elicitation, the mean of the Wishart is <span class="math inline">\(\nu \mathbf{S}\)</span></p>
<ul>
<li><span class="math inline">\(\nu\)</span> is thus a prior sample size</li>
<li><span class="math inline">\(\mathbf{S}\)</span> is a scale matrix, often the identity matrix.</li>
</ul>
</section>
<section id="inverse-wishart" class="slide level2">
<h2>Inverse Wishart</h2>
<p>Consider a prior for the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma} = \boldsymbol{Q}^{-1}\)</span>. Applying the change of variable formula, we get Jacobian <span class="math inline">\(|\boldsymbol{\Sigma}|^{p+1}\)</span>, and so <span class="math inline">\(\boldsymbol{\Sigma} \sim \mathsf{inv. Wishart}(\nu, \mathbf{S}^{-1}),\)</span> with density <span class="math display">\[\begin{align*}
p(\boldsymbol{\Sigma}) \propto |\boldsymbol{\Sigma}|^{-(\nu+p+1)/2} \exp\left\{-\frac{1}{2} \mathrm{tr}\left(\boldsymbol{S}^{-1}\boldsymbol{\Sigma}^{-1}\right)\right\}
\end{align*}\]</span> with expectation <span class="math inline">\(\mathbf{S}^{-1}(\nu-p-1)\)</span> for <span class="math inline">\(\nu &gt; p+1.\)</span></p>
</section>
<section id="wishart-as-conjugate-prior-in-gaussian-model" class="slide level2">
<h2>Wishart as conjugate prior in Gaussian model</h2>
<p>Consider <span class="math inline">\(\boldsymbol{\mu} \sim \mathsf{Gauss}_p(\boldsymbol{\mu}_0, \boldsymbol{Q}^{-1})\)</span> and <span class="math inline">\(\boldsymbol{Q} \sim \mathsf{Wishart}_p(\nu, \mathbf{S})\)</span> for <span class="math inline">\(\nu \geq p\)</span>. Then, <span class="math display">\[\begin{align*}
p(\boldsymbol{Q} \mid \boldsymbol{\mu}, \boldsymbol{\mu}_0) \propto &amp;  |\boldsymbol{Q}|^{(\nu-p-1)/2}\exp\{-\mathrm{tr}(\mathbf{S}^{-1}\boldsymbol{Q})/2\} \\ &amp;\times |\boldsymbol{Q}|^{1/2} \exp \left\{ -\frac{1}{2} (\boldsymbol{\mu}-\boldsymbol{\mu}_0)^\top\boldsymbol{Q}(\boldsymbol{\mu}-\boldsymbol{\mu}_0)\right\}
\end{align*}\]</span> and thus <span class="math display">\[ \boldsymbol{Q} \mid \boldsymbol{\mu}, \boldsymbol{\mu}_0 \sim \mathsf{Wishart}_p\{\nu + 1/2, \boldsymbol{S} + (\boldsymbol{\mu}-\boldsymbol{\mu}_0)(\boldsymbol{\mu}-\boldsymbol{\mu}_0)^\top\}.\]</span></p>
</section>
<section id="conjugacy" class="slide level2">
<h2>Conjugacy</h2>
<p>Note that a <span class="math inline">\(1 \times 1\)</span> matrix is equal to it’s trace, and the trace operator is invariant to cyclic of it’s argument, meaning that <span class="math display">\[\begin{align*}
(\boldsymbol{\mu}-\boldsymbol{\mu}_0)^\top\boldsymbol{Q}(\boldsymbol{\mu}-\boldsymbol{\mu}_0) = \mathrm{tr}\left\{ \boldsymbol{Q}(\boldsymbol{\mu}-\boldsymbol{\mu}_0)(\boldsymbol{\mu}-\boldsymbol{\mu}_0)^\top\right\}.
\end{align*}\]</span></p>
</section>
<section id="properties-of-wishart" class="slide level2">
<h2>Properties of Wishart</h2>
<p>The marginal precision for the Wishart variate are gamma distributed with the same degrees of freedom <span class="math inline">\(\nu\)</span>.</p>
<ul>
<li>there is a single parameter governing all marginal variances.</li>
</ul>
<p>Moreover, the absolute value of the correlation and marginal variance parameters are negatively related <span class="citation" data-cites="Gelman:2013">(<a href="#/references" role="doc-biblioref" onclick="">Gelman et al., 2013</a>)</span>. Large variance thus correspond to small correlations shrunk towards zero when the degrees of freedom increase.</p>
</section>
<section id="wishart-draws" class="slide level2">
<h2>Wishart draws</h2>

<img data-src="bayesmod-slides8_files/figure-revealjs/fig-draws-Wishart-1.png" width="960" class="r-stretch quarto-figure-center" id="fig-draws-Wishart"><p class="caption">
Figure&nbsp;2: Prior draws from a bivariate inverse Wishart with identity scale matrix and <span class="math inline">\(\nu \in \{3, 20\}\)</span> degrees of freedom.
</p></section>
<section id="onion-peel-prior" class="slide level2">
<h2>Onion peel prior</h2>
<p>A better alternative is to specify</p>
<ul>
<li>different prior for each marginal scale <span class="math inline">\(\sigma_j\)</span> and</li>
<li>a prior on the correlation matrix <span class="math inline">\(\mathbf{R}.\)</span></li>
</ul>
<p>For the latter, the onion peel or LKJ prior, named after the authors of <span class="citation" data-cites="Lewandowski:2009">Lewandowski et al. (<a href="#/references" role="doc-biblioref" onclick="">2009</a>)</span>, is <span class="math display">\[p(\mathbf{R}) \propto |\mathbf{R}|^{\eta-1}, \eta&gt;0\]</span></p>
<p>The case <span class="math inline">\(\eta=1\)</span> leads to uniform over the space of correlation matrices, and <span class="math inline">\(\eta&gt;1\)</span> favours the identity matrix.</p>
</section>
<section id="shrinkage-and-variable-selection" class="slide level2">
<h2>Shrinkage and variable selection</h2>
<p>With <span class="math inline">\(p\)</span> covariates, there are <span class="math inline">\(2^p\)</span> potential regression models.</p>
<p>This is too many models to explore for <span class="math inline">\(p\)</span> large, and too many parameters relative to sample size <span class="math inline">\(n\)</span>.</p>
<p>Two solutions:</p>
<ul>
<li>shrinkage priors: penalize small coefficients by shrinking towards zero via priors on <span class="math inline">\(\boldsymbol{\beta}\)</span></li>
<li>Bayesian model averaging: assign prior to each model (different sets of covariates <span class="math inline">\(\mathbf{X}\)</span>) and get a mixture of models.</li>
</ul>
</section>
<section id="spike-and-slab-prior" class="slide level2">
<h2>Spike-and-slab prior</h2>
<p>The <strong>discrete spike-and-slab prior</strong> <span class="citation" data-cites="Mitchell.Beauchamp:1988">(<a href="#/references" role="doc-biblioref" onclick="">Mitchell &amp; Beauchamp, 1988</a>)</span> is a two-component mixture with</p>
<ul>
<li>the spike: a point mass <span class="math inline">\(\delta_0\)</span> or a vary narrow distribution centered at zero</li>
<li>the slab, a diffuse distribution.</li>
</ul>
<p><span class="math display">\[\begin{align*}
\beta_j \mid \gamma_j, \sigma^2 \sim \gamma_j \delta_0 + (1-\gamma_j)\mathsf{Gauss}(0, \sigma^2)
\end{align*}\]</span></p>
</section>
<section id="prior-for-the-spike-and-slab-prior-probability" class="slide level2">
<h2>Prior for the spike-and-slab prior probability</h2>
<p>Set independent and identically distributed conjugate prior for <span class="math inline">\(\gamma_j \sim \mathsf{binom}(1, \omega),\)</span> whence <span class="math display">\[\begin{align*}
p(\boldsymbol{\gamma} \mid \omega) = \prod_{j=1}^n \omega^{\gamma_j} (1-\omega)^{1-\gamma_j}
\end{align*}\]</span></p>
<p>Apriori, we set <span class="math inline">\(\omega \in (0,1)\)</span> as the proportion of the <span class="math inline">\(p\)</span> coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> that are zero (so <span class="math inline">\(p(1-\omega)\)</span> nonzero coefficients).</p>
</section>
<section id="continuous-spike-and-slab-prior" class="slide level2">
<h2>Continuous spike-and-slab prior</h2>
<p><span class="citation" data-cites="George.McCulloch:1993">George &amp; McCulloch (<a href="#/references" role="doc-biblioref" onclick="">1993</a>)</span> replaced the spike by a Gaussian with near infinite precision around zero, with <span class="math display">\[\begin{align*}
\beta_j \mid \gamma_j, \sigma_j^2,\phi^2_j \sim  \gamma_j \mathsf{Gauss}(0, \sigma_j^2\phi^2) + (1-\gamma_j) \mathsf{Gauss}(0, \sigma^2_j)
\end{align*}\]</span> where <span class="math inline">\(\phi^2_j\)</span> is very nearly zero, typically <span class="math inline">\(\phi_j^2=0.001\)</span>.</p>
<p>The construction allows for variable augmentation with mixture indicators and Gibbs sampling, although mixing tends to be poor.</p>
</section>
<section id="horseshoe-prior" class="slide level2">
<h2>Horseshoe prior</h2>
<p>The horseshoe prior of <span class="citation" data-cites="Carvalho.Polson.Scott:2010">Carvalho et al. (<a href="#/references" role="doc-biblioref" onclick="">2010</a>)</span> is a hierarchical prior of the form <span class="math display">\[\begin{align*}
\beta_j \mid \sigma^2_j &amp;\sim \mathsf{Gauss}(0, \sigma^2_j),\\\sigma^2_j \mid \lambda  &amp;\sim \mathsf{Student}_{+}(0, \lambda, 1),\\ \lambda &amp;\sim \mathsf{Student}_{+}(0, \omega, 1)
\end{align*}\]</span> where <span class="math inline">\(\mathsf{Student}_{+}(0, a, 1)\)</span> denotes a half-Cauchy distribution with scale <span class="math inline">\(a&gt;0,\)</span> truncated on <span class="math inline">\(\mathbb{R}_{+}.\)</span></p>
</section>
<section id="understanding-shrinkage-priors" class="slide level2">
<h2>Understanding shrinkage priors</h2>
<p>The choice of <span class="math inline">\(\sigma^2_j\)</span> leads to an unconditional scale mixture of Gaussian for <span class="math inline">\(\beta_j\)</span>.</p>
<p>Better is to consider <span class="math display">\[\kappa = 1 - 1/(1+\sigma^2) \in [0,1].\]</span> Penalization of near-zero components can be deduced from the density of <span class="math inline">\(\kappa \to 0\)</span>, and similarly penalization of large signals by looking at the density when <span class="math inline">\(\kappa \to 1.\)</span></p>
</section>
<section id="shrinkage-weights" class="slide level2">
<h2>Shrinkage weights</h2>
<p>Weighting implied by Gaussian mixture density with Cauchy/Laplace/horsehoe.</p>

<img data-src="bayesmod-slides8_files/figure-revealjs/fig-weights-shrinkage-1.png" width="960" class="r-stretch quarto-figure-center" id="fig-weights-shrinkage"><p class="caption">
Figure&nbsp;3: Density of penalization weights <span class="math inline">\(\kappa\)</span> of spike (near zero) and slab (near one) for three shrinkage priors.
</p></section>
<section id="comparison-of-shrinkage-priors" class="slide level2">
<h2>Comparison of shrinkage priors</h2>

<img data-src="bayesmod-slides8_files/figure-revealjs/fig-shrinkage-1.png" width="960" class="r-stretch quarto-figure-center" id="fig-shrinkage"><p class="caption">
Figure&nbsp;4: Marginal density for a regression coefficient <span class="math inline">\(\beta\)</span> with horseshoe prior (full), Laplace (dashed) and a Student-<span class="math inline">\(t\)</span> (thick dotted).
</p></section>
<section id="comments-about-the-horseshoe" class="slide level2">
<h2>Comments about the horseshoe</h2>
<p>While the horseshoe prior guarantees that large coefficients are not regularized, this feature of the shrinkage prior is harmful in certain instances, for example separation of variables for logistic regression.</p>
<p>Markov chain Monte Carlo simulations are hampered by these parameters whose posterior mean does not exist, leading to poor mixing.</p>
</section>
<section id="finnish-horseshoe-aka-regularized-horseshoe" class="slide level2">
<h2>Finnish horseshoe (aka regularized horseshoe)</h2>
<p><span class="citation" data-cites="Piironen.Vehtari:2017">Piironen &amp; Vehtari (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span> proposed instead <span class="math display">\[\begin{align*}
\beta_j \mid \lambda, \tau_j, c^2 &amp;\sim \mathsf{Gauss}\left(0, \lambda\frac{c^2\tau_j^2}{c^2 + \tau^2_j\lambda^2}\right), \\
\tau_j &amp;\sim \mathsf{Student}_{+}(0, 1, 1)\\
c^2 \mid s^2, \nu &amp;\sim \mathsf{inv. gamma}(\nu/2, \nu s^2/2).
\end{align*}\]</span></p>
</section>
<section id="shrinkage-for-finnish-horsshoe" class="slide level2">
<h2>Shrinkage for Finnish horsshoe</h2>
<p>When <span class="math inline">\(\tau^2\lambda^2_j\)</span> is much greater than <span class="math inline">\(c^2\)</span>, this amounts to having a Student slab with <span class="math inline">\(\nu\)</span> degrees of freedom for large coefficients.</p>
<p>Taking a small value of <span class="math inline">\(\nu\)</span> allows for large, but not extreme components, and the authors use <span class="math inline">\(s^2=2, \nu=4.\)</span></p>
</section>
<section id="hyperprior-for-finnish-horseshoe" class="slide level2">
<h2>Hyperprior for Finnish horseshoe</h2>
<p>The above specification does not specify the prior for the global scale <span class="math inline">\(\lambda\)</span>, for which <span class="citation" data-cites="Piironen.Vehtari:2017">Piironen &amp; Vehtari (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span> recommend <span class="math display">\[\lambda \sim \mathsf{Student}_{+}\left\{0, \frac{p_0}{(p-p_0)}\frac{\sigma}{n^{1/2}}, 1\right\},\]</span> where <span class="math inline">\(p_0\)</span> is a prior guess for the number of non-zero components out of <span class="math inline">\(p,\)</span> <span class="math inline">\(n\)</span> is the sample size and <span class="math inline">\(\sigma\)</span> is some level of the noise.</p>
</section>
<section id="comparison-of-shrinkage-priors-1" class="slide level2">
<h2>Comparison of shrinkage priors</h2>
<p>We revisit the <code>diabetes</code> data from the <strong>R</strong> package <code>lars</code>, which was used in <span class="citation" data-cites="Park.Casella:2008">Park &amp; Casella (<a href="#/references" role="doc-biblioref" onclick="">2008</a>)</span> to illustrate the Bayesian LASSO. We consider three methods:</p>
<ul>
<li>the default Gaussian prior, which gives a ridge penalty,</li>
<li>the Bayesian LASSO of <span class="citation" data-cites="Park.Casella:2008">Park &amp; Casella (<a href="#/references" role="doc-biblioref" onclick="">2008</a>)</span></li>
<li>the horseshoe prior.</li>
</ul>
<p>Models are fitted using the <code>bayesreg</code> package.</p>
</section>
<section id="density-estimates-of-ordered-coefficients" class="slide level2">
<h2>Density estimates of ordered coefficients</h2>

<img data-src="bayesmod-slides8_files/figure-revealjs/fig-shrinkage-dens-comparison-1.png" width="960" class="r-stretch quarto-figure-center" id="fig-shrinkage-dens-comparison"><p class="caption">
Figure&nbsp;5: Density estimates for regression coefficients with Gaussian (ridge), double exponential (Laplace) and horseshoe priors for the <code>diabetes</code> data.
</p></section>
<section id="comments-on-penalization" class="slide level2">
<h2>Comments on penalization</h2>
<ul>
<li>Ridge has the widest intervals of all methods, providing some shrinkage only for large values of <span class="math inline">\(\beta\)</span>.</li>
<li>The horseshoe has typically narrower intervals, with more mass in a neighborhood of zero for smaller coefficients, and asymmetric intervals.</li>
<li>The effective sample size fraction relative to the number of samples ranges from 11% to 85%, compared to 54% to 100% for the Bayesian LASSO and near-independent draws with the conjugate ridge.</li>
</ul>
</section>
<section id="bayesian-model-averaging" class="slide level2">
<h2>Bayesian model averaging</h2>
<p>BMA refers to situation where we specify a mixture of models <span class="math inline">\(M_1, \ldots,\)</span>, and we wish to recover the posterior weights of these.</p>
<p>This is useful for predictions (ensemble) methods to account for uncertainty in variable selection.</p>
<p>We consider design of MCMC for moving between models.</p>
</section>
<section id="reversible-jump-mcmc" class="slide level2">
<h2>Reversible jump MCMC</h2>
<p>Reversible jump Markov chain Monte Carlo <span class="citation" data-cites="Green:1995">(<a href="#/references" role="doc-biblioref" onclick="">Green, 1995</a>)</span> is an extension of the classical Metropolis–Hastings scheme that allows for arbitrary measures and through this varying dimensions.</p>
<p>Varying dimensions occurs not only with variable selection, but also changepoint analysis and mixture models with varying number of components.</p>
<p>Reversible jump requires <strong>dimension-balancing</strong> and defining different types of moves for jumping between dimensions.</p>
</section>
<section id="dimension-changes-and-jacobians" class="slide level2">
<h2>Dimension changes and jacobians</h2>
<p>Dimensions changes are integrated in the Metropolis–Hastings step through a Jacobian term <span class="math inline">\(J\)</span>: the probability of rejection <span class="math inline">\(R\)</span> for Metropolis becomes <span class="math display">\[\begin{align*}
    R = J\frac{p(\boldsymbol{\theta}_t^{\star})}{p(\boldsymbol{\theta}_{t-1})}\frac{q(\boldsymbol{\theta}_{t-1} \mid \boldsymbol{\theta}_t^{\star} )}{q(\boldsymbol{\theta}_t^{\star} \mid \boldsymbol{\theta}_{t-1})}
\end{align*}\]</span></p>
<p>In regression models, we will consider moves that adds or removes one parameter/regressor at a time.</p>
</section>
<section id="setup" class="slide level2">
<h2>Setup</h2>
<p>We consider models <span class="math inline">\(M_1, \ldots, M_m\)</span> with for simplicity <span class="math inline">\(p(M_i)=1\)</span> for all models that include an intercept.</p>
<p>We write <span class="math inline">\(|M|\)</span> for the cardinality of the set of non-zero coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> in model <span class="math inline">\(M.\)</span></p>
<p>Define <span class="math inline">\(\mathbf{X}^{(m)}\)</span> and <span class="math inline">\(\boldsymbol{\beta}^{(m)}\)</span> as the model matrix and the associated vector of non-zero coefficients associated with model <span class="math inline">\(M_m\)</span></p>
</section>
<section id="setup-for-regression" class="slide level2">
<h2>Setup for regression</h2>
<p><span class="math display">\[\begin{align*}
\boldsymbol{Y} \mid M_m, \boldsymbol{\beta}, \sim \mathsf{Gauss}(\mathbf{X}^{(m)}\boldsymbol{\beta}^{(m)}, \sigma^2 \mathbf{I}_n).
\end{align*}\]</span></p>
<p>We assign a Gaussian prior on <span class="math inline">\(\boldsymbol{\beta}^{(m)} \mid M_m,\)</span> is assigned a Gaussian prior, etc.</p>
</section>
<section id="conditional-bayes-factor" class="slide level2">
<h2>Conditional Bayes factor</h2>
<p>Write <span class="math inline">\(\boldsymbol{\theta}\)</span> for all parameters other than the response, model and vector of coefficients.</p>
<p>We can consider a joint update of the regression parameters <span class="math inline">\(\boldsymbol{\beta}, M\)</span> by sampling from their joint distribution via <span class="math display">\[p(\boldsymbol{\beta} \mid M, \boldsymbol{\theta}) p(M \mid \boldsymbol{\theta}).\]</span> The update for <span class="math inline">\(p(\boldsymbol{\beta} \mid M, \boldsymbol{\theta})\)</span> is as usual.</p>
</section>
<section id="update-for-model" class="slide level2">
<h2>Update for model</h2>
<p>The conditional Bayes factor <span class="math display">\[\begin{align*}
p(M \mid \boldsymbol{Y}, \boldsymbol{\theta}) &amp;\stackrel{M}{\propto} p(M) p(\boldsymbol{Y} \mid M, \boldsymbol{\theta})
\\&amp;= p(M) \int_{\mathbb{R}^{\mathbb{|M|}}}p(\boldsymbol{Y} \mid M, \boldsymbol{\beta},\boldsymbol{\theta}) p(\boldsymbol{\beta} \mid M, \boldsymbol{\theta}) d \boldsymbol{\beta}
\end{align*}\]</span></p>
</section>
<section id="marginalization" class="slide level2">
<h2>Marginalization</h2>
<p>We can thus marginalize over <span class="math inline">\(\boldsymbol{\beta}\)</span> to get <span class="math display">\[\begin{align*}
p(\boldsymbol{M} \mid \boldsymbol{Y}, \boldsymbol{\theta}) \propto p(M) |\boldsymbol{Q}_{\boldsymbol{\beta}}|^{-1/2}\exp\left( \frac{1}{2} \boldsymbol{\mu}_{\boldsymbol{\beta}}^\top\boldsymbol{Q}_{\boldsymbol{\beta}} \boldsymbol{\mu}_{\boldsymbol{\beta}}\right)
\end{align*}\]</span> where <span class="math inline">\(\boldsymbol{\mu}_{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\boldsymbol{Q}_{\boldsymbol{\beta}}\)</span> are the mean and precision of <span class="math inline">\(p(\boldsymbol{\beta} \mid \boldsymbol{Y}, M, \boldsymbol{\theta}).\)</span></p>
</section>
<section id="moves-for-variable-selection" class="slide level2">
<h2>Moves for variable selection</h2>
<p>We consider different types of move for the <span class="math inline">\(k_{\max}\)</span> potential covariates (including interactions, etc.) <span class="citation" data-cites="Holmes:2002">(<a href="#/references" role="doc-biblioref" onclick="">Holmes et al., 2002</a>)</span></p>
<ul>
<li>birth: adding an unused covariate chosen at random from the remaining ones</li>
<li>death: removing one covariate at random from the current matrix</li>
<li>swap an active covariate for an unused one.</li>
</ul>
<p>Only the last type of move preserves the dimension.</p>
</section>
<section id="jacobians-for-reversible-jump" class="slide level2">
<h2>Jacobians for reversible jump</h2>
<p>For most moves <span class="math inline">\(J=1\)</span> in this case, except in four cases where the dimension <span class="math inline">\(|M| \in\{1, 2, k_{\max}-1, k_{\max}\}\)</span> and</p>
<ul>
<li><span class="math inline">\(J=2/3\)</span> if <span class="math inline">\(|M|=1\)</span> and we try to add a covariate, or if <span class="math inline">\(|M|=k_{\max}\)</span> and we try to remove a covariate</li>
<li><span class="math inline">\(J=3/2\)</span> if <span class="math inline">\(|M|=2\)</span> and we try to remove a covariate, or if <span class="math inline">\(|M|=k_{\max}-1\)</span> and we try to add the last covariate.</li>
</ul>
</section>
<section id="posterior-weights" class="slide level2">
<h2>Posterior weights</h2>
<p>We can keep track of which variables are active at each iteration of the MCMC and obtain the marginal posterior probability of inclusion through sample proportions.</p>
<p>This methods that explores neighbouring models (Grey code) only works with a limited number of covariates <span class="math inline">\(p &lt; 25.\)</span></p>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-Carvalho.Polson.Scott:2010" class="csl-entry" role="listitem">
Carvalho, C. M., Polson, N. G., &amp; Scott, J. G. (2010). The horseshoe estimator for sparse signals. <em>Biometrika</em>, <em>97</em>(2), 465–480. <a href="https://doi.org/10.1093/biomet/asq017">https://doi.org/10.1093/biomet/asq017</a>
</div>
<div id="ref-Gelman:2013" class="csl-entry" role="listitem">
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). <em>Bayesian data analysis</em> (3rd ed.). Chapman; Hall/CRC. <a href="https://doi.org/10.1201/b16018">https://doi.org/10.1201/b16018</a>
</div>
<div id="ref-George.McCulloch:1993" class="csl-entry" role="listitem">
George, E. I., &amp; McCulloch, R. E. (1993). Variable selection via <span>G</span>ibbs sampling. <em>Journal of the American Statistical Association</em>, <em>88</em>(423), 881–889. <a href="https://doi.org/10.1080/01621459.1993.10476353">https://doi.org/10.1080/01621459.1993.10476353</a>
</div>
<div id="ref-Green:1995" class="csl-entry" role="listitem">
Green, P. J. (1995). <span class="nocase">Reversible jump <span>M</span>arkov chain <span>M</span>onte <span>C</span>arlo computation and <span>B</span>ayesian model determination</span>. <em>Biometrika</em>, <em>82</em>(4), 711–732. <a href="https://doi.org/10.1093/biomet/82.4.711">https://doi.org/10.1093/biomet/82.4.711</a>
</div>
<div id="ref-Holmes:2002" class="csl-entry" role="listitem">
Holmes, C. C., Denison, D. G. T., &amp; Mallick, B. K. (2002). Accounting for model uncertainty in seemingly unrelated regressions. <em>Journal of Computational and Graphical Statistics</em>, <em>11</em>(3), 533–551. <a href="http://www.jstor.org/stable/1391112">http://www.jstor.org/stable/1391112</a>
</div>
<div id="ref-Lewandowski:2009" class="csl-entry" role="listitem">
Lewandowski, D., Kurowicka, D., &amp; Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. <em>Journal of Multivariate Analysis</em>, <em>100</em>(9), 1989–2001. <a href="https://doi.org/10.1016/j.jmva.2009.04.008">https://doi.org/10.1016/j.jmva.2009.04.008</a>
</div>
<div id="ref-Lin.Kim.Uduehi.Keinan:2024" class="csl-entry" role="listitem">
Lin, J. D., Kim, N. Y. J., Uduehi, E., &amp; Keinan, A. (2024). Culture for sale: Unpacking consumer perceptions of cultural appropriation. <em>Journal of Consumer Research</em>. <a href="https://doi.org/10.1093/jcr/ucad076">https://doi.org/10.1093/jcr/ucad076</a>
</div>
<div id="ref-Mitchell.Beauchamp:1988" class="csl-entry" role="listitem">
Mitchell, T. J., &amp; Beauchamp, J. J. (1988). Bayesian variable selection in linear regression. <em>Journal of the American Statistical Association</em>, <em>83</em>(404), 1023–1032. <a href="https://doi.org/10.1080/01621459.1988.10478694">https://doi.org/10.1080/01621459.1988.10478694</a>
</div>
<div id="ref-Park.Casella:2008" class="csl-entry" role="listitem">
Park, T., &amp; Casella, G. (2008). The <span>B</span>ayesian <span>L</span>asso. <em>Journal of the American Statistical Association</em>, <em>103</em>(482), 681–686. <a href="https://doi.org/10.1198/016214508000000337">https://doi.org/10.1198/016214508000000337</a>
</div>
<div id="ref-Piironen.Vehtari:2017" class="csl-entry" role="listitem">
Piironen, J., &amp; Vehtari, A. (2017). Sparsity information and regularization in the horseshoe and other shrinkage priors. <em>Electronic Journal of Statistics</em>, <em>11</em>(2), 5018–5051. <a href="https://doi.org/10.1214/17-ejs1337si">https://doi.org/10.1214/17-ejs1337si</a>
</div>
</div>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="fig/logo_hec_montreal_bleu_web.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/bayesmod");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>