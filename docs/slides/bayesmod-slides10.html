<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-81b5c3e63835cfde897ecd3d35a35a41.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.3">

  <meta name="author" content="Léo Belzile">
  <title>Bayesian modelling</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-da069d641d4916e8549bf8dc2b95f825.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-color="#ff585d" class="quarto-title-block center">
  <h1 class="title">Bayesian modelling</h1>
  <p class="subtitle">Variational inference</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Léo Belzile 
</div>
</div>
</div>

  <p class="date">Last compiled Monday Apr 14, 2025</p>
</section>
<section id="variational-inference" class="slide level2">
<h2>Variational inference</h2>
<p>Laplace approximation provides a heuristic for large-sample approximations, but it fails to characterize well <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span>.</p>
<p>We consider rather a setting where we approximate <span class="math inline">\(p\)</span> by another distribution <span class="math inline">\(g\)</span> which we wish to be close.</p>
<p>The terminology <strong>variational</strong> is synonym for optimization in this context.</p>
</section>
<section id="kullbackleibler-divergence" class="slide level2">
<h2>Kullback–Leibler divergence</h2>
<p>The Kullback–Leibler divergence between densities <span class="math inline">\(f_t(\cdot)\)</span> and <span class="math inline">\(g(\cdot; \boldsymbol{\psi}),\)</span> is <span class="math display">\[\begin{align*}
\mathsf{KL}(f_t \parallel g) &amp;=\int \log \left(\frac{f_t(\boldsymbol{x})}{g(\boldsymbol{x}; \boldsymbol{\psi})}\right) f_t(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}\\
&amp;= \int \log f_t(\boldsymbol{x}) f_t(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} - \int \log g(\boldsymbol{x}; \boldsymbol{\psi}) f_t(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
\\ &amp;= {\color{#c38f16}{\mathsf{E}_{f_t}\{\log f_t(\boldsymbol{X})\}}} - \mathsf{E}_{f_t}\{\log g(\boldsymbol{X}; \boldsymbol{\psi})\}
\end{align*}\]</span> The <span class="math inline">\({\color{#c38f16}{\text{negative entropy}}}\)</span> does not depend on <span class="math inline">\(g(\cdot).\)</span></p>
</section>
<section id="model-misspecification" class="slide level2">
<h2>Model misspecification</h2>
<ul>
<li>The divergence is strictly positive unless <span class="math inline">\(g(\cdot; \boldsymbol{\psi}) \equiv f_t(\cdot).\)</span></li>
<li>The divergence is not symmetric.</li>
</ul>
<p>The Kullback–Leibler divergence notion is central to study of model misspecification.</p>
<ul>
<li>if we fit <span class="math inline">\(g(\cdot)\)</span> when data arise from <span class="math inline">\(f_t,\)</span> the maximum likelihood estimator of the parameters <span class="math inline">\(\widehat{\boldsymbol{\psi}}\)</span> will be the value of the parameter that minimizes the Kullback–Leibler divergence <span class="math inline">\(\mathsf{KL}(f_t \parallel g)\)</span>.</li>
</ul>
</section>
<section id="marginal-likelihood" class="slide level2">
<h2>Marginal likelihood</h2>
<p>Consider now the problem of approximating the marginal likelihood, sometimes called the evidence, <span class="math display">\[\begin{align*}
p(\boldsymbol{y}) = \int_{\boldsymbol{\Theta}} p(\boldsymbol{y}, \boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}.
\end{align*}\]</span> where we only have the joint <span class="math inline">\(p(\boldsymbol{y}, \boldsymbol{\theta})\)</span> is the product of the likelihood times the prior.</p>
</section>
<section id="approximating-the-marginal-likelihood" class="slide level2">
<h2>Approximating the marginal likelihood</h2>
<p>Consider <span class="math inline">\(g(\boldsymbol{\theta};\boldsymbol{\psi})\)</span> with <span class="math inline">\(\boldsymbol{\psi} \in \mathbb{R}^J\)</span> an approximating density function</p>
<ul>
<li>whose integral is one over <span class="math inline">\(\boldsymbol{\Theta} \subseteq \mathbb{R}^p\)</span> (normalized density)</li>
<li>whose support is part of that of <span class="math inline">\(\mathrm{supp} (g) \subseteq \mathrm{supp}(p) = \boldsymbol{\Theta}\)</span> (so KL divergence is not infinite)</li>
</ul>
<p>Objective: minimize the Kullback–Leibler divergence <span class="math display">\[\mathsf{KL}\left\{p(\boldsymbol{\theta} \mid \boldsymbol{y}) \parallel g(\boldsymbol{\theta};\boldsymbol{\psi})\right\}.\]</span></p>
</section>
<section id="problems-ahead" class="slide level2">
<h2>Problems ahead</h2>
<p>Minimizing the Kullback–Leibler divergence is not feasible to evaluate the posterior.</p>
<p>Taking <span class="math inline">\(f_t = p(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span> is not feasible: we need the marginal likelihood to compute the expectation!</p>
</section>
<section id="alternative-expression-for-the-marginal-likelihood" class="slide level2">
<h2>Alternative expression for the marginal likelihood</h2>
<p>We consider a different objective to bound the marginal likelihood. Write</p>
<p><span class="math display">\[\begin{align*}
p(\boldsymbol{y}) = \int_{\boldsymbol{\Theta}}  \frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{g(\boldsymbol{\theta};\boldsymbol{\psi})} g(\boldsymbol{\theta};\boldsymbol{\psi}) \mathrm{d} \boldsymbol{\theta}.
\end{align*}\]</span></p>
</section>
<section id="bounding-the-marginal-likelihood" class="slide level2">
<h2>Bounding the marginal likelihood</h2>
<p>For <span class="math inline">\(h(x)\)</span> a convex function, <strong>Jensen’s inequality</strong> implies that <span class="math display">\[h\{\mathsf{E}(X)\} \leq \mathsf{E}\{h(X)\},\]</span> and applying this with <span class="math inline">\(h(x)=-\log(x),\)</span> we get <span class="math display">\[\begin{align*}
-\log p(\boldsymbol{y}) \leq -\int_{\boldsymbol{\Theta}} \log  \left(\frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{g(\boldsymbol{\theta};\boldsymbol{\psi})}\right) g(\boldsymbol{\theta};\boldsymbol{\psi}) \mathrm{d} \boldsymbol{\theta}.
\end{align*}\]</span></p>
</section>
<section id="evidence-lower-bound" class="slide level2">
<h2>Evidence lower bound</h2>
<p>We can thus consider the model that minimizes the <strong>reverse Kullback–Leibler divergence</strong> <span class="math display">\[\begin{align*}
g(\boldsymbol{\theta}; \widehat{\boldsymbol{\psi}}) = \mathrm{argmin}_{\boldsymbol{\psi}} \mathsf{KL}\{g(\boldsymbol{\theta};\boldsymbol{\psi}) \parallel p(\boldsymbol{\theta} \mid \boldsymbol{y})\}.
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(p(\boldsymbol{\theta}, \boldsymbol{y}) = p(\boldsymbol{\theta} \mid \boldsymbol{y}) p(\boldsymbol{y})\)</span>, <span class="math display">\[\begin{align*}
\mathsf{KL}\{g(\boldsymbol{\theta};\boldsymbol{\psi}) \parallel p(\boldsymbol{\theta} \mid \boldsymbol{y})\} &amp;= \mathsf{E}_{g}\{\log g(\boldsymbol{\theta})\} - \mathsf{E}_g\{\log p( \boldsymbol{\theta}, \boldsymbol{y})\} \\&amp;\quad+ \log p(\boldsymbol{y}).
\end{align*}\]</span></p>
</section>
<section id="evidence-lower-bound-1" class="slide level2">
<h2>Evidence lower bound</h2>
<p>Instead of minimizing the Kullback–Leibler divergence, we can equivalently maximize the so-called <strong>evidence lower bound</strong> (ELBO) <span class="math display">\[\begin{align*}
\mathsf{ELBO}(g) = \mathsf{E}_g\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\} - \mathsf{E}_{g}\{\log g(\boldsymbol{\theta})\}
\end{align*}\]</span></p>
<p>The ELBO is a lower bound for the marginal likelihood because a Kullback–Leibler divergence is non-negative and <span class="math display">\[\begin{align*}
\log p(\boldsymbol{y}) = \mathsf{ELBO}(g) +  \mathsf{KL}\{g(\boldsymbol{\theta};\boldsymbol{\psi}) \parallel p(\boldsymbol{\theta} \mid \boldsymbol{y})\}.
\end{align*}\]</span></p>
</section>
<section id="use-of-elbo" class="slide level2">
<h2>Use of ELBO</h2>
<p>The idea is that we will approximate the density <span class="math display">\[p(\boldsymbol{\theta} \mid \boldsymbol{y}) \approx g(\boldsymbol{\theta}; \widehat{\boldsymbol{\psi}}).\]</span></p>
<ul>
<li>the ELBO can be used for model comparison (but we compare bounds…)</li>
<li>we can sample from <span class="math inline">\(q\)</span> as before.</li>
</ul>
</section>
<section id="heuristics-of-elbo" class="slide level2">
<h2>Heuristics of ELBO</h2>
<p>Maximize the evidence, subject to a regularization term: <span class="math display">\[\begin{align*}
\mathsf{ELBO}(g) = \mathsf{E}_g\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\} - \mathsf{E}_{g}\{\log g(\boldsymbol{\theta})\}
\end{align*}\]</span></p>
<p>The ELBO is an objective function comprising:</p>
<ul>
<li>the first term will be maximized by taking a distribution placing mass near the MAP of <span class="math inline">\(p(\boldsymbol{y}, \boldsymbol{\theta}),\)</span></li>
<li>the second term can be viewed as a penalty that favours high entropy of the approximating family (higher for distributions which are diffuse).</li>
</ul>
</section>
<section id="laplace-vs-variational-approximation" class="slide level2">
<h2>Laplace vs variational approximation</h2>

<img data-src="bayesmod-slides10_files/figure-revealjs/fig-skewed-1.png" width="960" class="r-stretch quarto-figure-center" id="fig-skewed"><p class="caption">
Figure&nbsp;1: Skewed density with the Laplace approximation (dashed orange) and variational Gaussian approximation (dotted blue).
</p></section>
<section id="choice-of-approximating-density" class="slide level2">
<h2>Choice of approximating density</h2>
<p>In practice, the quality of the approximation depends on the choice of <span class="math inline">\(g(\cdot; \boldsymbol{\psi}).\)</span></p>
<ul>
<li>We typically want matching support.</li>
<li>The approximation will be affected by the correlation between posterior components <span class="math inline">\(\boldsymbol{\theta} \mid \boldsymbol{y}.\)</span></li>
<li>Derivations can also be done for <span class="math inline">\((\boldsymbol{U}, \boldsymbol{\theta})\)</span>, where <span class="math inline">\(\boldsymbol{U}\)</span> are latent variables from a data augmentation scheme.</li>
</ul>
</section>
<section id="factorization" class="slide level2">
<h2>Factorization</h2>
<p>We can consider densities <span class="math inline">\(g(;\boldsymbol{\psi})\)</span> that factorize into blocks with parameters <span class="math inline">\(\boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_M,\)</span> where <span class="math display">\[\begin{align*}
g(\boldsymbol{\theta}; \boldsymbol{\psi}) = \prod_{j=1}^M g_j(\boldsymbol{\theta}_j; \boldsymbol{\psi}_j)
\end{align*}\]</span> If we assume that each of the <span class="math inline">\(J\)</span> parameters <span class="math inline">\(\theta_1, \ldots, \theta_J\)</span> are independent, then we obtain a <strong>mean-field</strong> approximation.</p>
</section>
<section id="maximizing-the-elbo-one-step-at-a-time" class="slide level2">
<h2>Maximizing the ELBO one step at a time</h2>
<p><span class="math display">\[\begin{align*}
\mathsf{ELBO}(g) &amp;= \int \log p(\boldsymbol{y}, \boldsymbol{\theta}) \prod_{j=1}^M g_j(\boldsymbol{\theta}_j)\mathrm{d} \boldsymbol{\theta} \\&amp;\quad- \sum_{j=1}^M \int \log \{ g_j(\boldsymbol{\theta}_j) \} g_j(\boldsymbol{\theta}_j) \mathrm{d}  \boldsymbol{\theta}_j
\\&amp; \stackrel{\boldsymbol{\theta}_i}{\propto} \mathsf{E}_{i}\left[\mathsf{E}_{-i}\left\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\right\} \right] - \mathsf{E}_i\left[\log \{ g_i(\boldsymbol{\theta}_i) \}\right]
\end{align*}\]</span> which is the negative of a Kullback–Leibler divergence.</p>
</section>
<section id="optimal-choice-of-approximating-density" class="slide level2">
<h2>Optimal choice of approximating density</h2>
<p>The maximum possible value of zero for the KL is attained when <span class="math display">\[\log \{ g_i(\boldsymbol{\theta}_i) \} = \mathsf{E}_{-i}\left\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\right\}.\]</span> The choice of marginal <span class="math inline">\(g_i\)</span> that maximizes the ELBO is <span class="math display">\[\begin{align*}
g^{\star}_i(\boldsymbol{\theta}_i) \propto \exp \left[ \mathsf{E}_{-i}\left\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\right\}\right].
\end{align*}\]</span> Often, we look at the kernel of <span class="math inline">\(g^{\star}_j\)</span> to deduce the normalizing constant.</p>
</section>
<section id="coordinate-ascent-variational-inference-cavi" class="slide level2">
<h2>Coordinate-ascent variational inference (CAVI)</h2>
<ul>
<li>We can maximize <span class="math inline">\(g^{\star}_j\)</span> in turn for each <span class="math inline">\(j=1, \ldots, M\)</span> treating the other parameters as fixed.</li>
<li>This scheme is guaranteed to monotonically increase the ELBO until convergence to a local maximum.</li>
<li>Convergence: monitor ELBO and stop when the change is lower then some present numerical tolerance.</li>
<li>The approximation may have multiple local optima: perform random initializations and keep the best one.</li>
</ul>
</section>
<section id="example-of-cavi-mean-field-for-gaussian-target" class="slide level2">
<h2>Example of CAVI mean-field for Gaussian target</h2>
<p>We consider the example from Section 2.2.2 of <span class="citation" data-cites="Ormerod.Wand:2010">Ormerod &amp; Wand (<a href="#/references" role="doc-biblioref" onclick="">2010</a>)</span> for approximation of a Gaussian distribution, with <span class="math display">\[\begin{align*}
Y_i &amp;\sim \mathsf{Gauss}(\mu, \tau^{-1}), \qquad i =1, \ldots, n;\\
\mu &amp;\sim \mathsf{Gauss}\{\mu_0, (\tau\tau_0)^{-1}\} \\
\tau &amp;\sim \mathsf{gamma}(a_0, b_0).
\end{align*}\]</span> This is an example where the full posterior is available in closed-form, so we can compare our approximation with the truth.</p>
</section>
<section id="variational-approximation-to-gaussian-mean" class="slide level2">
<h2>Variational approximation to Gaussian — mean</h2>
<p>We assume a factorization of the variational approximation <span class="math inline">\(g_\mu(\mu)g_\tau(\tau);\)</span> the factor for <span class="math inline">\(g_\mu\)</span> is proportional to <span class="math display">\[\begin{align*}
\log g^{\star}_{\mu}(\mu) \propto -\frac{\mathsf{E}_{\tau}(\tau)}{2} \left\{ \sum_{i=1}^n (y_i-\mu)^2-\frac{\tau_0}{2}(\mu-\mu_0)^2\right\},
\end{align*}\]</span> which is quadratic in <span class="math inline">\(\mu\)</span> and thus must be Gaussian with precision <span class="math inline">\(\tau_n = \mathsf{E}_{\tau}(\tau)(\tau_0 + n)\)</span> and mean <span class="math inline">\(\tau_0\mu_0 +n\overline{y}.\)</span></p>
</section>
<section id="variational-approximation-to-gaussian-precision" class="slide level2">
<h2>Variational approximation to Gaussian — precision</h2>
<p>The optimal precision factor satisfies <span class="math display">\[\begin{align*}
\ln g^{\star}_{\tau}(\tau) &amp;\propto \log \tau\left(\frac{n+1}{2} + a_0-1\right)   \\&amp;\quad- \tau {\color{#c38f16}{\left[b_0  + \frac{\mathsf{E}_{\mu}\left\{\sum_{i=1}^n (y_i-\mu)^2\right\}}{2}  + \frac{\tau_0\mathsf{E}{\mu}\left\{(\mu-\mu_0)^2\right\}}{2}\right]}}.
\end{align*}\]</span> Thus a gamma with shape <span class="math inline">\(a_n =a_0 +n/2\)</span> and rate <span class="math inline">\({\color{#c38f16}{b_n}}\)</span>.</p>
</section>
<section id="rate-of-the-gamma-for-g_tau" class="slide level2">
<h2>Rate of the gamma for <span class="math inline">\(g_\tau\)</span></h2>
<p>It is helpful to rewrite the expected value as <span class="math display">\[\begin{align*}
\mathsf{E}_{\mu}\left\{\sum_{i=1}^n (y_i-\mu)^2\right\} = \sum_{i=1}^n \{y_i - \mathsf{E}_{\mu}(\mu)\}^2 + n \mathsf{Var}_{\mu}(\mu),
\end{align*}\]</span> so that it depends on the parameters of the distribution of <span class="math inline">\(\mu\)</span> directly.</p>
</section>
<section id="cavi-for-gaussian" class="slide level2">
<h2>CAVI for Gaussian</h2>
<p>The algorithm cycles through the following updates until convergence:</p>
<ul>
<li><span class="math inline">\(\mathsf{Va}_{\mu}(\mu) = \{\mathsf{E}_{\tau}(\tau)(\tau_0 + n )\}^{-1},\)</span></li>
<li><span class="math inline">\(\mathsf{E}_{\mu}(\mu) = \mathsf{Va}_{\mu}(\mu)\{\tau_0\mu_0 + n \overline{y}\},\)</span></li>
<li><span class="math inline">\(\mathsf{E}_{\tau}(\tau) = a_n/b_n\)</span> where <span class="math inline">\(b_n\)</span> is a function of both <span class="math inline">\(\mathsf{E}_{\mu}(\mu)\)</span> and <span class="math inline">\(\mathsf{Var}_{\mu}(\mu).\)</span></li>
</ul>
<p>We only compute the ELBO at the end of each cycle.</p>
</section>
<section id="maximization" class="slide level2">
<h2>Maximization?</h2>
<p>Recall that alternating these steps is <strong>equivalent</strong> to maximization of the ELBO.</p>
<ul>
<li>each iteration performs conditional optimization implicitly (as we minimize the reverse KL divergence).</li>
</ul>
</section>
<section id="monitoring-convergence" class="slide level2">
<h2>Monitoring convergence</h2>
<p>The derivation of the ELBO is straightforward but tedious; <span class="math display">\[\begin{align*}
\mathsf{ELBO}(g) &amp; = a_0\log(b_0) -a_n\log b_n + \log \{\Gamma(a_n)/\Gamma(a_0)\}
\\&amp; \quad - \frac{n}{2}\log(2\pi)+ \frac{1 + \log (\tau_0/\tau_n)}{2}.
\end{align*}\]</span></p>
<p>We can also consider relative changes in parameter values as tolerance criterion.</p>
</section>
<section id="bivariate-posterior-density" class="slide level2">
<h2>Bivariate posterior density</h2>

<img data-src="bayesmod-slides10_files/figure-revealjs/fig-approx-cavi-gauss-1.png" width="960" class="r-stretch quarto-figure-center" id="fig-approx-cavi-gauss"><p class="caption">
Figure&nbsp;2: Bivariate density posterior for the conjugate Gaussian-gamma model (left) and CAVI approximation (right).
</p></section>
<section id="marginal-posterior-densities" class="slide level2">
<h2>Marginal posterior densities</h2>

<img data-src="bayesmod-slides10_files/figure-revealjs/fig-gauss-cavi-margdens-1.png" width="960" class="r-stretch quarto-figure-center" id="fig-gauss-cavi-margdens"><p class="caption">
Figure&nbsp;3: Marginal posterior density of the mean and precision of the Gaussian (full line), with CAVI approximation (dashed).
</p></section>
<section id="cavi-for-probit-regression" class="slide level2">
<h2>CAVI for probit regression</h2>
<p>A probit regression is a generalized linear model with probability of success <span class="math inline">\(\Phi(\mathbf{x}_i\boldsymbol{\beta}),\)</span> where <span class="math inline">\(\Phi(\cdot)\)</span> is the cumulative distribution function of a standard Gaussian variable.</p>
<p>We can write the model as <span class="math display">\[\begin{align*}
p(\boldsymbol{y} \mid \boldsymbol{\beta}) = \Phi(\mathbf{X}\boldsymbol{\beta})^{\boldsymbol{y}}\Phi(-\mathbf{X}\boldsymbol{\beta})^{\boldsymbol{1}_n -\boldsymbol{y}}
\end{align*}\]</span> since <span class="math inline">\(1-\Phi(x) = \Phi(-x).\)</span></p>
</section>
<section id="data-augmentation-and-cavi" class="slide level2">
<h2>Data augmentation and CAVI</h2>
<p>Consider data augmentation with auxiliary variables <span class="math inline">\(Z_i \mid \boldsymbol{\beta}\sim \mathsf{Gauss}(\mathbf{x}_i\boldsymbol{\beta}, 1).\)</span></p>
<p>With <span class="math inline">\(\boldsymbol{\beta} \sim \mathsf{Gauss}_p(\boldsymbol{\mu}_0, \mathbf{Q}_0^{-1}),\)</span> the model admits conditionals <span class="math display">\[\begin{align*}
\boldsymbol{\beta} \mid \boldsymbol{Z} &amp;\sim \mathsf{Gauss}_p\left\{\mathbf{Q}_{\boldsymbol{\beta}}^{-1}(\mathbf{X}\boldsymbol{Z} + \mathbf{Q}_0\boldsymbol{\mu}_0),  \mathbf{Q}_{\boldsymbol{\beta}}^{-1} \right\}
\\
Z_i \mid y_i, \boldsymbol{\beta} &amp;\sim \mathsf{trunc. Gauss}(\mathbf{x}_i\boldsymbol{\beta}, 1, l_i, u_i)
\end{align*}\]</span> where <span class="math inline">\(\mathbf{Q}_{\boldsymbol{\beta}}= \mathbf{X}^\top\mathbf{X} + \mathbf{Q_0},\)</span> and <span class="math inline">\([l_i, u_i]\)</span> is <span class="math inline">\((-\infty,0)\)</span> if <span class="math inline">\(y_i=0\)</span> and <span class="math inline">\((0, \infty)\)</span> if <span class="math inline">\(y_i=1.\)</span></p>
</section>
<section id="cavi-factorization-for-probit-model" class="slide level2">
<h2>CAVI factorization for probit model</h2>
<p>If we consider a factorization of the form <span class="math display">\[g_{\boldsymbol{Z}}(\boldsymbol{z})g_{\boldsymbol{\beta}}(\boldsymbol{\beta}),\]</span> then we exploit the conditionals in the same way as for Gibbs sampling, but substituting unknown parameter functionals by their expectations.</p>
<p>Furthermore, the optimal form of the density further factorizes as <span class="math display">\[g_{\boldsymbol{Z}}(\boldsymbol{z}) = \prod_{i=1}^n g_{Z_i}(z_i).\]</span></p>
</section>
<section id="updates-for-cavi---probit-regression" class="slide level2">
<h2>Updates for CAVI - probit regression</h2>
<p>The model depends on</p>
<ul>
<li><span class="math inline">\(\mu_{\boldsymbol{Z}}\)</span>, the mean parameter of <span class="math inline">\(\boldsymbol{Z}\)</span></li>
<li><span class="math inline">\(\mu_{\boldsymbol{\beta}},\)</span> the mean of <span class="math inline">\(\boldsymbol{\beta}.\)</span></li>
</ul>
<p>Consider the terms in the posterior proportional to <span class="math inline">\(Z_i\)</span>, where <span class="math display">\[\begin{align*}
p(z_i \mid \boldsymbol{\beta}, y_i) \propto -\frac{z_i^2 - 2z_i \mathbf{x}_i\boldsymbol{\beta}}{2} \times \mathrm{I}(z_i &gt;0)^{y_i}\mathrm{I}(z_i &lt;0)^{1-y_i}
\end{align*}\]</span> which is linear in <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</section>
<section id="truncated-gaussian" class="slide level2">
<h2>Truncated Gaussian</h2>
<p>The expectation of a univariate truncated Gaussian <span class="math inline">\(Z \sim \mathsf{trunc. Gauss}(\mu,\sigma^2, l, u)\)</span> is <span class="math display">\[\begin{align*}
\mathsf{E}(Z) = \mu - \sigma\frac{\phi\{(u-\mu/\sigma)\} - \phi\{(l-\mu/\sigma)\}}{\Phi\{(u-\mu/\sigma)\} - \Phi\{(l-\mu/\sigma)\}}.
\end{align*}\]</span></p>
</section>
<section id="update-for-cavi" class="slide level2">
<h2>Update for CAVI</h2>
<p>If we replace <span class="math inline">\(\mu=\mathbf{x}_i\mu_{\boldsymbol{\beta}}\)</span>, we get the update <span class="math display">\[\begin{align*}
\mu_{Z_i}(z_i) = \begin{cases}
\mathbf{x}_i\mu_{\boldsymbol{\beta}} - \frac{ \phi(\mathbf{x}_i\mu_{\boldsymbol{\beta}})}{1-\Phi(\mathbf{x}_i\mu_{\boldsymbol{\beta}})} &amp; y_i = 0;\\
\mathbf{x}_i\mu_{\boldsymbol{\beta}} + \frac{ \phi(\mathbf{x}_i\mu_{\boldsymbol{\beta}})}{\Phi(\mathbf{x}_i\mu_{\boldsymbol{\beta}})} &amp; y_i = 1,
\end{cases}
\end{align*}\]</span> since <span class="math inline">\(\phi(x)=\phi(-x).\)</span></p>
</section>
<section id="update-for-regression-parameters" class="slide level2">
<h2>Update for regression parameters</h2>
<p>The optimal form for <span class="math inline">\(\boldsymbol{\beta}\)</span> is Gaussian and proceeding similarly, <span class="math display">\[\begin{align*}
\boldsymbol{\mu}_{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X} + \mathbf{Q_0})^{-1}(\mathbf{X}\boldsymbol{\mu}_{\boldsymbol{Z}} + \mathbf{Q}_0\boldsymbol{\mu}_0)
\end{align*}\]</span></p>
<!--
The ELBO is
\begin{align*}
\mathsf{ELBO} = -\frac{1}{2}(\boldsymbol{z} - \mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{z} - \mathbf{X}\boldsymbol{\beta})\right\} \times \prod_{i=1}^n \mathrm{I}(z_i > 0)^{y_i}\mathrm{I}(z_i \le 0)^{1-y_i}
\end{align*}
where use the fact $\mathsf{E}_{Z_i}\{\mathrm{I}(z_i \le 0)\}=$that the entropy of a $p$-dimensional multivariate Gaussian random vector with precision $\mathbf{Q}$ is $p/2\{1+\log(2\pi)\} - \log |\mathbf{Q}|/2.$
-->
</section>
<section id="example" class="slide level2">
<h2>Example</h2>
<p>We consider for illustration purposes data from Experiment 2 of <span class="citation" data-cites="Duke.Amir:2023">Duke &amp; Amir (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span> on the effect of sequential decisions and purchasing formats.</p>
<ul>
<li>We fit a model with age of the participant and the binary variable <code>format</code>, which indicate the experimental condition.</li>
</ul>
</section>
<section id="comments" class="slide level2">
<h2>Comments</h2>
<ul>
<li><p>With vague priors, the coefficients for the mean <span class="math inline">\(\boldsymbol{\mu}_{\boldsymbol{\beta}}=(\beta_0, \beta_1, \beta_2)^\top\)</span> matches the frequentist point estimates of the probit regression to four significant digits.</p></li>
<li><p>Convergence is very fast, as shown by the ELBO plot.</p></li>
<li><p>The marginal density approximations are underdispersed.</p></li>
</ul>
</section>
<section id="elbo-and-marginal-density-approximation" class="slide level2">
<h2>ELBO and marginal density approximation</h2>

<img data-src="bayesmod-slides10_files/figure-revealjs/fig-elbo-CAVI-probit-1.png" width="960" class="r-stretch quarto-figure-center" id="fig-elbo-CAVI-probit"><p class="caption">
Figure&nbsp;4: ELBO (left) and marginal density approximation with true density (full) versus variational approximation (dashed).
</p></section>
<section id="stochastic-optimization" class="slide level2">
<h2>Stochastic optimization</h2>
<p>We consider alternative numeric schemes which rely on stochastic optimization <span class="citation" data-cites="Hoffman:2013">(<a href="#/references" role="doc-biblioref" onclick="">Hoffman et al., 2013</a>)</span>.</p>
<p>The key idea behind these methods is that</p>
<ul>
<li>we can use gradient-based algorithms,</li>
<li>and approximate the expectations with respect to <span class="math inline">\(g\)</span> by drawing samples from it</li>
</ul>
<p>Also allows for minibatch (random subset) selection to reduce computational costs in large samples</p>
</section>
<section id="black-box-variational-inference" class="slide level2">
<h2>Black-box variational inference</h2>
<p><span class="citation" data-cites="Ranganath.Gerrish.Blei:2014">Ranganath et al. (<a href="#/references" role="doc-biblioref" onclick="">2014</a>)</span> shows that the gradient of the ELBO reduces to <span class="math display">\[\begin{align*}
\frac{\partial}{\partial \boldsymbol{\psi}} \mathsf{ELBO}(g) &amp;=\mathsf{E}_{g}\left\{\frac{\partial \log g(\boldsymbol{\theta}; \boldsymbol{\psi})}{\partial \boldsymbol{\psi}} \times \log \left( \frac{p(\boldsymbol{\theta}, \boldsymbol{y})}{g(\boldsymbol{\theta}; \boldsymbol{\psi})}\right)\right\}
\end{align*}\]</span> using the change rule, differentiation under the integral sign (dominated convergence theorem) and the identity <span class="math display">\[\begin{align*}
\frac{\partial \log g(\boldsymbol{\theta}; \boldsymbol{\psi})}{\partial \boldsymbol{\psi}} g(\boldsymbol{\theta}; \boldsymbol{\psi}) = \frac{\partial g(\boldsymbol{\theta}; \boldsymbol{\psi})}{\partial \boldsymbol{\psi}}
\end{align*}\]</span></p>
</section>
<section id="black-box-variational-inference-in-practice" class="slide level2">
<h2>Black-box variational inference in practice</h2>
<ul>
<li>Note that the gradient simplifies for <span class="math inline">\(g_i\)</span> in exponential families (covariance of sufficient statistic with <span class="math inline">\(\log(p/g)\)</span>).</li>
<li>The gradient estimator is particularly noisy, so <span class="citation" data-cites="Ranganath.Gerrish.Blei:2014">Ranganath et al. (<a href="#/references" role="doc-biblioref" onclick="">2014</a>)</span> provide two methods to reduce the variance of this expression using control variates and Rao–Blackwellization.</li>
</ul>
</section>
<section id="automatic-differentiation-variational-inference" class="slide level2">
<h2>Automatic differentiation variational inference</h2>
<p><span class="citation" data-cites="Kucukelbir:2017">Kucukelbir et al. (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span> proposes a stochastic gradient algorithm, but with two main innovations.</p>
<ul>
<li>The first is the general use of Gaussian approximating densities for factorized density, with parameter transformations to map from the support of <span class="math inline">\(T: \boldsymbol{\Theta} \mapsto \mathbb{R}^p\)</span> via <span class="math inline">\(T(\boldsymbol{\theta})=\boldsymbol{\zeta}.\)</span></li>
<li>The second is to use the resulting <strong>location-scale</strong> family to obtain an alternative form of the gradient.</li>
</ul>
</section>
<section id="gaussian-full-rank-approximation" class="slide level2">
<h2>Gaussian full-rank approximation</h2>
<p>Consider an approximation <span class="math inline">\(g(\boldsymbol{\zeta}; \boldsymbol{\psi})\)</span> where <span class="math inline">\(\boldsymbol{\psi}\)</span> consists of</p>
<ul>
<li>mean parameters <span class="math inline">\(\boldsymbol{\mu}\)</span> and</li>
<li>covariance <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, parametrized through a Cholesky decomposition</li>
</ul>
<p>The full approximation is of course more flexible when the transformed parameters <span class="math inline">\(\boldsymbol{\zeta}\)</span> are correlated, but is more expensive to compute than the mean-field approximation.</p>
</section>
<section id="change-of-variable" class="slide level2">
<h2>Change of variable</h2>
<p>The change of variable introduces a Jacobian term <span class="math inline">\(\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})\)</span> for the approximation to the density <span class="math inline">\(p(\boldsymbol{\theta}, \boldsymbol{y})\)</span>, where</p>
<p><span class="math display">\[\begin{align*}
p(\boldsymbol{\theta}, \boldsymbol{y}) = p(\boldsymbol{\zeta}, \boldsymbol{y}) \left|\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})\right|
\end{align*}\]</span></p>
</section>
<section id="gaussian-entropy" class="slide level2">
<h2>Gaussian entropy</h2>
<p>The entropy of the multivariate Gaussian with mean <span class="math inline">\(\boldsymbol{\mu}\)</span> and covariance <span class="math inline">\(\boldsymbol{\Sigma} = \mathbf{LL}^\top\)</span>, where <span class="math inline">\(\mathbf{L}\)</span> is a lower triangular matrix, is <span class="math display">\[\begin{align*}
\mathcal{E}(\mathbf{L}) = - \mathsf{E}_g(\log g) &amp;= \frac{D+D\log(2\pi) + \log |\mathbf{LL}^\top|}{2},
\end{align*}\]</span> and only depends on <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</p>
</section>
<section id="elbo-with-gaussian-approximation" class="slide level2">
<h2>ELBO with Gaussian approximation</h2>
<p>Since the Gaussian is a location-scale family, we can rewrite the model in terms of a standardized Gaussian variable <span class="math inline">\(\boldsymbol{Z}\sim \mathsf{Gauss}_p(\boldsymbol{0}_p, \mathbf{I}_p)\)</span> where <span class="math inline">\(\boldsymbol{\zeta} = \boldsymbol{\mu} + \mathbf{L}\boldsymbol{Z}\)</span> (this transformation has unit Jacobian).</p>
<p>The ELBO with the transformation becomes <span class="math display">\[\begin{align*}
\mathsf{E}_{\boldsymbol{Z}}\left[ \log p\{\boldsymbol{y}, T^{-1}(\boldsymbol{\zeta})\} + \log \left|\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})\right|\right] + \mathcal{E}(\mathbf{L}).
\end{align*}\]</span></p>
</section>
<section id="chain-rule" class="slide level2">
<h2>Chain rule</h2>
<p>If <span class="math inline">\(\boldsymbol{\theta} = T^{-1}(\boldsymbol{\zeta})\)</span> and <span class="math inline">\(\boldsymbol{\zeta} = \boldsymbol{\mu} + \mathbf{L}\boldsymbol{z},\)</span> we have for <span class="math inline">\(\boldsymbol{\psi}\)</span> equal to either <span class="math inline">\(\boldsymbol{\mu}\)</span> or <span class="math inline">\(\mathbf{L}\)</span>, using the chain rule, <span class="math display">\[\begin{align*}
&amp; \frac{\partial}{\partial \boldsymbol{\psi}}\log p(\boldsymbol{y}, \boldsymbol{\theta})
\\&amp;\quad  = \frac{\partial \log p(\boldsymbol{y}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}
   \times \frac{\partial T^{-1}(\boldsymbol{\zeta})}{\partial \boldsymbol{\zeta}}
\times \frac{\partial (\boldsymbol{\mu} + \mathbf{L}\boldsymbol{z})}{\partial \boldsymbol{\psi}}
\end{align*}\]</span></p>
</section>
<section id="gradients-for-advi" class="slide level2 smaller">
<h2>Gradients for ADVI</h2>
<p>The gradients of the ELBO with respect to the mean and variance are <span class="math display">\[\begin{align*}
\nabla_{\boldsymbol{\mu}} &amp;= \mathsf{E}_{\boldsymbol{Z}}\left\{\frac{\partial \log p(\boldsymbol{y}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \frac{\partial T^{-1}(\boldsymbol{\zeta})}{\partial \boldsymbol{\zeta}}  + \frac{\partial \log \left|\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})\right|}{\partial \boldsymbol{\zeta}}\right\} \\
\nabla_{\mathbf{L}} &amp;= \mathsf{E}_{\boldsymbol{Z}}\left[\left\{\frac{\partial \log p(\boldsymbol{y}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \frac{\partial T^{-1}(\boldsymbol{\zeta})}{\partial \boldsymbol{\zeta}}  + \frac{\partial \log \left|\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})\right|}{\partial \boldsymbol{\zeta}}\right\}\boldsymbol{Z}^\top\right] + \mathbf{L}^{-\top}.
\end{align*}\]</span> and we can approximate the expectation by drawing standard Gaussian samples <span class="math inline">\(\boldsymbol{Z}_1, \ldots, \boldsymbol{Z}_B.\)</span></p>
</section>
<section id="quality-of-approximation" class="slide level2">
<h2>Quality of approximation</h2>
<p>Consider the stochastic volatility model.</p>

<img data-src="fig/stochvol-volatility.png" class="quarto-figure quarto-figure-center r-stretch" style="width:90.0%"><p>Fitting HMC-NUTS to the exchange rate data takes 156 seconds for 10K iterations, vs 2 seconds for the mean-field approximation.</p>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-Duke.Amir:2023" class="csl-entry" role="listitem">
Duke, K. E., &amp; Amir, O. (2023). The importance of selling formats: When integrating purchase and quantity decisions increases sales. <em>Marketing Science</em>, <em>42</em>(1), 87–109. <a href="https://doi.org/10.1287/mksc.2022.1364">https://doi.org/10.1287/mksc.2022.1364</a>
</div>
<div id="ref-Hoffman:2013" class="csl-entry" role="listitem">
Hoffman, M. D., Blei, D. M., Wang, C., &amp; Paisley, J. (2013). Stochastic variational inference. <em>Journal of Machine Learning Research</em>, <em>14</em>(40), 1303–1347. <a href="http://jmlr.org/papers/v14/hoffman13a.html">http://jmlr.org/papers/v14/hoffman13a.html</a>
</div>
<div id="ref-Kucukelbir:2017" class="csl-entry" role="listitem">
Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., &amp; Blei, D. M. (2017). Automatic differentiation variational inference. <em>Journal of Machine Learning Research</em>, <em>18</em>(14), 1–45. <a href="http://jmlr.org/papers/v18/16-107.html">http://jmlr.org/papers/v18/16-107.html</a>
</div>
<div id="ref-Ormerod.Wand:2010" class="csl-entry" role="listitem">
Ormerod, J. T., &amp; Wand, M. P. (2010). Explaining variational approximations. <em>The American Statistician</em>, <em>64</em>(2), 140–153. <a href="https://doi.org/10.1198/tast.2010.09058">https://doi.org/10.1198/tast.2010.09058</a>
</div>
<div id="ref-Ranganath.Gerrish.Blei:2014" class="csl-entry" role="listitem">
Ranganath, R., Gerrish, S., &amp; Blei, D. (2014). Black box variational inference. In S. Kaski &amp; J. Corander (Eds.), <em>Proceedings of the seventeenth international conference on artificial intelligence and statistics</em> (Vol. 33, pp. 814–822). Pmlr. <a href="https://proceedings.mlr.press/v33/ranganath14.html">https://proceedings.mlr.press/v33/ranganath14.html</a>
</div>
</div>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="fig/logo_hec_montreal_bleu_web.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/bayesmod");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>