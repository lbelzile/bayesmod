---
title: "Bayesian modelling"
author: "LÃ©o Belzile"
subtitle: "Expectation propagation"
date: today
date-format: "[Last compiled] dddd MMM D, YYYY"
eval: true
echo: true
cache: true
bibliography: MATH80601A.bib
format:
  revealjs:
    slide-number: true
    html-math-method: mathjax
    preview-links: auto
    theme: [simple, hecmontreal.scss]
    title-slide-attributes:
      data-background-color: "#ff585d"
    logo: "fig/logo_hec_montreal_bleu_web.png"
---


```{r}
#| include: false
#| eval: true
#| echo: false
hecbleu <- c("#002855")
fcols <- c(gris = "#888b8d",
           bleu = "#0072ce",
           aqua = "#00aec7",
           vert = "#26d07c",
           rouge = "#ff585d",
           rose = "#eb6fbd",
           jaune = "#f3d03e")
pcols <- c(gris = "#d9d9d6",
           bleu = "#92c1e9",
           agua = "#88dbdf",
           vert = "#8fe2b0",
           rouge = "#ffb1bb",
           rose = "#eab8e4",
           jaune = "#f2f0a1")
library(ggplot2)
theme_set(theme_classic())
library(patchwork)
knitr::opts_chunk$set(fig.retina = 3, collapse = TRUE)
options(digits = 3, width = 75)
```

## Revisiting Kullback--Leibler divergence

The Kullback--Leibler divergence between densities $f_t(\cdot)$ and $g(\cdot; \boldsymbol{\psi}),$ is
\begin{align*}
\mathsf{KL}(f_t \parallel g) &=\int \log \left(\frac{f_t(\boldsymbol{x})}{g(\boldsymbol{x}; \boldsymbol{\psi})}\right) f_t(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}\\
\\ &= \mathsf{E}_{f_t}\{\log f_t(\boldsymbol{X})\} - \mathsf{E}_{f_t}\{\log g(\boldsymbol{X}; \boldsymbol{\psi})\}
\end{align*}

## Forward Kullback--Leibler divergence

If $g(\cdot; \boldsymbol{\mu}, \boldsymbol{\Sigma})$ is Gaussian approximating density, then we minimize the KL divergence by matching moments:
\begin{align*}
\boldsymbol{\mu}^* &= \mathsf{E}_{f_t}(\boldsymbol{X})\\
\boldsymbol{\Sigma}^* &= \mathsf{E}_{f_t}\left\{(\boldsymbol{X}-\boldsymbol{\mu})(\boldsymbol{X}-\boldsymbol{\mu})^\top\right\}
\end{align*}
See Exercise 10.1 for a derivation.



## Variational inference

We don't know the posterior mean and variance! (they depend on unknown normalizing constant).

Variational inference finds rather the approximation that minimizes the **reverse Kullback--Leibler divergence** $\mathsf{KL}(g \parallel f_t).$

Qualitatively, this yields a very different approximation.

## Comparing approximations



```{r}
#| eval: true
#| echo: false
#| message: false
#| label: fig-klvsrev
#| fig-cap: "Approximation of a correlated bivariate Gaussian density by independent Gaussians."
library(ellipse, quietly = TRUE, warn.conflicts = FALSE)
# From Ben Bolker: https://stackoverflow.com/a/36222689

## Reverse vs KL divergence for bivariate Gaussian
mu <- c(2,1)
Sigma <- matrix(c(2,1,1,0.8), nrow = 2, ncol = 2)
Q <- solve(Sigma)
alpha_levels <- seq(0.25, 0.75, by = 0.25) 
names(alpha_levels) <- alpha_levels ## to get id column in result
contour_data <- plyr::ldply(alpha_levels,
                      ellipse,
                      x = cov2cor(Sigma),
                      scale = diag(Sigma), 
                      centre = mu)
contour_data_rKL <- plyr::ldply(alpha_levels,
                      ellipse,
                      x = diag(2),
                      scale = 1/diag(Q),
                      centre = mu)

contour_data_KL <- plyr::ldply(alpha_levels,
                          ellipse,
                          x = diag(2),
                          scale = diag(Sigma),  
                          centre = mu)
g1 <- ggplot() +
  geom_path(data = contour_data,
            mapping = aes(x = x, y = y, group = .id),
            color = "gray10", linetype = "dashed") +
  geom_path(data = contour_data_rKL,
            mapping = aes(x = x, y = y, group = .id)) +
  labs(x = expression(x[1]), y = expression(x[2]),
       subtitle = "reverse Kullback-Leibler (variational)") +
  theme_classic()

g2 <- ggplot() +
  geom_path(data = contour_data,
            mapping = aes(x = x, y = y, group = .id),
            color = "gray10", linetype = "dashed") +
  geom_path(data = contour_data_KL,
            mapping = aes(x = x, y = y, group = .id)) +
  labs(x = expression(x[1]), y = expression(x[2]),
       subtitle = "Kullback-Leibler (EP)") +
  theme_classic()

g1 + g2
```



## Gaussian as exponential family

Write the Gaussian distribution in terms of canonical parameters
\begin{align*}
 q(\boldsymbol{\theta}) \propto \exp \left( - \frac{1}{2} \boldsymbol{\theta}^\top \mathbf{Q}\boldsymbol{\theta} + \boldsymbol{\theta}^\top \boldsymbol{r}\right)
\end{align*}
where $\mathbf{Q}$ is the precision matrix and $\boldsymbol{r}=\mathbf{Q}\boldsymbol{\mu},$ the linear shift.


## Notation


Let $p(\boldsymbol{\theta} \mid \boldsymbol{y})=\exp\{-\psi(\boldsymbol{\theta})\}$ denote the posterior density. 


Since logarithm is a monotonic transform, we can equivalent minimize $\psi(\boldsymbol{\theta})$ to find the posterior mode.

Denote 
- the gradient $\nabla_{\boldsymbol{\theta}} \psi(\boldsymbol{\theta}) = \partial \psi/\partial \boldsymbol{\theta}$
- the Hessian matrix $\mathbf{H}(\boldsymbol{\theta}) = \partial^2 \psi/(\partial \boldsymbol{\theta}\partial \boldsymbol{\theta}^\top).$

## Newton algorithm

Starting from an initial value $\boldsymbol{\theta}_{(0)},$  we consider at step $i$, a second order Taylor series expansion of $\psi(\boldsymbol{\theta})$ around $\boldsymbol{\theta}_{(i)},$ which gives
\begin{align*}
 \psi(\boldsymbol{\theta}) &\approx \psi(\boldsymbol{\theta}_{(i)}) + \nabla_{\boldsymbol{\theta}} \psi(\boldsymbol{\theta}_{(i)})(\boldsymbol{\theta} - \boldsymbol{\theta}_{(i)}) \\& \quad + (\boldsymbol{\theta} - \boldsymbol{\theta}_{(i)})^\top\mathbf{H}(\boldsymbol{\theta}_{(i)})(\boldsymbol{\theta} - \boldsymbol{\theta}_{(i)})
\end{align*}


## Gaussian smoothing

The term $\psi(\boldsymbol{\theta}_{(i)})$ is constant, so if we plug-in this inside the exponential, we obtain
\begin{align*}
 q_{(i+1)}(\boldsymbol{\theta}) &\propto \exp \left\{ - \frac{1}{2} \boldsymbol{\theta}^\top\mathbf{H}(\boldsymbol{\theta}_{(i)}) \boldsymbol{\theta} + \boldsymbol{\theta}^\top\mathbf{H}(\boldsymbol{\theta}_{(i)})\boldsymbol{\theta}_{(i+1)}\right\}
 \end{align*}
 where the mean of the approximation is
 \begin{align*}
 \boldsymbol{\theta}_{(i+1)} = \boldsymbol{\theta}_{(i)} - \mathbf{H}^{-1}(\boldsymbol{\theta}_{(i)}) \nabla_{\boldsymbol{\theta}} \psi(\boldsymbol{\theta}_{(i)}).
\end{align*}

## Side remarks

The new mean vector $\boldsymbol{\theta}_{(i+1)}$ corresponds to a Newton update, and at the same time we have defined a sequence of Gaussian updating approximations.

This scheme works provided that $\mathbf{H}(\boldsymbol{\theta}_{(i)})$ is positive definite and invertible. Without convexity, we get a divergent sequence.

The fixed point to which the algorithm converges is the Laplace approximation.

## Location-scale transformation gradients

For location-scale family, with a Gaussian approximation on the target $\boldsymbol{\theta} = \boldsymbol{\mu} + \mathbf{L}\boldsymbol{Z}$ with $\mathbf{LL}^\top=\boldsymbol{\Sigma}$ and $\boldsymbol{Z} \sim \mathsf{Gauss}_p(\boldsymbol{0}_p, \mathbf{I}_p)$ that the gradient satisfies
\begin{align*}
 \nabla_{\boldsymbol{\mu}}\mathsf{ELBO}(q)&= -\mathsf{E}_{\boldsymbol{Z}}\{\nabla_{\boldsymbol{\theta}}\psi(\boldsymbol{\theta})\} \\
 \nabla_{\mathbf{L}}\mathsf{ELBO}(q)&= -\mathsf{E}_{\boldsymbol{Z}}\{\nabla_{\boldsymbol{\theta}}\psi(\boldsymbol{\theta})\boldsymbol{Z}^\top\} + \mathbf{L}^{-\top}
\end{align*}

## Stein's lemma

Consider $h: \mathbb{R}^d \to \mathbb{R}$ a differentiable function and integration with respect to $\boldsymbol{X} \sim \mathsf{Gauss}_d(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ such that the gradient is absolutely integrable, $\mathsf{E}_{\boldsymbol{X}}\{|\nabla_i h(\boldsymbol{X})|\} < \infty$ for $i=1, \ldots, d.$ Then [@Liu:1994],
\begin{align*}
\mathsf{E}_{\boldsymbol{X}}\left\{h(\boldsymbol{X})(\boldsymbol{X}-\boldsymbol{\mu})\right\} = \boldsymbol{\Sigma}\mathsf{E}_{\boldsymbol{X}}\left\{\nabla h(\boldsymbol{X})\right\}
\end{align*}




## Alternative expression for the scale

If we apply Stein's lemma,
\begin{align*}
  \nabla_{\mathbf{L}}\mathsf{ELBO}(q)&= -\mathsf{E}_{\boldsymbol{Z}}\left\{ \frac{\partial^2 \psi(\boldsymbol{\theta})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\top}\right\}\mathbf{L} + \mathbf{L}^{-\top}.
\end{align*}

## Variational inference

At a critical point, both of these derivatives must be zero, whence
\begin{align*}
 \mathsf{E}_{\boldsymbol{Z}}\{\nabla_{\boldsymbol{\theta}}\psi(\boldsymbol{\theta})\} &= \boldsymbol{0}_p. \\
 \mathsf{E}_{\boldsymbol{Z}}\left\{ \frac{\partial^2 \psi(\boldsymbol{\theta})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\top}\right\} &= \boldsymbol{\Sigma}^{-1}.
\end{align*}

## Variational inference vs Laplace

Compared to the Laplace approximation, the variational Gaussian approximation returns 

- a vector $\boldsymbol{\mu}$ around which the expected value of the gradient is zero 
- and similarly $\boldsymbol{\Sigma}$ for which the  expected value of the curvature (Hessian) is equal to the precision. 

The averaging step is what distinguishes the Laplace and variational approximations.

## References

