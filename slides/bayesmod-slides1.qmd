---
title: "Bayesian modelling"
author: "Léo Belzile, HEC Montréal"
subtitle: "Introduction"
date: today
date-format: YYYY
eval: true
cache: true
echo: true
standalone: true
bibliography: MATH80601A.bib
format:
  revealjs:
    slide-number: true
    preview-links: auto
    theme: [simple, hecmontreal.scss]
    title-slide-attributes:
      data-background-color: "#ff585d"
    logo: "fig/logo_hec_montreal_bleu_web.png"
---


```{r include=FALSE}

hecbleu <- c("#002855")
fcols <- c(gris = "#888b8d",
           bleu = "#0072ce",
           aqua = "#00aec7",
           vert = "#26d07c",
           rouge = "#ff585d",
           rose = "#eb6fbd",
           jaune = "#f3d03e")
pcols <- c(gris = "#d9d9d6",
           bleu = "#92c1e9",
           agua = "#88dbdf",
           vert = "#8fe2b0",
           rouge = "#ffb1bb",
           rose = "#eab8e4",
           jaune = "#f2f0a1")
library(ggplot2)
theme_set(theme_classic())
library(patchwork)
knitr::opts_chunk$set(fig.retina = 3, collapse = TRUE)
options(digits = 3, width = 75)
```



## Distribution and density function

Let $\boldsymbol{X} \in \mathbb{R}^d$ be a random vector with distribution function
\begin{align*}
 F_{\boldsymbol{X}}(\boldsymbol{x}) = \Pr(\boldsymbol{X} \leq \boldsymbol{x}) = \Pr(X_1 \leq x_1, \ldots, X_d \leq x_d).
\end{align*}

If the distribution of $\boldsymbol{X}$ is absolutely continuous, 
\begin{align*}
 F_{\boldsymbol{X}}(\boldsymbol{x}) = \int_{-\infty}^{x_d} \cdots \int_{-\infty}^{x_1} f_{\boldsymbol{X}}(z_1, \ldots, z_d) \mathrm{d} z_1 \cdots \mathrm{d} z_d,
\end{align*}
where $f_{\boldsymbol{X}}(\boldsymbol{x})$ is the joint **density function**.

## Mass function

By abuse of notation, we denote the mass function in the discrete case 
$$0 \leq f_{\boldsymbol{X}}(\boldsymbol{x}) = \Pr(X_1 = x_1, \ldots, X_d = x_d) \leq 1.$$

The support is the set of non-zero density/probability total probability over all points in the support,
$$\sum_{\boldsymbol{x} \in \mathsf{supp}(\boldsymbol{X})} f_{\boldsymbol{X}}(\boldsymbol{x}) = 1.$$

## Marginal distribution

The **marginal distribution** of a subvector $\boldsymbol{X}_{1:k}=(X_1, \ldots, X_k)^\top$ is
\begin{align*}
 F_{\boldsymbol{X}_{1:k}}(\boldsymbol{x}_{1:k}) &= \Pr(\boldsymbol{X}_{1:k} \leq \boldsymbol{x}_{1:k}) \\&= F_{\boldsymbol{X}}(x_1, \ldots, x_k, \infty, \ldots, \infty).
\end{align*}

## Marginal density

The **marginal density** of an absolutely continuous subvector $\boldsymbol{X}_{1:k}=(X_1, \ldots, X_k)^\top$ is
\begin{align*}
 f_{\boldsymbol{X}_{1:k}}(\boldsymbol{x}_{1:k}) = \int_{-\infty}^\infty \cdots  \int_{-\infty}^\infty  f_{\boldsymbol{X}}(x_1, \ldots, x_k, z_{k+1}, \ldots, z_{d}) \mathrm{d} z_{k+1} \cdots \mathrm{d}z_d.
\end{align*}
through integration from the joint density.



## Conditional distribution

The conditional distribution function of $\boldsymbol{Y}$ given $\boldsymbol{X}=\boldsymbol{x}$, is 
\begin{align*}
f_{\boldsymbol{Y} \mid \boldsymbol{X}}(\boldsymbol{y}; \boldsymbol{x}) = \frac{f_{\boldsymbol{X}, \boldsymbol{Y}}(\boldsymbol{x}, \boldsymbol{y})}{f_{\boldsymbol{X}}(\boldsymbol{x})}
\end{align*}
for any value of $\boldsymbol{x}$ in the support of $\boldsymbol{X}$.



## Conditional and marginal for contingency table

Consider a bivariate distribution for $(Y_1, Y_2)$ supported on $\{1,2\} \times \{1, 2, 3\}$ whose joint probability mass function is given in @tbl-bivardiscrete

```{r}
#| eval: true
#| echo: false
#| label: tbl-bivardiscrete
#| tbl-cap: "Bivariate mass function with probability of each outcome for $(Y_1, Y_2)$."
#| tbl-align: 'center'
tab <- rbind(c(0.2, 0.3, 0.1), c(0.15, 0.2, 0.05))
colnames(tab) <- c("$Y_1=1$","$Y_1=2$","$Y_1=3$")
rownames(tab) <- c("$Y_2=1$","$Y_2=2$")
knitr::kable(tab, escape = FALSE)
```

## Calculations for the marginal distribution

The marginal distribution of $Y_1$ is obtain by looking at the total probability for each row/column, e.g., $$\Pr(Y_1=i) = \Pr(Y_1=i, Y_2=1)+ \Pr(Y_1=i, Y_2=2).$$ 

- $\Pr(Y_1=1)=0.35$, $\Pr(Y_1=2)=0.5$, $\Pr(Y_1=3) = 0.15$. 
- $\Pr(Y_2=1)=0.6$ and $\Pr(Y_2=2)=0.4$

## Conditional distribution

The conditional distribution $$\Pr(Y_2 = i \mid Y_1=2) = \frac{\Pr(Y_1=2, Y_2=i)}{\Pr(Y_1=2)},$$ so 
\begin{align*}
\Pr(Y_2 = 1 \mid Y_1=2) &= 0.3/0.5 = 0.6
\\ \Pr(Y_2=2 \mid Y_1=2) &= 0.4.
\end{align*}


## Independence

Vectors $\boldsymbol{Y}$ and $\boldsymbol{X}$ are independent if
\begin{align*}
F_{\boldsymbol{X}, \boldsymbol{Y}}(\boldsymbol{x}, \boldsymbol{y}) = F_{\boldsymbol{X}}(\boldsymbol{x})F_{\boldsymbol{Y}}(\boldsymbol{y})
\end{align*}
for any value of $\boldsymbol{x}$, $\boldsymbol{y}$. 

The joint density, if it exists, also factorizes
\begin{align*}
f_{\boldsymbol{X}, \boldsymbol{Y}}(\boldsymbol{x}, \boldsymbol{y}) = f_{\boldsymbol{X}}(\boldsymbol{x})f_{\boldsymbol{Y}}(\boldsymbol{y}).
\end{align*}

If two subvectors $\boldsymbol{X}$ and $\boldsymbol{Y}$ are independent, then the conditional density $f_{\boldsymbol{Y} \mid \boldsymbol{X}}(\boldsymbol{y}; \boldsymbol{x})$ equals the marginal $f_{\boldsymbol{Y}}(\boldsymbol{y})$.



## Law of iterated expectation and variance

Let $\boldsymbol{Z}$ and $\boldsymbol{Y}$ be random vectors. The expected value of $\boldsymbol{Y}$ is
\begin{align*}
\mathsf{E}_{\boldsymbol{Y}}(\boldsymbol{Y}) = \mathsf{E}_{\boldsymbol{Z}}\left\{\mathsf{E}_{\boldsymbol{Y} \mid \boldsymbol{Z}}(\boldsymbol{Y})\right\}.
\end{align*}

The **tower** property gives a law of iterated variance 
\begin{align*}
\mathsf{Va}_{\boldsymbol{Y}}(\boldsymbol{Y}) = \mathsf{E}_{\boldsymbol{Z}}\left\{\mathsf{Va}_{\boldsymbol{Y} \mid \boldsymbol{Z}}(\boldsymbol{Y})\right\} + \mathsf{Va}_{\boldsymbol{Z}}\left\{\mathsf{E}_{\boldsymbol{Y} \mid \boldsymbol{Z}}(\boldsymbol{Y})\right\}.
\end{align*}


## Poisson distribution

The Poisson distribution has mass
\begin{align*}
f(x)=\mathsf{Pr}(Y=x) = \frac{\exp(-\lambda)\lambda^y}{\Gamma(y+1)}, \quad x=0, 1, 2, \ldots
\end{align*}
where $\Gamma(\cdot)$ denotes the gamma function. 

The parameter $\lambda$ of the Poisson distribution is both the expectation and the variance of the distribution, meaning $$\mathsf{E}(Y)=\mathsf{Va}(Y)=\lambda.$$


## Gamma distribution

A gamma distribution with shape $\alpha>0$ and rate $\beta>0$, denoted $Y \sim \mathsf{gamma}(\alpha, \beta)$, has density
\begin{align*}
f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}\exp(-\beta x), \qquad x \in (0, \infty),
\end{align*}
where $\Gamma(\alpha)\coloneqq\int_0^\infty t^{\alpha-1}\exp(-t)\mathrm{d} t$ is the gamma function.


## Poisson with random scale

To handle overdispersion in count data, take

\begin{align*}
Y \mid \Lambda = \lambda ^\sim \mathsf{Poisson}(\lambda)\\
\Lambda &\sim \mathsf{Gamma}(k\mu, k).
\end{align*}

The joint density of $Y$ and $\Lambda$ is
\begin{align*}
p(y, \lambda) &= p(y \mid \lambda)p(\lambda) \\
&= \frac{\lambda^y\exp(-\lambda)}{\Gamma(y+1)}  \frac{k^{k\mu}\lambda^{k\mu-1}\exp(-k\lambda)}{\Gamma(k\mu)}
\end{align*}

## Conditional distribution

The conditional distribution of $\Lambda \mid Y=y$ can be found by considering only terms that are function of $\lambda$, whence
\begin{align*}
f(\lambda \mid Y=y) \stackrel{\lambda}{\propto}\lambda^{y+k\mu-1}\exp(-(k+1)\lambda)
\end{align*}
so $\Lambda \mid Y=y \sim \mathsf{gamma}(k\mu + y, k+1)$.

## Marginal density of Poisson mean mixture

We can isolate the marginal density
\begin{align*}
p(y) &= \frac{p(y,  \lambda)}{p(\lambda \mid y)} \\&= \frac{\frac{\lambda^y\exp(-\lambda)}{\Gamma(y+1)}  \frac{k^{k\mu}\lambda^{k\mu-1}\exp(-k\lambda)}{\Gamma(k\mu)}}{ \frac{(k+1)^{k\mu+y}\lambda^{k\mu+y-1}\exp\{-(k+1)\lambda\}}{\Gamma(k\mu+y)}}\\
&= \frac{\Gamma(k\mu+y)}{\Gamma(k\mu)\Gamma(y+1)}k^{k\mu} (k+1)^{-k\mu-y}\\&= \frac{\Gamma(k\mu+y)}{\Gamma(k\mu)\Gamma(y+1)}\left(1-\frac{1}{k+1}\right)^{k\mu} \left(\frac{1}{k+1}\right)^y
\end{align*}
a negative binomial distribution with probability of success $1/(k+1)$. 

## Moments

By the laws of iterated expectation and iterative variance, 
\begin{align*}
\mathsf{E}(Y) &= \mathsf{E}_{\Lambda}\{\mathsf{E}(Y \mid \Lambda\} \\& = \mathsf{E}(\Lambda) = \mu\\
\mathsf{Va}(Y) &= \mathsf{E}_{\Lambda}\{\mathsf{Va}(Y \mid \Lambda)\} + \mathsf{Va}_{\Lambda}\{\mathsf{E}(Y \mid \Lambda)\} \\&= \mathsf{E}(\Lambda) + \mathsf{Va}(\Lambda) \\&= \mu + \mu/k.
\end{align*}
The marginal distribution of $Y$, unconditionally, has a variance which exceeds its mean, as
\begin{align*}
\mathsf{E}(Y) = \mu, \qquad \mathsf{Va}(Y) = \mu (1+1/k).
\end{align*}

## Monte Carlo methods

Suppose we can simulate $B$ i.i.d. variables with the same distribution, $X_b \sim F$ $(b=1, \ldots, B).$

We want to compute $\mathsf{E}\{g(X)\}=\mu_g$ for some functional $g(\cdot)$

- $g(x)=x$ (posterior mean)
- $g(x) = \mathsf{I}(x \in A)$ (probability of event)
- etc.

## Monte Carlo methods

We substitute expected value by sample average 
$$\widehat{\mu}_g = \frac{1}{B} \sum_{b=1}^B g(X_b), \qquad X_b \sim F$$

- law of large number guarantees convergence of $\widehat{\mu}_g \to \mu_g$ if the latter is finite.
- Under finite second moments, central limit theorem gives $$\sqrt{B}(\widehat{\mu}_g - \mu_g) \sim \mathsf{No}(0, \sigma^2_g).$$


## Ordinary Monte Carlo

We want to have an estimator as precise as possible.

- but we can't control the variance of $g(X)$, say $\sigma_g^2$
- the more simulations $B$, the lower the variance of the mean. 
- sample average for i.i.d. data has variance $\sigma^2_g/B$
- to reduce the standard deviation by a factor 10, we need $100$ times more draws!


Remember: the answer is **random**.

## Example: functionals of gamma distribution

```{r}
#| label: fig-monte-carlo-path
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Running mean trace plots for $g(x)=\\mathrm{I}(x<1)$ (left), $g(x)=x$ (middle) and $g(x)=1/x$ (right) for a Gamma distribution with shape 0.5 and rate 2, as a function of the Monte Carlo sample size."
set.seed(80601)
B <- 1e5L
Bseq <- seq_len(B)
alpha <- 0.5; beta <- 2
pv <- pgamma(1, shape = alpha, rate = beta)
samp <- rgamma(n = B, shape = alpha, rate = beta)
int1 <- cumsum(samp < 1)/Bseq
int2 <- cumsum(samp)/Bseq
int3 <- cumsum(1/samp)/Bseq
g1 <- ggplot(data = data.frame(
  nsim = Bseq, 
  y = int1,
  mu = pv,
  interv = qnorm(0.975)*sqrt(pv*(1-pv)/Bseq))) +
  geom_ribbon(mapping = aes(x = nsim, 
                           y = mu,
                           ymin = mu - interv,
                           ymax = interv + mu),
               alpha = 0.1) + 
  geom_hline(yintercept = pv, alpha = 0.5) +
  geom_line(aes(x = nsim, y = y)) +
  scale_x_continuous(limits = c(10, NA), trans = "sqrt",
                     labels = scales::unit_format(unit = "K", scale = 1e-3)) +
  scale_y_continuous(limits = c(0.9, 1), expand = c(0,0)) +
  labs(x = "number of simulations",
       y = "",
       subtitle = "Pr(X < 1)")
g2 <- ggplot(data = data.frame(
  nsim = Bseq, 
  y = int2,
  mu = alpha/beta,
  interv = qnorm(0.975)*sqrt(alpha/beta^2/Bseq))) +
  geom_ribbon(mapping = aes(x = nsim, 
                             y = mu,
                           ymin = -interv + mu,
                           ymax = interv + mu),
               alpha = 0.1) + 
  geom_hline(yintercept = alpha/beta, alpha = 0.5) +
  geom_line(mapping = aes(
               x = nsim, 
               y = y)) +
  scale_x_continuous(limits = c(10, NA), trans = "sqrt",
                     labels = scales::unit_format(unit = "K", scale = 1e-3)) +
  scale_y_continuous(limits = c(0.1, 0.4), expand = c(0,0)) +
  labs(x = "number of simulations",
       y = "",
       subtitle = "E(X)")
g3 <- ggplot(data = data.frame(nsim = Bseq, y = int3),
             mapping = aes(x = nsim, y = y)) +
  geom_line() +
  scale_x_continuous(limits = c(10, NA), trans = "sqrt",
                     labels = scales::unit_format(unit = "K", scale = 1e-3)) +
  labs(x = "number of simulations",
       y = "",
       subtitle = "E(1/X) (divergent)")
g1 + g2 + g3 & theme_classic()
```

## Simulation algorithms: inversion method

If $F$ is an absolutely continuous distribution function, then 
$$F(X) \sim \mathsf{U}(0,1).$$
The inversion method consists in applying the quantile function $F^{-1}$ to $U \sim \mathsf{U}(0,1)$, viz. $$F^{-1}(U) \sim X.$$

## Inversion method for truncated distributions

Consider a random variable $Y$ with distribution function $F$.

If $X$ follows the same distribution as $Y$, but restricted over the interval $[a,b]$, then 
$$\Pr(X \leq x) = \frac{F(x) - F(a)}{F(b)-F(a)}, \qquad a \leq x \leq b,$$

Therefore, $$F^{-1}[F(a) + \{F(b)-F(a)\}U] \sim X$$



## Simulation algorithms: accept-reject

- **Target**: sample from density $p(x)$ (hard to sample from)
- **Proposal**: find a density $q(x)$ with nested support, $\mathrm{supp}(p) \subseteq \mathrm{supp}(q)$, such that 
$$\frac{p(x)}{q(x)} \leq C, \quad C \geq 1.$$

## Rejection sampling algorithm

1. Generate $X$ from proposal with density $q(x)$.
2. Compute the ratio $R \gets p(X)/ q(X)$.
3. If $CU \leq R$ for $U \sim \mathsf{U}(0,1)$, return $X$, else go back to step 1.


## Remarks on rejection sampling

- Acceptance rate is $1/C$
   - we need on average $C$ draws from $q$ to get one from $p$
- $q$ must be more heavy-tailed than $p$
   - e.g., $q(x)$ Student-$t$ for $p(x)$ Gaussian
- $q$ should be cheap and easy to sample from!

## Designing a good proposal density

Good choices must satisfy the following constraints: 

- pick a family $q(x)$ so that $$C = \mathrm{sup}_x \frac{p(x)}{q(x)}$$ is as close to 1 as possible.
- you can use numerical optimization with $f(x) =\log p(x) - \log q(x)$ to find the mode $x^\star$ and the upper bound $C = \exp f(x^\star)$.

## Accept-reject illustration


```{r}
#| eval: true
#| echo: false
#| label: fig-acceptreject
#| fig-cap: "Target density (full) and scaled proposal density (dashed): the vertical segment at $x=1$ shows the percentage of acceptance for a uniform slice under the scaled proposal, giving an acceptance ratio of 0.58."
ptarget <- function(x){
  0.3*TruncatedNormal::dtmvnorm(x = cbind(x), mu = 2, sigma = matrix(0.25),lb = 0) + 
    0.4*TruncatedNormal::dtmvnorm(x = cbind(x), mu = 0, sigma = matrix(0.5),lb = 0, ub = 3) + 0.3*TruncatedNormal::dtmvnorm(x = cbind(x), mu = 0, sigma = matrix(4),lb = 0, ub = 5)
}

Cst <- dgamma(x = 4, shape = 4, rate = 1/0.75)/ptarget(2)
qprop <- function(x){1.02*dgamma(x = x+2, shape = 4, rate = 1/0.75)/Cst}
# curve(ptarget, from = -0.2, to = 4, n = 1001, ylim = c(0, 2))
# curve(qprop, from = 0, to = 4, add = TRUE, lty = 2)
ggplot() +
  stat_function(fun = ptarget, n = 1001, xlim = c(0, 5)) +
  stat_function(fun = qprop, n = 1001, xlim = c(0, 5), linetype = "dashed") +
  geom_segment(data = data.frame(
    x0 = 1, x1 = 1, y0 = 0, y1 = qprop(1)),
               mapping = aes(
                 x = x0, y = y0, xend = x1, yend = y1), 
    linewidth = 2) +
  geom_segment(data = data.frame(
    x0 = 1, x1 = 1, y0 = 0, y1 = ptarget(1)),
               mapping = aes(
                 x = x0, y = y0, xend = x1, yend = y1),
    linewidth = 2, col = "grey") +
  labs(x = "x", 
       y = "", 
       subtitle = "scaled proposal and target densities") +
    scale_y_continuous(limits = c(0, 0.62), expand = c(0,0)) + 
    scale_x_continuous(expand = c(0,0)) + 
  theme_classic() 
```

## Truncated Gaussian via accept-reject

Consider sampling $Y \sim \mathsf{No}(\mu, \sigma^2)$, but truncated in the interval $(a, b)$. The target density is
\begin{align*}
p(x; \mu, \sigma, a, b) = \frac{1}{\sigma}\frac{\phi\left(\frac{x-\mu}{\sigma}\right)}{\Phi(\beta)-\Phi(\alpha)}.
\end{align*}
for $\alpha= (a-\mu)/\sigma$ and $\beta = (b-\mu)/\sigma$.
where $\phi(\cdot), \Phi(\cdot)$ are respectively the density and distribution function of the standard Gaussian distribution.

## Accept-reject (crude version)

1. Simulate $X \sim \mathsf{No}(\mu, \sigma^2)$
2. reject any draw if $X < a$ or $X> b$. 

The acceptance rate is $C^{-1} = \{\Phi(\beta) - \Phi(\alpha)\}$

```{r}
#| eval: true
#| echo: true
# Standard Gaussian truncated on [0,1]
candidate <- rnorm(1e5)
trunc_samp <- candidate[candidate >= 0 & candidate <= 1]
# Acceptance rate
length(trunc_samp)/1e5
# Theoretical acceptance rate
pnorm(1)-pnorm(0)
```

## Accept-reject for truncated Gaussian  {.smaller}

Since the Gaussian is a location scale family, the inversion method gives
\begin{align*}
X \sim \mu + \sigma\Phi^{-1}\left[\Phi(\alpha) + \{\Phi(\beta)-\Phi(\alpha)\}U\right]
\end{align*}

We however need to evaluate $\Phi$ numerically (no closed-form expression).

The method fails for *rare event* simulation because the computer returns

- $\Phi(x) = 0$ for $x \leq -39$
- $\Phi(x)=1$ for $x \geq 8.3$,

implying that $a \leq 8.3$ for this approach to work [@LEcuyer.Botev:2017].

## Simulating tails of Gaussian variables {.smaller}

We consider simulation from a standard Gaussian truncated above $a>0$

Write the density of the truncated Gaussian as  [@Devroye:1986, p.381]$$f(x) = \frac{\exp(-x^2/2)}{\int_{a}^{\infty}\exp(-z^2/2)\mathrm{d} z}  =\frac{\exp(-x^2/2)}{c_1}.$$

Note that, for $x \geq a$, 
$$c_1f(x) \leq \frac{x}{a}\exp\left(-\frac{x^2}{2}\right)= a^{-1}\exp\left(-\frac{a^2}{2}\right)g(x);$$
where $g(x)$ is the density of a Rayleigh variable shifted by $a$.^[The constant $C= \exp(-a^2/2)(c_1a)^{-1}$ approaches 1 quickly as $a \to \infty$ (asymptotically optimality). ]

## Accept-reject: truncated Gaussian with Rayleigh  {.smaller}

The shifted Rayleigh has distribution function $$G(x) = 1-\exp\{(a^2-x^2)/2\}, x \geq a.$$ 




:::{.callout-important}

## Marsaglia algorithm
1. Generate a shifted Rayleigh  above $a$, $X \gets  \{a^2 - 2\log(U)\}^{1/2}$ for $U \sim \mathsf{U}(0,1)$
2. Accept $X$ if $XV \leq a$, where $V \sim \mathsf{U}(0,1)$.
:::

For sampling on $[a,b]$, propose from a Rayleigh truncated above at $b$ [@LEcuyer.Botev:2017].

```{r}
#| eval: true
#| echo: true
#| label: marsaglia-algo
a <- 8.3
niter <- 1000L
X <- sqrt(a^2 + 2*rexp(niter))
samp <- X[runif(niter)*X <= a]
```



## References
