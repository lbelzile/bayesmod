---
title: "Bayesian modelling"
author: "LÃ©o Belzile"
subtitle: "Markov chain Monte Carlo"
date: today
date-format: YYYY
eval: true
echo: true
cache: true
bibliography: MATH80601A.bib
format:
  revealjs:
    slide-number: true
    preview-links: auto
    theme: [simple, hecmontreal.scss]
    title-slide-attributes:
      data-background-color: "#ff585d"
    logo: "fig/logo_hec_montreal_bleu_web.png"
---


```{r}
#| include: false
#| eval: true
#| echo: false
hecbleu <- c("#002855")
fcols <- c(gris = "#888b8d",
           bleu = "#0072ce",
           aqua = "#00aec7",
           vert = "#26d07c",
           rouge = "#ff585d",
           rose = "#eb6fbd",
           jaune = "#f3d03e")
pcols <- c(gris = "#d9d9d6",
           bleu = "#92c1e9",
           agua = "#88dbdf",
           vert = "#8fe2b0",
           rouge = "#ffb1bb",
           rose = "#eab8e4",
           jaune = "#f2f0a1")
library(ggplot2)
theme_set(theme_classic())
library(patchwork)
knitr::opts_chunk$set(fig.retina = 3, collapse = TRUE)
options(digits = 3, width = 75)
```

## Bayesian inference beyond conjugate models

How to circumvent the problem of intractable posteriors?

- simulation-based methods: accept-reject, Markov chain Monte Carlo, particle filters, etc.
- deterministic methods: (integrated nested) Laplace approximations, variational Bayes, expectation propagation, etc.

We focus on Monte Carlo methods.


## Markov chains

Plain ordinary Monte Carlo is great, but few algorithms are generic enough to be useful in complex high-dimensional problems.

We will instead typically build Markov chains that target an invariant stationary distribution. 


## Caveats?

Markov chain Monte Carlo methods generate **correlated** draws. 

**Questions**: 

1. can we use them as ordinary independent samples? 
2. what is the price to pay?

We need to do a little theoretical detour to answer these questions.

## Stationarity and Markov property


A stochastic process is **(weakly) stationary** if 

- the distribution of $\{X_1, \ldots, X_t\}$ is the same as that of $\{X_{n+1}, \ldots X_{t+n}\}$ for any value of $n$ and given $t$.

A stochastic process is **Markov** if 

- it satisfies the Markov property: given the current state of the chain, the future only depends on the current state and not on the past.

## Autoregressive process of order 1

Consider a first-order autoregressive process, or $\mathsf{AR}(1)$, 

$$Y_t = \mu + \phi(Y_{t-1} - \mu) + \varepsilon_t,$$ where 

- $\phi$ is the lag-one correlation, 
- $\mu$ the global mean
- $\varepsilon_t$ is an iid innovation with mean zero and variance $\sigma^2$

If $|\phi| < 1$, the process is stationary, otherwise variance increases with $t$

## Variance of a stationary distribution

For a correlated sequence, the variance of the stationary distribution is \begin{align*}
\tau^2 = \mathsf{Va}(Y_t) + 2 \sum_{k=1}^\infty \mathsf{Co}(Y_t, Y_{t-k}).
\end{align*}

 - for i.i.d. data, $\tau^2 = \mathsf{Va}(Y_t)$
 - for stationary $\mathsf{AR}(1)$ process, we get $\sigma^2/(1-\phi^2)$ (geometric series)

## Variance of sample average

Intuitively, a sample of correlated observations carries less information than an independent sample of draws.

We want the variance of the sample average, which is
\begin{align*}
\mathsf{Va}\left(\overline{Y}_T\right) = \frac{1}{T}\sum_{t=1}^T \mathsf{Va}(Y_t) + \frac{2}{T} \sum_{t=1}^{T-1}\sum_{s = t+1}^T \mathsf{Co}(Y_t, Y_s).
\end{align*}

If the process is stationary, the covariances at lag $k$ are the same regardless of the time index and the unconditional variance is constant.

## Variance of sample average, redux

If a central limit theorem applies, the limiting variance of the sample mean simplifies to
\begin{align*}
\lim_{T \to \infty} T\mathsf{Va}\left(\overline{Y}_T\right) = \tau^2 \left\{1+2\sum_{t=1}^\infty \gamma_t\right\}.
\end{align*}
which is a function of 

- the unconditional variance $\tau^2$
- the lag-$k$ autocorrelation $\mathsf{Cor}(Y_{t}, Y_{t+k})=\gamma_k$

## Correlogram


```{r}
#| eval: true
#| echo: false
#| fig-height: 4
#| fig-width: 10
#| label: fig-correlograms
#| fig-cap: "Correlogram of two two Markov chains. These plots, often called acf or autocorrelation functions, show the lag-k sample autocorrelation against lag number."
chains <- cbind("chain 1" = arima.sim(model = list(ar = c(0.5,-0.1), 
                                       ma = c(0.4)), n = 10000),
                "chain 2" = arima.sim(model = list(ar = 0.99), n = 10000))
bayesplot::mcmc_acf(x = coda::as.mcmc(chains))
```

## Variance of sample mean of AR(1)

The lag-$k$ correlation of the stationary autoregressive process of order 1 is $\phi^k$, so $$T\mathsf{Va}\left(\overline{Y}_T\right)=\sigma^2(1+\phi)/(1-\phi).$$

For an independent sample, we have $$T\mathsf{Va}\left(\overline{Y}_T\right)=\sigma^2/(1-\phi^2).$$ 




## Inefficiency curve for AR(1) {.smaller}

```{r}
#| label: fig-ar1-variance
#| fig-width: 8
#| fig-height: 4
#| fig-cap: "Left: scaled asymptotic variance of the sample mean for AR(1) (full line) and independent observations with the same marginal variance (dashed). Right: variance ratio for positive correlations."
#| message: false
#| warning: false
#| echo: false
#| eval: true
var2 <- function(rho){(1+rho)/(1-rho)}
var1 <- function(rho){1/(1-rho^2)}
var_ratio <- function(rho){var2(rho)/var1(rho)}
g1 <- ggplot() +
  stat_function(fun = var1, xlim = c(-0.9,0.9), n = 1001L, linetype = "dashed") +
  stat_function(fun = var2, xlim = c(-0.9,0.9), n = 1001L) +
  labs(y = "", x = expression(phi)) +
  theme_classic()
g2 <- ggplot() +
  stat_function(fun = var_ratio,
                xlim = c(0,1), 
                n = 1001L) +
  labs(y = "") + 
  theme_classic()  
g1 + g2
```


To get the same precision for the mean of $\mathsf{AR}(1)$ process with $\phi \approx 0.75$ than with i.i.d. data, we would need 9 times as many observations.

## Morale of the story 

The price to pay for having correlated samples is 

:::{style="font-size: 2em; color: #ff585d; text-align: center"}

**inefficiency**

:::

The higher the autocorrelation, the larger the variability of our estimators.

## When can we use Markov chains?

If a Markov chain is irreducible and acyclic, it has a unique stationary distribution.

- irreducibility: means that the chain can move from anywhere to anywhere, so it doesn't get stuck in part of the space forever.
- acyclic: cyclical chains loop around and visit periodically a state

Ergodic theorem is our guarantee of convergence.

## Examples

Consider discrete Markov chains over the integers $1, 2, 3$ with transition matrices

$$
P_1 = \begin{pmatrix}
0.5 & 0.3 & 0.2 \\
0 & 0.4 & 0.6 \\
0 & 0.5 & 0.5
\end{pmatrix}, 
\quad 
P_2 = \begin{pmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{pmatrix}.
$$
Chain 1 is reducible to $\{2, 3\}$, chain 2 is cyclical.

## Convergence of Markov chains


```{r}
#| label: fig-discrete-markov-chain
#| eval: true
#| echo: false
#| cache: true
#| fig-cap: "Discrete Markov chain on integers from 1 to 5, with traceplot of 1000 first iterations (left) and running mean plots of sample proportion of each state visited (right)."
set.seed(1234)
P3 <- rbind(c(4,2,0,0,0),
            c(1,4,1,0,0),
            c(0,1,4,1,0),
            c(0,0,1,4,1),
            c(0,0,0,2,4)) / 6
chain <- integer(1e5L)
state <- sample.int(n = 5, size = 1)
for(i in seq_along(chain)){
  state <- sample.int(n = 5, size = 1, prob = P3[state,])
  chain[i] <- state
}
library(ggplot2)
library(patchwork)
g1 <- ggplot(data = data.frame(iteration = 1:1000,
                               state = chain[1:1000]),
             mapping = aes(x = iteration, y = state)) +
  geom_line() +
  theme_classic() +
  labs(y = "", subtitle = "state")
iter <- seq(from = 100, to = length(chain), by = 100)
sub <- sapply(iter,
              function(n){
                as.numeric(table(chain[seq_len(n)])/n)
                }) 
g2 <- ggplot(
  data = data.frame(
    x = rep(iter, each =5),
    y = c(sub),
    state = factor(rep(1:5, length.out = length(sub)))),
  mapping = aes(x = x, group = state, color = state, y = y)) +
  geom_line() +
  scale_x_continuous(trans = "sqrt", labels = scales::unit_format(unit = "K", scale = 1e-3)) + 
  labs(y = "",
       subtitle = "probability of state", 
       x = "iteration") +
  MetBrewer:::scale_color_met_d("Hiroshige") +
  theme_classic() +
  theme(legend.position = "bottom")
g1 + g2
```

## Markov chain Monte Carlo

We consider simulating from a distribution with associated density function $\propto p(\boldsymbol{\theta})$.

- known up to a normalizing factor not depending on $\boldsymbol{\theta}$.

We use $q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^*)$ as transition kernel to generate proposals.


## Metropolis--Hastings algorithm 

Starting from an initial value $\boldsymbol{\theta}_0$:

1.  draw a proposal value $\boldsymbol{\theta}_t^{\star} \sim q(\boldsymbol{\theta} \mid \boldsymbol{\theta}_{t-1})$.
2.  Compute the acceptance ratio $$
    R = \frac{p(\boldsymbol{\theta}_t^{\star})}{p(\boldsymbol{\theta}_{t-1})}\frac{q(\boldsymbol{\theta}_{t-1} \mid \boldsymbol{\theta}_t^{\star} )}{q(\boldsymbol{\theta}_t^{\star} \mid \boldsymbol{\theta}_{t-1})}
    $$
3.  With probability $\min\{R, 1\}$, accept the proposal and set $\boldsymbol{\theta}_t \gets \boldsymbol{\theta}_t^{\star}$, otherwise set the value to the previous state, $\boldsymbol{\theta}_t \gets \boldsymbol{\theta}_{t-1}$.

## Interpretation

- If $R>1$, the proposal has higher density and we always accept the move. 
- If we reject the move, the Markov chain stays at the current value, which induces autocorrelation.
- Since the acceptance probability depends only on the density through ratios, normalizing factors of $p$ and $q$ cancel out.


## Symmetric proposals and random walk

If the proposal is symmetric, the ratio of proposal densities is $$q(\boldsymbol{\theta}_{t-1} \mid \boldsymbol{\theta}_t^{\star} ) / q(\boldsymbol{\theta}_t^{\star} \mid \boldsymbol{\theta}_{t-1}) = 1.$$

Common examples include random walk proposals
$$\boldsymbol{\theta}_t^{\star} \gets \boldsymbol{\theta}_{t-1} + \tau Z, \qquad Z$$ where $Z$ is a mean zero, variance one random variable.

## Independent proposals

- If we pick instead a global proposal, we must ensure that $q$ samples in far regions (recall rejection sampling), otherwise ...
- Good proposals include heavy tailed distribution such as Student-$t$ with small degrees of freedom, centered at the maximum a posteriori $\widehat{\boldsymbol{\theta}}$ and with scale matrix $-\mathbf{H}^{-1}(\boldsymbol{\theta}_t^{\star})$, where $\mathbf{H}(\cdot)$ is the Hessian of the log posterior.


## Upworthy data example

We model the Poisson rates for headlines with questions or not. Our model is
\begin{align*}
Y_{i} &\sim \mathsf{Po}(n_i\lambda_i), \qquad (i=1,2)\\
\lambda_1 &= \exp(\beta + \kappa) \\
\lambda_2 &= \exp(\beta) \\
\beta & \sim \mathsf{No}(\log 0.01, 1.5) \\
\kappa &\sim \mathsf{No}(0, 1)
\end{align*}

## Implementation details: data and containers

In regression models, scale inputs if possible.

```{r}
#| eval: false
#| echo: true
#| # Load data
data(upworthy_question, package = "hecbayes")
# Compute sufficient statistics
data <- upworthy_question |>
  dplyr::group_by(question) |>
  dplyr::summarize(ntot = sum(impressions),
                   y = sum(clicks))
# Create containers for MCMC
niter <- 1e4L
chain <- matrix(0, nrow = niter, ncol = 2L)
colnames(chain) <- c("beta","kappa")
```


## Implementation details: log posterior function

Perform all calculations on the log scale to avoid numerical overflow! 

```{r}
#| eval: false
#| echo: true
# Code log posterior as sum of log likelihood and log prior
loglik <- function(par, counts = data$y, offset = data$ntot, ...){
  lambda <- exp(c(par[1] + log(offset[1]), par[1] + par[2] + log(offset[2])))
 sum(dpois(x = counts, lambda = lambda, log = TRUE))
}
# Note common signature of function
logprior <- function(par, ...){
  dnorm(x = par[1], mean = log(0.01), sd = 1.5, log = TRUE) +
    dnorm(x = par[2], log = TRUE)
}
logpost <- function(par, ...){
  loglik(par, ...) + logprior(par, ...)
}
```

## Implementation details: proposals

Use good starting values for your Markov chains, such as maximum a posteriori.

```{r}
#| eval: false
#| echo: true
# Compute maximum a posteriori (MAP)
map <- optim(
  par = c(-4, 0.07),
  fn = logpost,
  control = list(fnscale = -1),
  offset = data$ntot,
  counts = data$y,
  hessian = TRUE)
# Use MAP as starting value
cur <- map$par
# Compute logpost_cur - we can keep track of this to reduce calculations
logpost_cur <- logpost(cur)
# Proposal covariance
cov_map <- -2*solve(map$hessian)
chol <- chol(cov_map)
```

## Implementation details: Metropolis--Hastings algorithm

Use seed for reproducibility, do not compute posterior twice, compute log of acceptance ratio. 

```{r}
#| eval: false
#| echo: true
set.seed(80601)
naccept <- 0L
for(i in seq_len(niter)){
  # Multivariate normal proposal - symmetric random walk
  prop <- c(rnorm(n = 2) %*% chol + cur)
  logpost_prop <- logpost(prop)
  logR <- logpost_prop - logpost_cur
  if(logR > -rexp(1)){
    cur <- prop
    logpost_cur <- logpost_prop
    naccept <- naccept + 1L
  }
  chain[i,] <- cur
}
```

## Implementation details: analysis of output

Need specialized methods to compute standard errors of the posterior mean.

```{r}
#| eval: false
#| echo: true
# Posterior summaries
summary(coda::as.mcmc(chain))
# Computing standard errors using batch means
sqrt(diag(mcmc::olbm(chain, batch.length = niter/40)))
```

```
1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean       SD  Naive SE Time-series SE
beta  -4.51268 0.001697 1.697e-05      6.176e-05
kappa  0.07075 0.002033 2.033e-05      9.741e-05

2. Quantiles for each variable:

          2.5%      25%      50%      75%    97.5%
beta  -4.51591 -4.51385 -4.51273 -4.51154 -4.50929
kappa  0.06673  0.06933  0.07077  0.07212  0.07463
```


## References

