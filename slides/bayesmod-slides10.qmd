---
title: "Bayesian modelling"
author: "LÃ©o Belzile"
subtitle: "Variational inference"
date: today
date-format: "[Last compiled] dddd MMM D, YYYY"
eval: true
echo: true
cache: true
bibliography: MATH80601A.bib
format:
  revealjs:
    slide-number: true
    html-math-method: mathjax
    preview-links: auto
    theme: [simple, hecmontreal.scss]
    title-slide-attributes:
      data-background-color: "#ff585d"
    logo: "fig/logo_hec_montreal_bleu_web.png"
---


```{r}
#| include: false
#| eval: true
#| echo: false
hecbleu <- c("#002855")
fcols <- c(gris = "#888b8d",
           bleu = "#0072ce",
           aqua = "#00aec7",
           vert = "#26d07c",
           rouge = "#ff585d",
           rose = "#eb6fbd",
           jaune = "#f3d03e")
pcols <- c(gris = "#d9d9d6",
           bleu = "#92c1e9",
           agua = "#88dbdf",
           vert = "#8fe2b0",
           rouge = "#ffb1bb",
           rose = "#eab8e4",
           jaune = "#f2f0a1")
library(ggplot2)
theme_set(theme_classic())
library(patchwork)
knitr::opts_chunk$set(fig.retina = 3, collapse = TRUE)
options(digits = 3, width = 75)
```

## Variational inference

Laplace approximation provides a heuristic for large-sample approximations, but it fails to characterize well $p(\boldsymbol{\theta} \mid \boldsymbol{y})$.

We consider rather a setting where we approximate $p$ by another distribution $g$ which we wish to be close.


The terminology **variational** is synonym for optimization in this context.

## Kullback--Leibler divergence

The Kullback--Leibler divergence between densities $f_t(\cdot)$ and $g(\cdot; \boldsymbol{\psi}),$ is
\begin{align*}
\mathsf{KL}(f_t \parallel g) &=\int \log \left(\frac{f_t(\boldsymbol{x})}{g(\boldsymbol{x}; \boldsymbol{\psi})}\right) f_t(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}\\
&= \int \log f_t(\boldsymbol{x}) f_t(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} - \int \log g(\boldsymbol{x}; \boldsymbol{\psi}) f_t(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
\\ &= {\color{#c38f16}{\mathsf{E}_{f_t}\{\log f_t(\boldsymbol{X})\}}} - \mathsf{E}_{f_t}\{\log g(\boldsymbol{X}; \boldsymbol{\psi})\}
\end{align*}
The ${\color{#c38f16}{\text{negative entropy}}}$ does not depend on $g(\cdot).$


## Model misspecification


- The divergence is strictly positive unless $g(\cdot; \boldsymbol{\psi}) \equiv f_t(\cdot).$
- The divergence is not symmetric.


The Kullback--Leibler divergence notion is central to study of model misspecification.

- if we fit $g(\cdot)$ when data arise from $f_t,$ the maximum likelihood estimator of the parameters $\widehat{\boldsymbol{\psi}}$ will be the value of the parameter that minimizes the  Kullback--Leibler divergence $\mathsf{KL}(f_t \parallel g)$.



## Marginal likelihood

Consider now the problem of approximating the marginal likelihood, sometimes called the evidence,
\begin{align*}
p(\boldsymbol{y}) = \int_{\boldsymbol{\Theta}} p(\boldsymbol{y}, \boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}.
\end{align*}
where we only have the joint $p(\boldsymbol{y}, \boldsymbol{\theta})$ is the product of the likelihood times the prior.

## Approximating the marginal likelihood

Consider $g(\boldsymbol{\theta};\boldsymbol{\psi})$ with $\boldsymbol{\psi} \in \mathbb{R}^J$ an approximating density function 

- whose integral is one over $\boldsymbol{\Theta} \subseteq \mathbb{R}^p$ (normalized density)
- whose support is part of that of $\mathrm{supp} (g) \subseteq \mathrm{supp}(p) = \boldsymbol{\Theta}$ (so KL divergence is not infinite)

Objective: minimize the Kullback--Leibler divergence $$\mathsf{KL}\left\{p(\boldsymbol{\theta} \mid \boldsymbol{y}) \parallel g(\boldsymbol{\theta};\boldsymbol{\psi})\right\}.$$

## Problems ahead

Minimizing the Kullback--Leibler divergence is not feasible to evaluate the posterior.

Taking $f_t = p(\boldsymbol{\theta} \mid \boldsymbol{y})$ is not feasible: we need the marginal likelihood
to compute the expectation!


## Alternative expression for the marginal likelihood


We consider a different objective to bound the marginal likelihood. Write


\begin{align*}
p(\boldsymbol{y}) = \int_{\boldsymbol{\Theta}}  \frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{g(\boldsymbol{\theta};\boldsymbol{\psi})} g(\boldsymbol{\theta};\boldsymbol{\psi}) \mathrm{d} \boldsymbol{\theta}.
\end{align*}

## Bounding the marginal likelihood


For $h(x)$ a convex function, **Jensen's inequality** implies that $$h\{\mathsf{E}(X)\} \leq \mathsf{E}\{h(X)\},$$ and applying this with $h(x)=-\log(x),$ we get
\begin{align*}
-\log p(\boldsymbol{y}) \leq -\int_{\boldsymbol{\Theta}} \log  \left(\frac{p(\boldsymbol{y}, \boldsymbol{\theta})}{g(\boldsymbol{\theta};\boldsymbol{\psi})}\right) g(\boldsymbol{\theta};\boldsymbol{\psi}) \mathrm{d} \boldsymbol{\theta}.
\end{align*}


## Evidence lower bound

We can thus consider the model that minimizes the **reverse Kullback--Leibler divergence**
\begin{align*}
g(\boldsymbol{\theta}; \widehat{\boldsymbol{\psi}}) = \mathrm{argmin}_{\boldsymbol{\psi}} \mathsf{KL}\{g(\boldsymbol{\theta};\boldsymbol{\psi}) \parallel p(\boldsymbol{\theta} \mid \boldsymbol{y})\}.
\end{align*}

Since $p(\boldsymbol{\theta}, \boldsymbol{y}) = p(\boldsymbol{\theta} \mid \boldsymbol{y}) p(\boldsymbol{y})$, 
\begin{align*}
\mathsf{KL}\{g(\boldsymbol{\theta};\boldsymbol{\psi}) \parallel p(\boldsymbol{\theta} \mid \boldsymbol{y})\} &= \mathsf{E}_{g}\{\log g(\boldsymbol{\theta})\} - \mathsf{E}_g\{\log p( \boldsymbol{\theta}, \boldsymbol{y})\} \\&\quad+ \log p(\boldsymbol{y}).
\end{align*}


## Evidence lower bound

Instead of minimizing the Kullback--Leibler divergence, we can equivalently maximize the so-called **evidence lower bound** (ELBO)
\begin{align*}
\mathsf{ELBO}(g) = \mathsf{E}_g\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\} - \mathsf{E}_{g}\{\log g(\boldsymbol{\theta})\}
\end{align*}

The ELBO is a lower bound for the marginal likelihood because a Kullback--Leibler divergence is non-negative and
\begin{align*}
\log p(\boldsymbol{y}) = \mathsf{ELBO}(g) +  \mathsf{KL}\{g(\boldsymbol{\theta};\boldsymbol{\psi}) \parallel p(\boldsymbol{\theta} \mid \boldsymbol{y})\}.
\end{align*}

## Use of ELBO

The idea is that we will approximate the density $$p(\boldsymbol{\theta} \mid \boldsymbol{y}) \approx g(\boldsymbol{\theta}; \widehat{\boldsymbol{\psi}}).$$

- the ELBO can be used for model comparison (but we compare bounds...)
- we can sample from $q$ as before.

## Heuristics of ELBO

Maximize the evidence, subject to a regularization term:
\begin{align*}
\mathsf{ELBO}(g) = \mathsf{E}_g\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\} - \mathsf{E}_{g}\{\log g(\boldsymbol{\theta})\}
\end{align*}

The ELBO is an objective function comprising:

- the first term will be maximized by taking a distribution placing mass near the MAP of $p(\boldsymbol{y}, \boldsymbol{\theta}),$
- the second term can be viewed as a penalty that favours high entropy of the approximating family (higher for distributions which are diffuse).

## Laplace vs variational approximation


```{r}
#| eval: true
#| echo: false
#| fig-cap: "Skewed density with the Laplace approximation (dashed orange) and variational Gaussian approximation (dotted blue)."
#| label: fig-skewed
loc <- 0; scale <- 1; asym <- 10
# Laplace approximation
# Find mode by numerical optimization
mode_laplace <- optim(
   par = 0.2,
   fn = function(x){
     -sn::dsn(x, xi = loc,omega = scale, alpha = asym, log = TRUE)},
   method = "Brent",
   lower = -1,
   upper = 2, hessian = TRUE)
curve(sn::dsn(x, xi = loc,omega = scale, alpha = asym),
     from = -2,
     to = 4,
     n = 1001,
     bty = "n", ylab = "density",
     ylim = c(0,1), lwd = 1.5)
curve(dnorm(x,
  mean = mode_laplace$par,
  sd = sqrt(1/mode_laplace$hessian[1])),
from = -2, to = 4, add = TRUE, lty = 2,  lwd = 1.5, col = MetBrewer::met.brewer("Hiroshige", 2)[1])


sn_mean <- loc + scale*asym/sqrt(1+asym^2)*sqrt(2/pi)
sn_var <- scale^2*(1-2*asym^2/(1+asym^2)/pi)
# curve(dsn(x, alpha = 10), from = -2, to = 4)
curve(dnorm(x, sn_mean, sd = sqrt(sn_var)),
  add = TRUE, lty = 3, col = MetBrewer::met.brewer("Hiroshige", 2)[2],
 from = -2, to = 4, lwd = 2)
```


## Choice of approximating density

In practice, the quality of the approximation depends on the choice of $g(\cdot; \boldsymbol{\psi}).$

- We typically want matching support.
- The approximation will be affected by the correlation between posterior components $\boldsymbol{\theta} \mid \boldsymbol{y}.$
- Derivations can also be done for $(\boldsymbol{U}, \boldsymbol{\theta})$, where $\boldsymbol{U}$ are latent variables from a data augmentation scheme.


## Factorization

We can consider densities  $g(;\boldsymbol{\psi})$ that factorize into blocks with parameters $\boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_M,$ where
\begin{align*}
g(\boldsymbol{\theta}; \boldsymbol{\psi}) = \prod_{j=1}^M g_j(\boldsymbol{\theta}_j; \boldsymbol{\psi}_j)
\end{align*}
If we assume that each of the $J$ parameters $\theta_1, \ldots, \theta_J$ are independent, then
we obtain a **mean-field** approximation.

## Maximizing the ELBO one step at a time

\begin{align*}
\mathsf{ELBO}(g) &= \int \log p(\boldsymbol{y}, \boldsymbol{\theta}) \prod_{j=1}^M g_j(\boldsymbol{\theta}_j)\mathrm{d} \boldsymbol{\theta} \\&\quad- \sum_{j=1}^M \int \log \{ g_j(\boldsymbol{\theta}_j) \} g_j(\boldsymbol{\theta}_j) \mathrm{d}  \boldsymbol{\theta}_j
 \\& \stackrel{\boldsymbol{\theta}_i}{\propto} \mathsf{E}_{i}\left[\mathsf{E}_{-i}\left\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\right\} \right] - \mathsf{E}_i\left[\log \{ g_i(\boldsymbol{\theta}_i) \}\right]
\end{align*}
which is the negative of a Kullback--Leibler divergence.

## Optimal choice of approximating density

The maximum possible value of zero for the KL is attained when $$\log \{ g_i(\boldsymbol{\theta}_i) \} = \mathsf{E}_{-i}\left\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\right\}.$$
The choice of marginal $g_i$ that maximizes the ELBO is
\begin{align*}
 g^{\star}_i(\boldsymbol{\theta}_i) \propto \exp \left[ \mathsf{E}_{-i}\left\{\log p(\boldsymbol{y}, \boldsymbol{\theta})\right\}\right].
\end{align*}
Often, we look at the kernel of $g^{\star}_j$ to deduce the normalizing constant.


## Coordinate-ascent variational inference (CAVI)

- We can maximize $g^{\star}_j$ in turn for each $j=1, \ldots, M$ treating the other parameters as fixed.
- This scheme is guaranteed to monotonically increase the ELBO until convergence to a local maximum.
- Convergence: monitor ELBO and stop when the change is lower then some present numerical tolerance.
- The approximation may have multiple local optima: perform random initializations and keep the best one.

## Example of CAVI mean-field for Gaussian target


We consider the example from Section 2.2.2 of @Ormerod.Wand:2010 for approximation of a Gaussian distribution, with
\begin{align*}
Y_i &\sim \mathsf{Gauss}(\mu, \tau^{-1}), \qquad i =1, \ldots, n;\\
\mu &\sim \mathsf{Gauss}\{\mu_0, (\tau\tau_0)^{-1}\} \\
\tau &\sim \mathsf{gamma}(a_0, b_0).
\end{align*}
This is an example where the full posterior is available in closed-form, so we can compare our approximation with the truth.

## Variational approximation to Gaussian --- mean

We assume a factorization of the variational approximation $g_\mu(\mu)g_\tau(\tau);$ the factor for $g_\mu$ is proportional to
\begin{align*}
\log g^{\star}_{\mu}(\mu) \propto -\frac{\mathsf{E}_{\tau}(\tau)}{2} \left\{ \sum_{i=1}^n (y_i-\mu)^2-\frac{\tau_0}{2}(\mu-\mu_0)^2\right\},
\end{align*}
which is quadratic in $\mu$ and thus must be Gaussian with precision $\tau_n = \mathsf{E}_{\tau}(\tau)(\tau_0 + n)$ and mean $\tau_0\mu_0 +n\overline{y}.$

## Variational approximation to Gaussian --- precision

The optimal precision factor satisfies
\begin{align*}
 \ln g^{\star}_{\tau}(\tau) &\propto \log \tau\left(\frac{n+1}{2} + a_0-1\right)   \\&\quad- \tau {\color{#c38f16}{\left[b_0  + \frac{\mathsf{E}_{\mu}\left\{\sum_{i=1}^n (y_i-\mu)^2\right\}}{2}  + \frac{\tau_0\mathsf{E}{\mu}\left\{(\mu-\mu_0)^2\right\}}{2}\right]}}.
\end{align*}
Thus a gamma with shape $a_n =a_0 +n/2$ and rate ${\color{#c38f16}{b_n}}$.

## Rate of the gamma for $g_\tau$

It is helpful to rewrite the expected value as
\begin{align*}
 \mathsf{E}_{\mu}\left\{\sum_{i=1}^n (y_i-\mu)^2\right\} = \sum_{i=1}^n \{y_i - \mathsf{E}_{\mu}(\mu)\}^2 + n \mathsf{Var}_{\mu}(\mu),
\end{align*}
so that it depends on the parameters of the distribution of $\mu$ directly.

## CAVI for Gaussian

The algorithm cycles through the following updates until convergence:

- $\mathsf{Va}_{\mu}(\mu) = \{\mathsf{E}_{\tau}(\tau)(\tau_0 + n )\}^{-1},$
- $\mathsf{E}_{\mu}(\mu) = \mathsf{Va}_{\mu}(\mu)\{\tau_0\mu_0 + n \overline{y}\},$
- $\mathsf{E}_{\tau}(\tau) = a_n/b_n$ where $b_n$ is a function of both $\mathsf{E}_{\mu}(\mu)$ and $\mathsf{Var}_{\mu}(\mu).$

We only compute the ELBO at the end of each cycle. 

## Maximization?


Recall that alternating these steps is **equivalent** to maximization of the ELBO. 

- each iteration performs conditional optimization implicitly (as we minimize the reverse KL divergence).


## Monitoring convergence

The derivation of the ELBO is straightforward but tedious; 
\begin{align*}
\mathsf{ELBO}(g) & = a_0\log(b_0) -a_n\log b_n + \log \{\Gamma(a_n)/\Gamma(a_0)\}
\\& \quad - \frac{n}{2}\log(2\pi)+ \frac{1 + \log (\tau_0/\tau_n)}{2}.
\end{align*}

We can also consider relative changes in parameter values as tolerance criterion.

## Bivariate posterior density

```{r}
#| eval: true
#| echo: false
#| label: fig-approx-cavi-gauss
#| fig-cap: "Bivariate density posterior for the conjugate Gaussian-gamma model (left) and CAVI approximation (right)."
set.seed(1234)
y <- rnorm(n = 20, mean = 150, sd = sqrt(20))
n <- length(y)
a0 <- 0.01; 
b0 <- 0.01
mu0 <- 0; 
tau0 <- 1e-4
CAVI_gauss <- function(
    y,
    init = c(mean(y), length(y)/var(y), 1/var(y)),
    tol = 1e-4){
  a0 <- 0.01; b0 <- 0.01
  mu0 <- 0; tau0 <- 1e-4
  n <- length(y); sum_y <- sum(y)
  E_mu <- init[1]
  var_mu <- init[2]
  E_tau <- init[3]
  B <- 20
  an <- (a0+n/2-1)
  elbo <- numeric(B)
  lcst <- a0*log(b0) - lgamma(a0) + 0.5*(1 + log(tau0) - n*log(2*pi))
  for(i in 1:B){
    var_mu <- 1/(E_tau* (tau0 + n))
    E_mu <- (tau0*mu0 + sum_y)/(tau0 + n)
    bn <- b0 + 0.5 * (sum((y-E_mu)^2) + n * var_mu) + 0.5 * tau0 * ((E_mu-mu0)^2 + var_mu)
    E_tau <- an / bn
    elbo[i] <- lcst - an*log(bn) + lgamma(an) + 0.5*log(var_mu)
  }
 list(
  elbo = elbo, 
  mu_mean = E_mu, 
  mu_var = var_mu,
  tau_shape = an, 
  tau_rate = bn)
}
CAVI_approx <- CAVI_gauss(y = y)
gt <- function(x,y, pars){
  dnorm(x, mean = pars$mu_mean, sd = sqrt(pars$mu_var)) * 
    dgamma(y, rate = pars$tau_rate, shape = pars$tau_shape)
}
pars_f <- list(
  mu_mean = (sum(y) + tau0*mu0)/(n+tau0),
  mu_var = 1/(n+tau0),
  tau_rate = b0 + 0.5*(sum(y^2)+tau0*mu0^2 - (sum(y) + tau0*mu0)^2/(n+tau0)),
  tau_shape = 0.5*(n-1) + a0)
ft <- function(x,y, pars){
  dnorm(x, mean = pars$mu_mean, sd = sqrt(pars$mu_var)/sqrt(y)) * 
    dgamma(y, rate = pars$tau_rate, shape = pars$tau_shape)
}

nsim <- 1e4L
tau_s <- rgamma(n = nsim, rate = pars_f$tau_rate, shape = pars_f$tau_shape)
mu_s <- rnorm(n = nsim,
              mean = pars_f$mu_mean, sd = sqrt(pars_f$mu_var)/sqrt(tau_s)) 

library(ggdensity)
g1 <- ggplot() +
  # ggdensity::geom_hdr(
  #   data = data.frame(tau = 1/tau_s, mu = mu_s),
  #   probs = c(0.1, 0.25,0.5,0.75, 0.9),
  #   mapping = aes(x = mu, y = tau)) +
  geom_hdr_fun(fun = ft,
               probs = c(0.1, 0.25,0.5,0.75, 0.9),
               xlim = range(mu_s),
               ylim = range(tau_s),
               n = 1001L,
               args = list(pars = pars_f)) +
  labs(x = expression(mu), 
       y = expression(tau)) +
theme_classic()
g2 <- ggplot() +
  # ggdensity::geom_hdr(
  #   data = data.frame(tau = 1/tau_s, mu = mu_s),
  #   probs = c(0.1, 0.25,0.5,0.75, 0.9),
  #   mapping = aes(x = mu, y = tau)) +
  geom_hdr_fun(fun = gt,
               probs = c(0.1, 0.25,0.5,0.75, 0.9),
               xlim = range(mu_s),
               ylim = range(tau_s),
               n = 1001L,
               args = list(pars = CAVI_approx)) +
  labs(x = expression(mu), 
       y = expression(tau)) +
theme_classic()
g1 + g2  +
  plot_layout(guides = 'collect') &
  theme(legend.position = "bottom")
```

## Marginal posterior densities

```{r}
#| eval: true
#| echo: false
#| label: fig-gauss-cavi-margdens
#| fig-cap: "Marginal posterior density of the mean and precision of the Gaussian (full line), with CAVI approximation (dashed)."
# Marginal densities
mu_loc <- (tau0*mu0 + sum(y))/(tau0+n)
mu_scale <- sqrt((2*b0 + (n-1)*var(y) +tau0*n*(mean(y)-mu0)^2/(tau0+n))/((tau0+n)*(n+2*a0)))
mu_df <- 2*a0 +n

g3 <- ggplot() +
  stat_function(
    fun = function(x){
      dt((x - mu_loc)/mu_scale, 
         df = mu_df)},
    xlim = range(mu_s)) +
  stat_function(
    fun = dnorm, 
    args = list(
      mean = CAVI_approx$mu_mean, 
      sd = sqrt(CAVI_approx$mu_var)),
    xlim = range(mu_s),
    linetype = "dashed")  +
  labs(x = expression(mu), 
       y = "density") +
  theme_classic()
g4 <- ggplot() +
  stat_function(
    fun = dgamma,
    args = list(rate = pars_f$tau_rate,
                shape = pars_f$tau_shape),
    xlim = range(tau_s)) +
  stat_function(
    fun = dgamma, 
    args = list(
      rate = CAVI_approx$tau_rate, 
      shape = CAVI_approx$tau_shape),
    xlim = range(tau_s),
    linetype = "dashed")  +
  labs(x = expression(tau), 
       y = "density") +
  theme_classic()
g3 + g4
```

## CAVI for probit regression

A probit regression is a generalized linear model with probability of success $\Phi(\mathbf{x}_i\boldsymbol{\beta}),$ where $\Phi(\cdot)$ is the cumulative distribution function of a standard Gaussian variable. 

We can write the model as
\begin{align*}
p(\boldsymbol{y} \mid \boldsymbol{\beta}) = \Phi(\mathbf{X}\boldsymbol{\beta})^{\boldsymbol{y}}\Phi(-\mathbf{X}\boldsymbol{\beta})^{\boldsymbol{1}_n -\boldsymbol{y}}
\end{align*}
since $1-\Phi(x) = \Phi(-x).$

## Data augmentation and CAVI

Consider data augmentation  with auxiliary variables $Z_i \mid \boldsymbol{\beta}\sim \mathsf{Gauss}(\mathbf{x}_i\boldsymbol{\beta}, 1).$ 

With $\boldsymbol{\beta} \sim \mathsf{Gauss}_p(\boldsymbol{\mu}_0, \mathbf{Q}_0^{-1}),$ the model admits conditionals
\begin{align*}
\boldsymbol{\beta} \mid \boldsymbol{Z} &\sim \mathsf{Gauss}_p\left\{\mathbf{Q}_{\boldsymbol{\beta}}^{-1}(\mathbf{X}\boldsymbol{Z} + \mathbf{Q}_0\boldsymbol{\mu}_0),  \mathbf{Q}_{\boldsymbol{\beta}}^{-1} \right\}
\\
Z_i \mid y_i, \boldsymbol{\beta} &\sim \mathsf{trunc. Gauss}(\mathbf{x}_i\boldsymbol{\beta}, 1, l_i, u_i) 
\end{align*}
where $\mathbf{Q}_{\boldsymbol{\beta}}= \mathbf{X}^\top\mathbf{X} + \mathbf{Q_0},$ and $[l_i, u_i]$ is $(-\infty,0)$ if $y_i=0$ and $(0, \infty)$ if $y_i=1.$

## CAVI factorization for probit model

We consider a factorization of the form $$g_{\boldsymbol{Z}}(\boldsymbol{z})g_{\boldsymbol{\beta}}(\boldsymbol{\beta}).$$

Then, the optimal form of the density further factorizes as $$g_{\boldsymbol{Z}}(\boldsymbol{z}) = \prod_{i=1}^n g_{Z_i}(z_i).$$

## Gibbs, EM and CAVI

- We exploit the conditionals in the same way as for Gibbs sampling
- The only difference is that we substitute unknown parameter functionals by their expectations.
- Also deep links with the expectation-maximization (EM) algorithm, which optimizes at each step parameters after replacing the log posterior of augmented data by their expectation.
- CAVI however fixes the parameter values (less uncertainty in the posterior because of that).

## Updates for CAVI - probit regression

The model depends on 

- $\mu_{\boldsymbol{Z}}$, the mean parameter of $\boldsymbol{Z}$
- $\mu_{\boldsymbol{\beta}},$ the mean of $\boldsymbol{\beta}.$

Consider the terms in the posterior proportional to $Z_i$, where 
\begin{align*}
p(z_i \mid \boldsymbol{\beta}, y_i) \propto -\frac{z_i^2 - 2z_i \mathbf{x}_i\boldsymbol{\beta}}{2} \times \mathrm{I}(z_i >0)^{y_i}\mathrm{I}(z_i <0)^{1-y_i}
\end{align*}
which is linear in $\boldsymbol{\beta}$. 

## Truncated Gaussian

The expectation of a univariate truncated Gaussian $Z \sim \mathsf{trunc. Gauss}(\mu,\sigma^2, l, u)$ is 
\begin{align*}
\mathsf{E}(Z) = \mu - \sigma\frac{\phi\{(u-\mu/\sigma)\} - \phi\{(l-\mu/\sigma)\}}{\Phi\{(u-\mu/\sigma)\} - \Phi\{(l-\mu/\sigma)\}}.
\end{align*}

## Update for CAVI 

If we replace $\mu=\mathbf{x}_i\mu_{\boldsymbol{\beta}}$, we get the update
\begin{align*}
\mu_{Z_i}(z_i) = \begin{cases}
\mathbf{x}_i\mu_{\boldsymbol{\beta}} - \frac{ \phi(\mathbf{x}_i\mu_{\boldsymbol{\beta}})}{1-\Phi(\mathbf{x}_i\mu_{\boldsymbol{\beta}})} & y_i = 0;\\
\mathbf{x}_i\mu_{\boldsymbol{\beta}} + \frac{ \phi(\mathbf{x}_i\mu_{\boldsymbol{\beta}})}{\Phi(\mathbf{x}_i\mu_{\boldsymbol{\beta}})} & y_i = 1,
\end{cases}
\end{align*}
since $\phi(x)=\phi(-x).$

## Update for regression parameters

The optimal form for $\boldsymbol{\beta}$ is Gaussian and  proceeding similarly,
\begin{align*}
\boldsymbol{\mu}_{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X} + \mathbf{Q_0})^{-1}(\mathbf{X}\boldsymbol{\mu}_{\boldsymbol{Z}} + \mathbf{Q}_0\boldsymbol{\mu}_0)
\end{align*}
where $\boldsymbol{\mu}_{\boldsymbol{Z}} = \mathsf{E}_{\boldsymbol{Z}}(\boldsymbol{Z}).$

Other parameters of the distribution are known functions of covariates, etc. 

<!--
The ELBO is
\begin{align*}
\mathsf{ELBO} = -\frac{1}{2}(\boldsymbol{z} - \mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{z} - \mathbf{X}\boldsymbol{\beta})\right\} \times \prod_{i=1}^n \mathrm{I}(z_i > 0)^{y_i}\mathrm{I}(z_i \le 0)^{1-y_i}
\end{align*}
where use the fact $\mathsf{E}_{Z_i}\{\mathrm{I}(z_i \le 0)\}=$that the entropy of a $p$-dimensional multivariate Gaussian random vector with precision $\mathbf{Q}$ is $p/2\{1+\log(2\pi)\} - \log |\mathbf{Q}|/2.$
-->


```{r}
#| eval: true
#| echo: false
# Data augmentation: "z" is latent Gaussian, 
# truncated to be negative for failures 
# and positive for successes
cavi_probit <- function(
  y, # response vector (0/1)
  X, # model matrix
  prior_beta_prec = diag(rep(0.01, ncol(X))),
  prior_beta_mean = rep(0, ncol(X)),
  maxiter = 1000L,
  tol = 1e-4
) {
  # Precompute fixed quantity
  sc <- solve(crossprod(X) + prior_beta_prec)
  pmu_prior <- prior_beta_prec %*% prior_beta_mean
  n <- length(y) # number of observations
  stopifnot(nrow(X) == n)
  mu_z <- rep(0, n)
  y <- as.logical(y)
  ELBO <- numeric(maxiter)
  lcst <- -0.5 *
    log(det(solve(prior_beta_prec) %*% crossprod(X) + diag(ncol(X))))
  for (b in seq_len(maxiter)) {
    mu_b <- c(sc %*% (t(X) %*% mu_z + pmu_prior))
    lp <- c(X %*% mu_b)
    mu_z <- lp + dnorm(lp) / (pnorm(lp) - ifelse(y, 0, 1))
    ELBO[b] <- sum(pnorm(lp, lower.tail = y, log.p = TRUE)) -
      lcst - 0.5 *
        c(t(mu_b - prior_beta_mean) %*%
            prior_beta_prec %*%
            (mu_b - prior_beta_mean)) 
    if (b > 2 && (ELBO[b] - ELBO[b - 1]) < tol) {
      break
    }
  }
  list(mu_z = mu_z, mu_beta = mu_b, elbo = ELBO[1:b])
}
```

## Example

We consider for illustration purposes data from Experiment 2 of @Duke.Amir:2023 on the effect of sequential decisions and purchasing formats.

We fit a model with 
- `age` of the participant (scaled) and 
- `format,` the binary variable which indicate the experimental condition (sequential vs integrated).

## ELBO and marginal density approximation

```{r}
#| eval: true
#| echo: false
#| cache: true
#| label: fig-elbo-CAVI-probit
#| fig-cap: "ELBO (left) and marginal density approximation with true density (full) versus variational approximation (dashed)."
# Example with data from Experiment 2 of Duke and Amir (2023)
# on the effect of sequential decisions and purchasing formats
data(DA23_E2, package = "hecedsm")
X <- model.matrix(
  ~ scale(age) + format,
  data = DA23_E2,
  contrasts.arg = list(format = "contr.sum")
)
y <- DA23_E2$purchased
# Fit the probit model via coordinate-ascent variational inference
cavi_probit_DA <- cavi_probit(y = y, X = X)
# Compare posterior mean with frequentist estimates
coefs <- cbind(cavi_probit_DA$mu_beta, coef(glm(y ~ X - 1, family = binomial("probit"))))
library(rust)
prior_beta_prec = diag(rep(0.01, ncol(X)))
prior_beta_mean = rep(0, ncol(X))
loglik <- function(par, y, X) {
  par <- c(par)
  sum(
    y *
      pnorm(
        q = c(X %*% c(par)),
        log.p = TRUE
      ) +
      (1 - y) *
        pnorm(
          q = c(X %*% c(par)),
          log.p = TRUE,
          lower.tail = FALSE
        )
  ) +
    -0.5 *
      c(
        t(prior_beta_mean - par) %*% prior_beta_prec %*% (prior_beta_mean - par)
      )
}
pars <- glm(formula = y ~ X - 1, family = binomial("probit"))$coef
# optim(fn = loglik, par = pars, X = X, y = y, control = list(fnscale = -1))
samp <- rust::ru(
  logf = loglik,
  d = 3,
  n = 1e5,
  init = pars,
  X = X,
  y = y,
  trans = "BC"
)

marg_sd <- sqrt(diag(solve(crossprod(X) + prior_beta_prec)))


g1 <- ggplot(data = data.frame(x = samp$sim_vals[, 3])) +
  geom_density(mapping = aes(x = x)) +
  stat_function(
    fun = dnorm,
    n = 1001,
    args = list(mean = cavi_probit_DA$mu_beta[3], sd = marg_sd[3]),
    linetype = "dashed"
  ) +
  scale_y_continuous(limits = c(0, NA), expand = expansion()) +
  labs(x = expression(beta[2]), y = "", subtitle = "density") +
  theme_classic()
g2 <- ggplot(
  data = data.frame(
    niter = 1:length(cavi_probit_DA$elbo),
    elbo = cavi_probit_DA$elbo
  ),
  mapping = aes(x = niter, y = elbo)
) +
  geom_point() +
  geom_line() +
  labs(
    y = "",
    subtitle = "evidence lower bound (ELBO)",
    x = "number of iterations"
  ) +
  theme_classic()
g2 + g1
```

## Comments

- With vague priors, the coefficients for the mean $\boldsymbol{\mu}_{\boldsymbol{\beta}}=(\beta_0, \beta_1, \beta_2)^\top$ matches the frequentist point estimates of the probit regression to four significant digits.

- Convergence is very fast, as shown by the ELBO plot.

- The marginal density approximations are underdispersed.


## Stochastic optimization


We consider alternative numeric schemes which rely on stochastic optimization [@Hoffman:2013].

The key idea behind these methods is that

- we can use gradient-based algorithms,
- and approximate the expectations with respect to $g$ by drawing samples from it

Also allows for minibatch (random subset) selection to reduce computational costs in large samples

## Black-box variational inference

@Ranganath.Gerrish.Blei:2014 shows that the gradient of the ELBO reduces to
\begin{align*}
 \frac{\partial}{\partial \boldsymbol{\psi}} \mathsf{ELBO}(g) &=\mathsf{E}_{g}\left\{\frac{\partial \log g(\boldsymbol{\theta}; \boldsymbol{\psi})}{\partial \boldsymbol{\psi}} \times \log \left( \frac{p(\boldsymbol{\theta}, \boldsymbol{y})}{g(\boldsymbol{\theta}; \boldsymbol{\psi})}\right)\right\}
\end{align*}
using the change rule, differentiation under the integral sign (dominated convergence theorem) and the identity
\begin{align*}
\frac{\partial \log g(\boldsymbol{\theta}; \boldsymbol{\psi})}{\partial \boldsymbol{\psi}} g(\boldsymbol{\theta}; \boldsymbol{\psi}) = \frac{\partial g(\boldsymbol{\theta}; \boldsymbol{\psi})}{\partial \boldsymbol{\psi}}
\end{align*}

## Black-box variational inference in practice

- Note that the gradient simplifies for $g_i$ in exponential families (covariance of sufficient statistic with $\log(p/g)$).
- The gradient estimator is particularly noisy, so @Ranganath.Gerrish.Blei:2014 provide two methods to reduce the variance of this expression using control variates and Rao--Blackwellization.

## Automatic differentiation variational inference

@Kucukelbir:2017 proposes a stochastic gradient algorithm, but with two main innovations.

- The first is the general use of Gaussian approximating densities for factorized density, with parameter transformations to map from the support of $T: \boldsymbol{\Theta} \mapsto \mathbb{R}^p$ via $T(\boldsymbol{\theta})=\boldsymbol{\zeta}.$
- The second is to use the resulting **location-scale** family to obtain an alternative form of the gradient.

## Gaussian full-rank approximation

Consider an approximation $g(\boldsymbol{\zeta}; \boldsymbol{\psi})$ where $\boldsymbol{\psi}$ consists of

- mean parameters $\boldsymbol{\mu}$ and
- covariance $\boldsymbol{\Sigma}$, parametrized through a Cholesky decomposition

The full approximation is of course more flexible when the transformed parameters $\boldsymbol{\zeta}$ are correlated, but is more expensive to compute than the mean-field approximation.

## Change of variable

The change of variable introduces a Jacobian term $\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})$ for the approximation to the density $p(\boldsymbol{\theta}, \boldsymbol{y})$, where

\begin{align*}
p(\boldsymbol{\theta}, \boldsymbol{y}) = p(\boldsymbol{\zeta}, \boldsymbol{y}) \left|\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})\right|
\end{align*}

## Gaussian entropy

The entropy of the multivariate Gaussian with mean $\boldsymbol{\mu}$ and covariance $\boldsymbol{\Sigma} = \mathbf{LL}^\top$, where $\mathbf{L}$ is a lower triangular matrix, is
\begin{align*}
 \mathcal{E}(\mathbf{L}) = - \mathsf{E}_g(\log g) &= \frac{D+D\log(2\pi) + \log |\mathbf{LL}^\top|}{2},
\end{align*}
and only depends on $\boldsymbol{\Sigma}$.

## ELBO with Gaussian approximation

Since the Gaussian is a location-scale family, we can rewrite the model in terms of a standardized Gaussian variable $\boldsymbol{Z}\sim \mathsf{Gauss}_p(\boldsymbol{0}_p, \mathbf{I}_p)$ where $\boldsymbol{\zeta} = \boldsymbol{\mu} + \mathbf{L}\boldsymbol{Z}$ (this transformation has unit Jacobian).

The ELBO with the transformation becomes
\begin{align*}
 \mathsf{E}_{\boldsymbol{Z}}\left[ \log p\{\boldsymbol{y}, T^{-1}(\boldsymbol{\zeta})\} + \log \left|\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})\right|\right] + \mathcal{E}(\mathbf{L}).
\end{align*}

## Chain rule

If $\boldsymbol{\theta} = T^{-1}(\boldsymbol{\zeta})$ and $\boldsymbol{\zeta} = \boldsymbol{\mu} + \mathbf{L}\boldsymbol{z},$ we have for $\boldsymbol{\psi}$ equal to either $\boldsymbol{\mu}$ or $\mathbf{L}$, using the chain rule,
\begin{align*}
 & \frac{\partial}{\partial \boldsymbol{\psi}}\log p(\boldsymbol{y}, \boldsymbol{\theta})
 \\&\quad  = \frac{\partial \log p(\boldsymbol{y}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}}
   \times \frac{\partial T^{-1}(\boldsymbol{\zeta})}{\partial \boldsymbol{\zeta}}
\times \frac{\partial (\boldsymbol{\mu} + \mathbf{L}\boldsymbol{z})}{\partial \boldsymbol{\psi}}
\end{align*}

## Gradients for ADVI {.smaller}

The gradients of the ELBO with respect to the mean and variance are
\begin{align*}
 \nabla_{\boldsymbol{\mu}} &= \mathsf{E}_{\boldsymbol{Z}}\left\{\frac{\partial \log p(\boldsymbol{y}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \frac{\partial T^{-1}(\boldsymbol{\zeta})}{\partial \boldsymbol{\zeta}}  + \frac{\partial \log \left|\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})\right|}{\partial \boldsymbol{\zeta}}\right\} \\
 \nabla_{\mathbf{L}} &= \mathsf{E}_{\boldsymbol{Z}}\left[\left\{\frac{\partial \log p(\boldsymbol{y}, \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \frac{\partial T^{-1}(\boldsymbol{\zeta})}{\partial \boldsymbol{\zeta}}  + \frac{\partial \log \left|\mathbf{J}_{T^{-1}}(\boldsymbol{\zeta})\right|}{\partial \boldsymbol{\zeta}}\right\}\boldsymbol{Z}^\top\right] + \mathbf{L}^{-\top}.
\end{align*}
and we can approximate the expectation by drawing standard Gaussian samples $\boldsymbol{Z}_1, \ldots, \boldsymbol{Z}_B.$

## Quality of approximation

Consider the stochastic volatility model.

```{r}
#| eval: true
#| echo: false
#| fig-align: 'center'
#| out-width: '90%'
knitr::include_graphics("fig/stochvol-volatility.png")
```

Fitting HMC-NUTS to the exchange rate data takes 156 seconds for 10K iterations, vs 2 seconds for the mean-field approximation.






## References

